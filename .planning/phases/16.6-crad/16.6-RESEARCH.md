# Phase 16.6: CRAD (Corpus-Relative Aspect Differentiation) - Research

**Researched:** 2026-02-26
**Domain:** SQLite JSON extraction, Gemini File Search validation, series-relative frequency computation, identity header augmentation
**Confidence:** HIGH (all findings verified against codebase)

## Summary

Phase 16.6 implements a 3-pass CRAD algorithm that is entirely pure computation (Passes 1-3) with Gemini API validation afterward. The codebase already contains every pattern needed: `re_enrich_retrieval.py` provides the battle-tested re-upload sequence, `check_stability.py` provides the Gemini query-and-rank-check pattern, and `header_builder.py` provides the identity header template. All patterns were read and verified against the current code.

The most critical research finding is a **tie-breaking bug in the plan's Pass 2 design**: when multiple aspects share the same series frequency (common -- the ITOE AT OH series has 240 aspects at freq=1 out of 28 files), the plan's sort is not stable across Python list ordering. The S4a audit proved that "order of being vs. order of knowing" (the first freq=1 aspect in the list) fails as a query while "Zeno's arrow paradox" (the 7th freq=1 aspect) succeeds at rank 1. The algorithm needs secondary tie-breaking by corpus frequency or a validation-driven selection loop.

The second critical finding is a **schema mismatch in the plan's Pass 1 query**: `file_metadata_ai` has no `filename` column -- it uses `file_path`. All queries must join `files` on `file_path` to access `filename`. The correct series grouping uses `file_path LIKE '%/{parent_dir}/%'` not `filename LIKE '%{series_name}%'`.

**Primary recommendation:** Implement CRAD Passes 1-3 exactly as designed but with two fixes: (1) secondary tie-breaking in Pass 2 by corpus frequency ascending, and (2) correct JOIN pattern for `file_metadata_ai`. Reuse `re_enrich_retrieval.py` functions for re-upload via `importlib.util` (proven pattern from `re_enrich_office_hours.py`).

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| sqlite3 | stdlib | DB reads/writes for CRAD tables | Already used everywhere; no async needed for 63 files |
| json | stdlib | Parse `metadata_json` from `file_metadata_ai` | `json.loads()` on `metadata_json` field; `json_extract()` returns text not Python list |
| re | stdlib | Strip markdown from aspects (`re.sub(r'[*_\`]', '', aspect)`) | Same pattern used in `check_stability.py` line 633 |
| google-genai | installed | Gemini File Search queries for validation | Used by `check_stability.py` and `re_enrich_retrieval.py` |
| keyring | installed | API key retrieval (`keyring.get_password("objlib-gemini", "api_key")`) | Every standalone script uses this pattern |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| importlib.util | stdlib | Import `re_enrich_retrieval.py` functions | For re-upload step (proven in `re_enrich_office_hours.py` lines 34-46) |
| pathlib.PurePosixPath | stdlib | Extract parent dir name for series detection | `PurePosixPath(file_path).parent.name` gives the course folder |
| collections.Counter | stdlib | Frequency counting | Alternative to manual dict for aspect-frequency map |
| tempfile | stdlib | Temp files for re-upload | Used by `re_enrich_retrieval.py` process_file() |
| time | stdlib | Rate limiting between Gemini queries | `time.sleep(2)` between validation queries |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| sqlite3 sync | aiosqlite async | Overkill for 63 files; sync is simpler and every script uses it |
| Manual dict freq counting | collections.Counter | Counter is cleaner but dict pattern matches existing code |
| New re-upload code | Import from re_enrich_retrieval.py | Import is proven; re_enrich_office_hours.py already does this |

## Architecture Patterns

### Recommended Project Structure
```
scripts/
  crad_algorithm.py       # Pass 1, 2, 3 functions + DB schema creation
  crad_pilot.py           # Pilot runner (3 files)
  crad_full_run.py        # Full run (63 files) + re-upload
  re_enrich_retrieval.py  # Reused: upload_file_with_poll, import_to_store_with_poll, etc.
src/objlib/upload/
  header_builder.py       # Modified: add Discrimination field
scripts/
  check_stability.py      # Modified: CRAD phrase in A7 before S4a
```

### Pattern 1: Standalone Script with sys.path.insert
**What:** Every standalone script in `scripts/` adds the project's `src/` to Python path.
**When to use:** Always for scripts that import from `objlib`.
**Example:**
```python
# Source: scripts/check_stability.py line 63, re_enrich_retrieval.py line 32
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "src"))
```

### Pattern 2: Gemini API Key + Client Initialization
**What:** Every script that queries Gemini uses the same 4-line initialization.
**When to use:** For validation queries and re-upload.
**Example:**
```python
# Source: scripts/check_stability.py lines 829-830, re_enrich_retrieval.py lines 408-413
import keyring
from google import genai

api_key = keyring.get_password("objlib-gemini", "api_key")
if not api_key:
    print("ERROR: No API key found in keyring.", file=sys.stderr)
    sys.exit(2)
client = genai.Client(api_key=api_key)
```

### Pattern 3: Gemini File Search Query + Rank Extraction
**What:** Query Gemini with a text string and determine the rank of a target file in the results.
**When to use:** CRAD phrase validation against live store.
**Example:**
```python
# Source: scripts/check_stability.py lines 567-621 (A7 implementation)
from google.genai import types as genai_types

SEARCH_MODEL = "gemini-2.5-flash"

def query_and_get_rank(
    client: genai.Client,
    store_resource_name: str,
    query: str,
    target_store_doc_id: str,
    top_k: int = 20,
) -> int | None:
    """Return 1-indexed rank of target file, or None if not in top_k."""
    response = client.models.generate_content(
        model=SEARCH_MODEL,
        contents=query,
        config=genai_types.GenerateContentConfig(
            tools=[genai_types.Tool(
                file_search=genai_types.FileSearch(
                    file_search_store_names=[store_resource_name]
                )
            )]
        ),
    )
    store_doc_prefix = target_store_doc_id.split("-")[0] if target_store_doc_id else ""
    if response.candidates:
        gm = getattr(response.candidates[0], "grounding_metadata", None)
        if gm:
            chunks = getattr(gm, "grounding_chunks", []) or []
            for i, chunk in enumerate(chunks[:top_k]):
                rc = getattr(chunk, "retrieved_context", None)
                if not rc:
                    continue
                title = getattr(rc, "title", "") or ""
                if store_doc_prefix and title == store_doc_prefix:
                    return i + 1
    return None
```

### Pattern 4: Store Resolution
**What:** Map store display name to resource name.
**When to use:** Before any Gemini File Search query.
**Example:**
```python
# Source: scripts/check_stability.py lines 185-202
def resolve_store(client: genai.Client, display_name: str = "objectivism-library") -> str:
    for store in client.file_search_stores.list():
        if getattr(store, "display_name", None) == display_name:
            return store.name
    raise RuntimeError(f"Store '{display_name}' not found")
```

### Pattern 5: Re-upload via importlib Import
**What:** Import helper functions from `re_enrich_retrieval.py` without running its main().
**When to use:** For the 63-file re-upload step in Plan 16.6-02.
**Example:**
```python
# Source: scripts/re_enrich_office_hours.py lines 34-46
import importlib.util
_script = Path(__file__).resolve().parent / "re_enrich_retrieval.py"
_spec = importlib.util.spec_from_file_location("re_enrich_retrieval", _script)
_mod = importlib.util.module_from_spec(_spec)
_spec.loader.exec_module(_mod)

# Now use directly:
resolve_store_name = _mod.resolve_store_name
build_enriched_content_with_header = _mod.build_enriched_content_with_header
upload_file_with_poll = _mod.upload_file_with_poll
import_to_store_with_poll = _mod.import_to_store_with_poll
delete_old_store_doc = _mod.delete_old_store_doc
delete_old_raw_file = _mod.delete_old_raw_file
process_file = _mod.process_file
```

### Pattern 6: Series Detection via Parent Directory
**What:** Group files into series using their filesystem parent directory name.
**When to use:** CRAD Pass 1 genus computation.
**Example:**
```python
# Source: verified against DB structure (file_path column)
# Parent directories map directly to series:
#   ITOE Advanced Topics -> 58 files (both classes and OH)
#   ITOE -> 48 files (both classes and OH)
#   Objectivist Logic -> 50 files
#   Perception -> 32 files
#   Peikoff Podcast -> 334 files (Episodes)
#   Books -> 27 files
#   MOTM -> 468 files

from pathlib import PurePosixPath
series_name = PurePosixPath(file_path).parent.name
# e.g., "/Volumes/.../Courses/ITOE Advanced Topics/ITOE AT - Class 14-01 - OH.txt"
#   -> "ITOE Advanced Topics"
```

### Pattern 7: Identity Header Addition (Discrimination Field)
**What:** Add a `Discrimination:` field to the existing identity header structure.
**When to use:** Plan 16.6-03 permanent modification to `header_builder.py`.
**Example:**
```python
# Source: src/objlib/upload/header_builder.py lines 111-129 (current structure)
# Current header fields in order:
#   --- DOCUMENT METADATA ---
#   Title: {stem}
#   Course: {parent_dir_name}
#   Class: {class_id}          (optional)
#   Topic: {scanner_topic}     (optional)
#   Tags: {primary_topics}     (optional)
#   Aspects: {topic_aspects}   (optional)
#   --- END METADATA ---

# New field goes AFTER Aspects, BEFORE end marker:
#   Aspects: {topic_aspects}
#   Discrimination: {crad_phrase}   <-- NEW
#   --- END METADATA ---
```

### Anti-Patterns to Avoid
- **Querying `file_metadata_ai` by `filename`:** This table has NO `filename` column. Always JOIN with `files` on `file_path`.
- **Using `json_extract()` and treating result as Python list:** `json_extract(metadata_json, '$.topic_aspects')` returns a TEXT STRING in SQLite. You must `json.loads()` it in Python. Use the full `metadata_json` load pattern instead.
- **Using `--reset-existing` for re-upload:** Creates orphaned store documents. Use the upload-first/delete-old-after pattern from `re_enrich_retrieval.py`.
- **Sorting aspects only by series frequency:** When many aspects share freq=1, the ordering is by list position in the JSON array, which is arbitrary. Must use secondary tie-breaking.
- **Assuming `retrievability_audit.py` detect_series matches CRAD series:** They are different. The audit uses file_path substring matching with overlapping categories (ITOE AT OH vs ITOE AT). CRAD series = parent directory name, which is a clean partition (ITOE Advanced Topics contains BOTH class files and OH files).

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| File upload to Gemini | Custom upload code | `re_enrich_retrieval.py:upload_file_with_poll()` | Handles ACTIVE polling, timeout, error states |
| Store document import | Custom import code | `re_enrich_retrieval.py:import_to_store_with_poll()` | Handles operation polling, doc_name extraction from 3 different response formats |
| Old store doc deletion | Custom delete code | `re_enrich_retrieval.py:delete_old_store_doc()` | Handles 404/403 as success (expired files) |
| Old raw file deletion | Custom delete code | `re_enrich_retrieval.py:delete_old_raw_file()` | Handles 403 (Gemini returns 403 for expired, not 404) |
| DB update after upload | Custom SQL | `re_enrich_retrieval.py:process_file()` | Updates gemini_file_id, gemini_store_doc_id, timestamps atomically |
| Identity header building | Custom header code | `header_builder.build_identity_header()` | Handles all conditional fields, joins file_primary_topics |
| Gemini query with retry | Custom retry loop | `GeminiSearchClient.query_with_retry()` in `src/objlib/search/client.py` | tenacity-based retry with exponential backoff+jitter |

**Key insight:** The entire re-upload pipeline is already factored into reusable functions in `re_enrich_retrieval.py`. The `re_enrich_office_hours.py` script proves these can be imported via `importlib.util` without modification. The only new code needed is the CRAD computation itself (Pass 1-3) and the Gemini validation loop.

## Common Pitfalls

### Pitfall 1: `file_metadata_ai` Has No `filename` Column
**What goes wrong:** The plan's Pass 1 pseudocode queries `SELECT filename, json_extract(metadata_json, ...) FROM file_metadata_ai WHERE filename LIKE ?`. This fails with `OperationalError: no such column: filename`.
**Why it happens:** `file_metadata_ai` schema is: `(metadata_id, file_path, metadata_json, model, model_version, prompt_version, extraction_config_hash, is_current, created_at)`. No `filename`.
**How to avoid:** Always JOIN with `files` table:
```python
rows = conn.execute("""
    SELECT f.filename, fma.metadata_json
    FROM files f
    JOIN file_metadata_ai fma ON fma.file_path = f.file_path AND fma.is_current = 1
    WHERE f.file_path LIKE ?
""", (f"%/{parent_dir_name}/%",)).fetchall()
```
**Warning signs:** `sqlite3.OperationalError: no such column: filename` when running Pass 1.

### Pitfall 2: Tie-Breaking When Multiple Aspects Share Same Frequency
**What goes wrong:** For ITOE AT OH (28 files), 240 out of ~280 unique aspects have freq=1. Sorting by series frequency ascending gives an arbitrary order among freq=1 aspects (Python list order from the JSON array). The first aspect fills the 7-word budget before a more discriminating aspect is reached.
**Why it happens:** Mistral extraction produces 10 aspects per file, but in a 28-file series, most aspects appear in only 1 file. The "rarest" is not unique -- there are dozens of equally rare aspects, and the most discriminating one may not be first.
**How to avoid:** Use secondary tie-breaking: when series frequency is equal, prefer aspects with lower CORPUS frequency (more globally unique). If still tied, prefer shorter aspects (more words left for additional discrimination).
```python
# Tie-breaking: series freq ascending, then corpus freq ascending, then word count ascending
sorted_aspects = sorted(
    non_genus_aspects,
    key=lambda a: (
        series_freq.get(a, 0),      # Primary: rarest in series
        corpus_freq.get(a, 0),      # Secondary: rarest in corpus
        len(a.split()),             # Tertiary: shorter aspects first (more budget)
    )
)
```
**Warning signs:** Generated phrase for a known-failing file does not match the known-working S4a query. Validation rank > 5 for pilot files.
**Verified evidence:** For ITOE AT Class 14-01 OH, the S4a query "order of being vs. order of knowing perception and concept formation law of identity and motion" MISSED (rank > 5), while "Zeno's arrow paradox" succeeded at rank 1. Both have corpus freq=1 and series freq=1. The S4a query was constructed with the same naive sort that the CRAD plan uses.

### Pitfall 3: 7-Word Budget Exhaustion on First Aspect
**What goes wrong:** A single long aspect like "order of being vs. order of knowing" (7 words) fills the entire word budget, leaving no room for additional discriminating aspects. The resulting single-aspect phrase may not find the target file.
**Why it happens:** The Pass 3 essentialization loop adds words aspect-by-aspect. If the first aspect is 7+ words, the loop stops after one aspect.
**How to avoid:** Two strategies: (1) Prefer shorter aspects in tie-breaking (see Pitfall 2). (2) If the first aspect alone fills the budget, try validation with just that aspect; if it fails, skip to the second aspect instead.
**Warning signs:** All phrases for a series are single-aspect. High escalation rate.

### Pitfall 4: CRAD Series != Retrievability Audit Series
**What goes wrong:** Using `detect_series()` from `retrievability_audit.py` for CRAD gives overlapping categories: "ITOE AT OH" (28), "ITOE AT" (30), "ITOE OH" (16), "ITOE" (32). CRAD genus computation requires mutually exclusive series.
**Why it happens:** `retrievability_audit.py` uses cascading `if/elif` on file_path patterns with subcategory splits (OH vs non-OH). CRAD series = parent directory = clean partition.
**How to avoid:** Use `PurePosixPath(file_path).parent.name` for CRAD series. This gives:
- "ITOE Advanced Topics": 58 files (includes ALL ITOE AT and ITOE AT OH)
- "ITOE": 48 files (includes ALL ITOE and ITOE OH)
- "Objectivist Logic": 50 files
- "Perception": 32 files
- "Peikoff Podcast": 334 files
- "Books": 27 files
- "MOTM": 468 files
**Warning signs:** Series file count doesn't match expected (e.g., ITOE AT OH shows 28 instead of 58 for the full parent directory).

### Pitfall 5: json_extract Returns Text, Not Python List
**What goes wrong:** Code assumes `json_extract(metadata_json, '$.topic_aspects')` returns a Python list. It returns a TEXT string that looks like `'["aspect1", "aspect2"]'`.
**Why it happens:** SQLite's `json_extract()` returns the JSON text representation.
**How to avoid:** Load the full `metadata_json` and parse in Python:
```python
meta = json.loads(row["metadata_json"])
aspects = meta.get("topic_aspects", [])
```
Or if using `json_extract()`, always wrap with `json.loads()` in Python.
**Warning signs:** `TypeError: argument of type 'str' is not iterable` when doing `if aspect in json_extract_result`.

### Pitfall 6: Rate Limits on 63 Validation Queries
**What goes wrong:** Running 63 Gemini queries too fast causes 429 errors.
**How to avoid:** The Phase 16.5 audit ran 1,749 queries in 16.5 minutes (1.77/sec) with concurrency=10 and zero errors. For 63 sequential queries at 2-second spacing (as the plan specifies), total time is ~126 seconds. This is well within limits.
**Verified evidence:** Phase 16.5-02 metadata shows `"api_calls": 1752, "errors": 0` at 1.77 queries/sec with concurrency=10.
**Recommendation:** 2-second spacing between sequential validation queries is conservative and safe. No need for concurrency -- 63 sequential queries at 2s/each = 126 seconds total.

### Pitfall 7: Stale Store Doc IDs After Re-upload
**What goes wrong:** After re-uploading 63 files, the old store documents are still in the store. Store-sync must run to clean them up, or subsequent stability checks will show 63 orphans.
**Why it happens:** `re_enrich_retrieval.py:process_file()` deletes the old store doc AFTER uploading the new one (upload-first safety). But if the script crashes mid-run, some old docs may remain.
**How to avoid:** Always run `python -m objlib store-sync --store objectivism-library --no-dry-run --yes` after re-upload. Verify: DB=1749, Store=1749, Orphans=0.
**Warning signs:** `check_stability.py` Assertion 1 fails (count mismatch) or Assertion 3 fails (orphaned store docs).

### Pitfall 8: file_discrimination_phrases Keyed by filename, Not file_path
**What goes wrong:** The proposed schema uses `filename TEXT PRIMARY KEY` but `header_builder.py` receives `file_path`, not `filename`. The lookup in `header_builder.py` must extract the filename from the file_path.
**Why it happens:** The `files` table has both `filename` and `file_path`. The `build_identity_header()` function receives `file_path` as its parameter.
**How to avoid:** In `header_builder.py`, use `Path(file_path).name` for the lookup or join via the `files` table. The existing function already does a query `SELECT filename, metadata_json FROM files WHERE file_path = ?`, so `filename` is available.

## Code Examples

### CRAD Pass 1: Build Genus Profile (Corrected)
```python
# CORRECTED version of Plan 16.6-01 Task 2
# Fixes: (1) JOIN with files table, (2) file_path LIKE for series detection
def build_genus_profile(conn: sqlite3.Connection, parent_dir_name: str) -> dict:
    """Build genus profile for all files in a series (identified by parent dir)."""
    rows = conn.execute("""
        SELECT f.filename, fma.metadata_json
        FROM files f
        JOIN file_metadata_ai fma ON fma.file_path = f.file_path AND fma.is_current = 1
        WHERE f.gemini_state = 'indexed'
          AND f.file_path LIKE ?
    """, (f"%/{parent_dir_name}/%",)).fetchall()

    freq = {}
    file_count = len(rows)
    for filename, mj in rows:
        aspects = json.loads(mj).get("topic_aspects", [])
        for a in aspects:
            freq[a] = freq.get(a, 0) + 1

    threshold_80pct = max(1, int(file_count * 0.8))
    shared = [a for a, c in freq.items() if c >= threshold_80pct]

    return {
        "series_name": parent_dir_name,
        "file_count": file_count,
        "shared_aspects": shared,
        "aspect_frequency_map": freq,
        "rarity_threshold": 2,
    }
```

### CRAD Pass 2: Build Differentia (with Tie-Breaking)
```python
# ENHANCED version of Plan 16.6-01 Task 3
# Adds: secondary tie-breaking by corpus frequency
def build_file_differentia(
    filename: str,
    aspects: list[str],
    genus: dict,
    corpus_freq: dict[str, int],
) -> dict:
    """Compare file's aspects against genus, rank by series rarity then corpus rarity."""
    series_freq = genus["aspect_frequency_map"]
    shared = set(genus["shared_aspects"])

    non_genus = [a for a in aspects if a not in shared]

    # Sort by: (1) series freq ascending, (2) corpus freq ascending, (3) word count ascending
    non_genus.sort(key=lambda a: (
        series_freq.get(a, 0),
        corpus_freq.get(a, 0),
        len(a.split()),
    ))

    return {
        "filename": filename,
        "series_name": genus["series_name"],
        "top_3_rarest": non_genus[:3],
        "all_differentia": non_genus,
    }
```

### CRAD Pass 3: Build Discrimination Phrase
```python
# Same as Plan 16.6-01 Task 4 (no changes needed)
import re

def build_discrimination_phrase(differentia: dict, max_words: int = 7) -> dict:
    """Concatenate top-N rarest aspects into phrase within word budget."""
    def clean(aspect: str) -> str:
        return re.sub(r'[*_`]', '', aspect).strip()

    words_used = []
    aspects_used = []
    for aspect in differentia["top_3_rarest"]:
        cleaned = clean(aspect)
        words = cleaned.split()
        if len(words_used) + len(words) <= max_words:
            words_used.extend(words)
            aspects_used.append(aspect)
        else:
            break

    return {
        "filename": differentia["filename"],
        "series_name": differentia["series_name"],
        "phrase": " ".join(words_used),
        "word_count": len(words_used),
        "aspects_used": aspects_used,
    }
```

### Corpus Frequency Map Builder (for Pass 2 tie-breaking)
```python
# Source: scripts/check_stability.py lines 494-508, retrievability_audit.py lines 95-111
def build_corpus_freq_map(conn: sqlite3.Connection) -> dict[str, int]:
    """Build aspect frequency map across ALL files (not just one series)."""
    freq = {}
    rows = conn.execute(
        "SELECT metadata_json FROM file_metadata_ai WHERE is_current = 1"
    ).fetchall()
    for (mj,) in rows:
        if not mj:
            continue
        try:
            for a in json.loads(mj).get("topic_aspects", []):
                freq[a] = freq.get(a, 0) + 1
        except (json.JSONDecodeError, TypeError):
            pass
    return freq
```

### Re-Upload Pattern (import from existing script)
```python
# Source: scripts/re_enrich_office_hours.py lines 34-46
import importlib.util
from pathlib import Path

_script = Path(__file__).resolve().parent / "re_enrich_retrieval.py"
_spec = importlib.util.spec_from_file_location("re_enrich_retrieval", _script)
_mod = importlib.util.module_from_spec(_spec)
_spec.loader.exec_module(_mod)

# Available functions:
# _mod.resolve_store_name(client) -> str
# _mod.build_enriched_content_with_header(file_path, conn) -> str | None
# _mod.upload_file_with_poll(client, tmp_path, display_name) -> str
# _mod.import_to_store_with_poll(client, store_name, file_name) -> str
# _mod.delete_old_store_doc(client, store_name, old_store_doc_id) -> bool
# _mod.delete_old_raw_file(client, old_file_id) -> bool
# _mod.process_file(client, store_name, conn, file_info, idx, total, dry_run) -> bool
```

### A7 CRAD Phrase Lookup (for check_stability.py)
```python
# Insert in check_stability.py _check_targeted_searchability() between S1 check and S4a fallback
def _get_crad_phrase(conn: sqlite3.Connection, filename: str) -> str | None:
    """Look up validated CRAD discrimination phrase for a file."""
    row = conn.execute(
        "SELECT phrase FROM file_discrimination_phrases "
        "WHERE filename = ? AND validation_status = 'validated'",
        (filename,),
    ).fetchone()
    return row[0] if row else None

# Usage in A7 (between S1 fail and S4a fallback):
# if not found:
#     crad_phrase = _get_crad_phrase(conn, filename)
#     if crad_phrase:
#         <query Gemini with crad_phrase, check rank>
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| S1-only query | S1 -> S4a -> S4b cascade | Phase 16.5 (2026-02-25) | 96.4% -> 100% retrieval |
| Content-absolute aspects | Corpus-relative differentia (CRAD) | Phase 16.6 (pending) | Deterministic phrases vs stochastic S4a |
| Exclude 333 Episodes + 60 OH from A7 | Zero exclusions, zero tolerance | Phase 16.5 (2026-02-25) | All 1,749 files tested |
| S4a uses corpus frequency for tie-breaking | CRAD uses series frequency (primary) + corpus frequency (secondary) | Phase 16.6 (pending) | Better discrimination within series |

**Key S4a limitation being fixed by CRAD:** S4a sorts by corpus frequency only. For files in large series (58 files in ITOE Advanced Topics), many aspects have corpus freq=1 but series freq=1 as well. S4a misses 28/63 S1-failing files because the tie-breaking (list order) is arbitrary. CRAD fixes this by: (1) computing series-specific genus, (2) using secondary tie-breaking, (3) validating each phrase before committing.

## Open Questions

1. **How to handle "Other" series with 765 files?**
   - What we know: 5 of the 63 S1-failing files come from parent dirs in "Other" (diverse courses). Their parent dirs are specific (e.g., "Perception" with 32 files, "Peikoff Podcast" with 334 files).
   - What's unclear: Whether the CRAD genus computation is meaningful for a 334-file series like "Peikoff Podcast" (Episodes). Genus for 334 files means 80% = 267+ files share an aspect, which may yield zero genus aspects (no aspect appears in 267/334 files).
   - Recommendation: Use parent directory as series. For large series where genus is empty (no aspects reach 80% threshold), all aspects are differentia -- the algorithm degrades gracefully to "pick the 3 rarest aspects" which is equivalent to S4a.

2. **Should the 80% genus threshold be configurable per series?**
   - What we know: For ITOE AT OH (28 files), 80% = 22 files. The actual data shows 0 genus aspects (no aspect appears in 22+ of 28 files). This means EVERY aspect is differentia, and the algorithm becomes pure rarity sorting.
   - What's unclear: Whether a lower threshold (e.g., 50%) would produce meaningful genus subtraction.
   - Recommendation: Keep 80% as designed. When genus is empty, the algorithm still works -- it just skips the subtraction step. The rarity sorting is the real discriminator.

3. **What if CRAD phrase also fails validation (rank > 5)?**
   - What we know: S4b cascade (top-5, individual, aspect+course, course+topic+classnum) found ALL 28 S1+S4a misses. The cascade always has a working query for every file.
   - What's unclear: Whether the CRAD phrase (which picks the top-3 rarest aspects by series+corpus freq) will always succeed at rank <= 5.
   - Recommendation: The plan's escalation path is correct: extend to top-5, add series_name, try individual aspects. Maximum 5 escalations out of 63 is acceptable.

## Sources

### Primary (HIGH confidence)
- `scripts/check_stability.py` -- A7 implementation, Gemini query pattern, rank extraction, S4a fallback (lines 444-694)
- `scripts/re_enrich_retrieval.py` -- Full re-upload pipeline: upload, import, delete old, DB update (lines 150-358)
- `scripts/re_enrich_office_hours.py` -- Proven pattern for importing re_enrich_retrieval.py functions via importlib (lines 34-46)
- `scripts/re_upload_phase164.py` -- Duplicate of re-upload pattern, confirms stability of the approach
- `scripts/retrievability_audit.py` -- S4a/S4b query building, corpus frequency map construction, series detection (lines 95-111, 227-304)
- `src/objlib/upload/header_builder.py` -- Identity header structure (lines 111-129), DB joins for metadata
- `src/objlib/search/client.py` -- GeminiSearchClient with tenacity retry (lines 87-117)
- `data/library.db` -- Verified schema: `file_metadata_ai` has `file_path` not `filename`; `json_extract()` returns TEXT
- `.planning/phases/16.5-strategy4-rarest-aspect-exhaustive-audit/16.5-02-results.json` -- 1,749-file audit, 63 S1 failures identified
- `.planning/phases/16.5-strategy4-rarest-aspect-exhaustive-audit/16.5-02-hit-rates.md` -- Per-series breakdown, S4b cascade stats

### Secondary (MEDIUM confidence)
- `.planning/phases/16.5-strategy4-rarest-aspect-exhaustive-audit/16.5-ROOT-CAUSE.md` -- Root cause analysis, confirmed all 12 failures retrievable with right query

### Verified by Code Execution (HIGH confidence)
- `json_extract()` returns type `str`, parsed by `json.loads()` to get Python list (verified via Python execution)
- ITOE AT OH series has 28 files with metadata, 0 genus aspects at 80% threshold, 240 rare aspects at freq<=2 (verified)
- S1 failures: exactly 63 files across 7 parent directories (verified from 16.5-02-results.json)
- API rate: 1,749 queries in 16.5 min = 1.77/sec with concurrency=10, 0 errors (verified from 16.5-02 metadata)
- Series by parent directory: ITOE Advanced Topics=58, ITOE=48, Objectivist Logic=50, Perception=32, Peikoff Podcast=334, Books=27, MOTM=468 (verified via SQL)

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- all libraries already in use in the codebase
- Architecture: HIGH -- all patterns verified by reading existing scripts
- Pitfalls: HIGH -- Pitfall 2 (tie-breaking) verified by running the actual algorithm on ITOE AT OH data and comparing to known S4a results
- Re-upload pattern: HIGH -- three separate scripts use this exact pattern
- Rate limits: HIGH -- empirical data from 1,749-query audit with zero errors

**Research date:** 2026-02-26
**Valid until:** 2026-03-26 (stable -- internal patterns, no external API changes expected)

---

## Architecture Revision: Claude as Pass 2 Agent

**Decision (2026-02-26):** Pass 2 redesigned to use Claude (Anthropic API) for philosophical discrimination, not pure frequency sorting.

### Rationale: Why Frequency Collapses

The original Pass 2 design sorts a file's non-genus aspects by series frequency (ascending) and takes the top-3 rarest. This fails for the most problematic series in the corpus.

For ITOE AT OH (28 files, the series with the highest S1 failure rate at 71%), the data shows:
- ~280 unique aspects across 28 files
- ~240 of those 280 aspects have series frequency = 1 (appear in exactly 1 file)
- The "rarest" signal carries no discrimination information within this tied set

The naive sort produces an arbitrary ordering at freq=1. "methodology" (freq=1, philosophically generic) is ranked identically to "Zeno's arrow paradox" (freq=1, philosophically specific). The Phase 16.5 root cause analysis confirmed this directly: "order of being vs. order of knowing" (corpus freq=1, series freq=1) MISSED as a query while "Zeno's arrow paradox" (also corpus freq=1, series freq=1) retrieved the target file at rank 1. Both aspects score identically under any frequency-based sort. The difference is philosophical specificity — a property not captured by frequency.

Secondary tie-breaking by corpus frequency (documented in Pitfall 2 above) was considered but also insufficient: both aspects have corpus freq=1, so the secondary sort is also tied. The tertiary sort (word count ascending) would prefer shorter aspects, but "Zeno's arrow paradox" (3 words) and "order of being" (4 words) are both short — and neither the frequency nor the length signal distinguishes them on philosophical utility.

### Implementation

**Function:** `get_claude_discrimination_phrase()` in `scripts/crad_algorithm.py`

**Signature:**
```python
def get_claude_discrimination_phrase(
    target_filename: str,
    target_aspects: list[str],
    series_name: str,
    series_aspects_by_file: dict[str, list[str]],
    series_freq_map: dict[str, int],
    corpus_freq_map: dict[str, int],
    max_retries: int = 2,
) -> dict:  # returns: discrimination_phrase, aspects_used, reasoning, model
```

**Model:** `claude-haiku-4-5-20251001` (Haiku 4.5)

Rationale for Haiku over Sonnet: The discrimination task is well-defined, not open-ended. The
prompt provides the full candidate set (all sibling aspects) and asks Claude to apply a specific
criterion (philosophical specificity). The input context is ~300-700 tokens per call. At 63 total
calls with ~100-150 tokens of output each, Haiku handles this comfortably. The quality gap
between Haiku and Sonnet is negligible for a structured selection task with explicit criteria.
The speed and cost advantages favor Haiku: ~$0.02 total vs ~$0.30 for Sonnet.

**API pattern:** Synchronous `anthropic.Anthropic().messages.create()`. No async needed — 63
sequential calls take ~60-90 seconds total. No batch API needed (Anthropic Batch API has
minimum 100-request overhead; 63 calls don't justify it). The `anthropic` SDK (v0.57.1) is
confirmed installed in the project environment.

**Prompt design (key elements):**

The system prompt instructs Claude to:
1. Apply the Genus Method: select aspects that are differentia (specific to THIS file), not genus (shared across the series)
2. Prefer philosophically SPECIFIC aspects over philosophically generic ones ("Zeno's arrow paradox" > "epistemology")
3. Use the frequency map as a hint about genus membership, not as the sole decision criterion
4. Output ONLY valid JSON: `{"discrimination_phrase": "...", "aspects_used": [...], "reasoning": "..."}`

The user message provides:
- All sibling filenames with their aspects (full series territory — this is what lets Claude see what is shared vs unique)
- Frequency annotations on the target's aspects ([series_freq, corpus_freq] per aspect)
- Explicit instruction to select what makes the TARGET file philosophically distinct from ALL siblings

**JSON output structure:**
```json
{
  "discrimination_phrase": "Zeno's arrow paradox",
  "aspects_used": ["Zeno's arrow paradox"],
  "reasoning": "Zeno's arrow paradox is the distinctive historical content of this session; no other sibling file addresses it, and it names a concrete philosophical problem rather than a general method."
}
```

Pass 3 takes `aspects_used` from this output and applies the ≤7-word budget and markdown
cleaning — same as before. Claude's discrimination_phrase IS the Pass 3 output in most cases;
Pass 3 only truncates or cleans if needed.

### Why This Is Correct

The Genus Method is a reasoning method, not a sorting method. Frequency is a useful signal for
genus identification (Pass 1: aspects appearing in ≥80% of files ARE genus by any standard) but
is insufficient for differentia selection (Pass 2) when many aspects tie at the same frequency.

Pass 1 (frequency computation) remains pure computation — it correctly identifies that
"epistemology", "concept formation", and "Objectivism" appear in every session of a course.
Pass 3 (phrase construction, word budget, markdown cleaning) remains pure computation.

Pass 2 is the philosophical judgment step. When frequency ties, the question is: "Of all the
aspects in this file that are NOT genus, which ones would a philosopher use to uniquely identify
this specific session?" That question requires reasoning about philosophical content, historical
specificity, and conceptual concreteness — all of which Claude can evaluate from the full series
context provided in the prompt.

The frequency map is retained as a HINT in the Claude prompt (it signals what is shared genus),
not as the decision mechanism. Claude uses it to confirm which aspects are non-genus candidates,
then applies philosophical judgment to select the most specific ones.

### Cost and Performance

- Model: `claude-haiku-4-5-20251001`
- Calls: 3 (pilot) / 63 (full run)
- Input tokens: ~400-700 per call (series context)
- Output tokens: ~100-150 per call (JSON)
- Total estimated cost: ~$0.02 for 63 files
- Estimated time: ~60-90 seconds for 63 sequential calls
- No rate limiting needed (well below Haiku API limits)
- `ANTHROPIC_API_KEY` must be set in environment before running
