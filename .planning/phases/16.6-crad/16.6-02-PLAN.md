# Plan 16.6-02: Full CRAD Run — All 63 S1-Failing Files

**Phase:** 16.6 — Corpus-Relative Aspect Differentiation (CRAD)
**Status:** NOT STARTED
**Prerequisite:** Plan 16.6-01 GATE PASSED (all 3 pilot phrases at rank ≤ 5)
**Autonomous:** Yes

---

## Goal

Run the CRAD 3-pass algorithm (implemented in Plan 16.6-01) on all 63 S1-failing files, grouped
by series. Generate discrimination phrases for all 63 files. Update identity headers to include
`Discrimination:` field. Re-upload the 63 modified files via the FSM pipeline. Validate all 63
phrases achieve rank ≤ 5. Confirm store integrity.

---

## Input: The 63 S1-Failing Files

From Phase 16.5-02 hit-rates, per-series breakdown:

| Series       | S1 failures | Notes |
|-------------|------------|-------|
| ITOE AT OH  | 20 files   | 28 total; 71% fail S1 |
| ITOE OH     | 9 files    | 16 total; 56% fail S1 |
| OL          | 11 files   | 50 total; 22% fail S1 |
| ITOE        | 5 files    | 32 total; 16% fail S1 |
| Books       | 2 files    | 27 total; 7% fail S1 |
| Other       | 16 files   | Various; low-density clusters |

**Query to identify all 63:** Run the Phase 16.5 S1 audit query against check_stability.py
results, or re-run a targeted S1 check across the full corpus (--sample-count 1749).

---

## Tasks

### Task 1: Build S1-fail list from DB

Query DB for all indexed files and run S1 check to identify the exact 63 failing files.
Use Phase 16.5 results JSON as the authoritative input to avoid re-querying Gemini.

```python
# Load 16.5 results to identify S1-failing files
import json
with open(".planning/phases/16.5-strategy4-rarest-aspect-exhaustive-audit/16.5-02-results.json") as f:
    results = json.load(f)

s1_failures = [r for r in results if not r["s1_found"]]
# Group by series
from collections import defaultdict
by_series = defaultdict(list)
for r in s1_failures:
    series = r.get("series", "Other")
    by_series[series].append(r["filename"])
```

### Task 2: Run Pass 1 for each series (6 series × 1 Pass 1 each)

For each series in `by_series.keys()`, build the GENUS_PROFILE using the full series (not just
the failing files). Store in `series_genus` table.

Estimated time: ~1 min per series × 6 = ~6 min (pure DB computation, no Gemini API calls).

### Task 3: Run Pass 2 (Claude) + Pass 3 for each of the 63 files

Pass 2 uses Claude (Anthropic API) for philosophical discrimination. Pure frequency sorting
collapses for large series where many aspects tie at series freq=1. Claude evaluates
philosophical specificity within tied sets (see Plan 16.6-01 Task 3 for full rationale,
function signatures, and prompt design).

For each failing file:
1. Load its aspects from `file_metadata_ai`
2. Load ALL sibling file aspects for the series (for the Claude context window)
3. Call `get_claude_discrimination_phrase()` with target aspects, series context, and
   frequency maps (Pass 1 output). Uses `claude-haiku-4-5-20251001`.
4. Run Pass 3 on the `aspects_used` from Claude's response (phrase ≤7 words, markdown-clean)
5. Store candidate phrase in `file_discrimination_phrases`

**Claude API call pattern (synchronous, no batch needed):**
```python
import anthropic
# client = anthropic.Anthropic()  -- reads ANTHROPIC_API_KEY from env
# One call per file, max_tokens=512, sequential with no concurrency needed
```

**Cost estimate:**
- Model: `claude-haiku-4-5-20251001`
- Input tokens per call: ~400-700 (series context grows with series size)
  - ITOE Advanced Topics (58 files × ~8 aspects each): ~600 tokens
  - Smaller series (16-32 files): ~300-400 tokens
- Output tokens per call: ~100-150 (JSON with phrase + aspects + one-sentence reasoning)
- Total calls: 63
- Estimated total input: ~35,000 tokens; total output: ~8,000 tokens
- Haiku pricing (2026): ~$0.25/MTok input, ~$1.25/MTok output
- Estimated cost: ~$0.02 total (well under $1)

**Rate limiting:** No rate limit spacing needed between Claude calls at this volume.
Sequential calls with no sleep. Estimated time: ~30-60 seconds for all 63 files.

Estimated time breakdown:
- Pass 1 (genus profiles, pure DB): ~6 min
- Pass 2 (Claude calls, 63 × ~1 sec): ~1-2 min
- Pass 3 (phrase construction, pure computation): ~5 sec

### Task 4: Validate all 63 phrases against Gemini

For each of the 63 candidate phrases:
1. Query Gemini File Search with the phrase (top_k=20)
2. Find rank of target file
3. If rank ≤ 5: mark `validation_status = 'validated'`, store `validation_rank`
4. If rank > 5 on first try: retry twice more
5. If rank > 5 after 3 tries: mark `validation_status = 'escalated'`, log for manual review

**Escalation threshold:** Any file with rank > 5 after 3 attempts is escalated — the phrase is
stored but flagged. This plan does NOT fail if 1-2 files need manual escalation; it fails only
if more than 5 files (8%) cannot be validated. Escalated files are reviewed before Plan 16.6-03.

**Rate limiting:** Space validation queries 2 seconds apart to avoid Gemini rate limits.
Use `asyncio.sleep(2)` between queries. Total validation time: ~63 × 2s = ~126 sec.

### Task 5: Escalation handling for borderline files

For each escalated file:
1. Log the phrase, the aspects used, and the ranks from all 3 attempts
2. Apply fallback strategy: extend to top-5 rarest aspects (up to 7 words)
3. Re-validate the extended phrase
4. If still > rank 5: add series_name as first token and retry
5. If still > rank 5 after all attempts: document in `16.6-02-ESCALATIONS.md` for manual review

### Task 6: Update identity headers for all validated files

For each file with `validation_status = 'validated'`:
1. Read the current identity header structure from `header_builder.py`
2. Confirm `Discrimination:` field does NOT already exist (guard against double-insertion)
3. Add `Discrimination:` field after `Aspects:` field in the header template

**No change to `header_builder.py` yet** — that happens in Plan 16.6-03 (permanent schema update).
For this plan, build modified headers inline in the upload script.

```python
def build_discrimination_header_addition(phrase: str) -> str:
    return f"Discrimination: {phrase}\n"
```

### Task 7: Re-upload 63 files via FSM pipeline

Use `re_enrich_retrieval.py` pattern (from Phase 16.3-03) to re-upload with updated headers:
- Upload new version of each file to Gemini Files API
- Import into store (creates new store document)
- Update DB: `gemini_file_id`, `gemini_store_doc_id`, `gemini_state = 'indexed'`
- Do NOT use `--reset-existing` flag

**Important:** The re-upload creates new store documents. After re-uploading all 63 files, run
`store-sync --no-dry-run` to delete the 63 stale store documents from the previous upload version.

Expected orphan count after re-upload: exactly 63 (one per re-uploaded file).

### Task 8: Store integrity verification

After re-upload and store-sync:

```bash
python -m objlib store-sync --store objectivism-library
# Expected: DB=1,749, Store=1,749, Orphans=0
```

Also verify: the 3 pilot files now have updated store documents with `Discrimination:` field
visible in their indexed content (check via `python -m objlib view FILENAME --show-related`).

### Task 9: Document results

Create `16.6-02-RESULTS.md` with:
- Total files processed: 63
- Per-series breakdown: phrases generated, validation ranks
- Escalated files: count, filenames, reason, final resolution
- Store integrity: DB=1,749, Store=1,749, Orphans=0
- Representative sample: 5 phrases with their validation ranks

---

## Validation Gates

- [ ] All 63 files have entries in `file_discrimination_phrases`
- [ ] ≤5 files marked `validation_status = 'escalated'`; all others `'validated'`
- [ ] All validated files have `validation_rank ≤ 5`
- [ ] All escalated files have documented manual resolutions or are queued for Plan 16.6-03
- [ ] 63 files re-uploaded with `Discrimination:` field in identity header
- [ ] `store-sync`: DB=1,749, Store=1,749, Orphans=0
- [ ] `16.6-02-RESULTS.md` committed with per-series breakdown

---

## Files Created/Modified

- `scripts/crad_full_run.py` — full-run script (extends crad_algorithm.py from Plan 16.6-01)
- `.planning/phases/16.6-crad/16.6-02-RESULTS.md` — full run results
- `.planning/phases/16.6-crad/16.6-02-ESCALATIONS.md` — escalated files (if any)
- `data/library.db` — 63 rows in `file_discrimination_phrases`; 63 files re-uploaded (new gemini_store_doc_id)
