---
phase: 04-quality-enhancements
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/objlib/search/synthesizer.py
autonomous: true

must_haves:
  truths:
    - "Synthesis produces multi-paragraph answer with inline citations from 5-10 diverse passages"
    - "Each claim has a verbatim quote that is an exact substring of the source passage text"
    - "MMR diversity filter limits to max 2 passages per file and prefers distinct courses"
    - "Synthesis falls back to labeled excerpts when fewer than 5 passages or validation fails"
  artifacts:
    - path: "src/objlib/search/synthesizer.py"
      provides: "Synthesis pipeline with MMR diversity and citation validation"
      exports: ["synthesize_answer", "apply_mmr_diversity", "validate_citations"]
      min_lines: 120
  key_links:
    - from: "src/objlib/search/synthesizer.py"
      to: "google.genai"
      via: "generate_content with response_schema=SynthesisOutput"
      pattern: "response_schema.*SynthesisOutput"
    - from: "src/objlib/search/synthesizer.py"
      to: "src/objlib/search/models.py"
      via: "import SynthesisOutput, Claim, CitationRef"
      pattern: "from objlib\\.search\\.models import"
---

<objective>
Build the multi-document synthesis pipeline that takes reranked passages, applies MMR diversity filtering, generates a structured synthesis via Gemini Flash with claim-level citations, and validates every quote against source text.

Purpose: Transforms raw search excerpts into a coherent, cited answer that synthesizes knowledge across 5-10 diverse sources. Citation validation ensures philosophical attribution integrity -- every claim traces to a verbatim quote from the source material.
Output: synthesizer.py module with synthesize_answer(), apply_mmr_diversity(), and validate_citations() functions.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-quality-enhancements/04-CONTEXT.md
@.planning/phases/04-quality-enhancements/CLARIFICATIONS-ANSWERED.md
@.planning/phases/04-quality-enhancements/04-RESEARCH.md
@src/objlib/models.py
@src/objlib/search/client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: MMR diversity filter and citation validation</name>
  <files>src/objlib/search/synthesizer.py</files>
  <action>
Create `src/objlib/search/synthesizer.py` with the following pure-logic functions (no API calls in this task):

**Function 1: `apply_mmr_diversity`**
```python
def apply_mmr_diversity(
    citations: list[Citation],
    max_per_file: int = 2,
    max_results: int = 10,
) -> list[Citation]:
```

Per locked decision (Q2): Maximal Marginal Relevance — max 2 passages per file, prefer distinct courses.

Implementation (adapted from research findings):
1. Build `file_counts: dict[str, int]` and `course_seen: set[str]`.
2. First pass: iterate citations in order. For each, get file_id from `citation.file_path or citation.title` and course from `(citation.metadata or {}).get("course", "")`. If file_counts[file_id] == 0 AND course not in course_seen, add to result, increment file_counts, add course to course_seen.
3. Second pass: iterate again. For each citation not yet in result, if file_counts[file_id] < max_per_file and len(result) < max_results, add it.
4. Return result[:max_results].

**Function 2: `validate_citations`**
```python
def validate_citations(
    claims: list[Claim],
    passage_texts: dict[str, str],
) -> tuple[list[Claim], list[str]]:
```

Per locked decision (Q3): exact substring validation with whitespace normalization.

Implementation:
1. For each claim, look up its `claim.citation.passage_id` in passage_texts dict.
2. Normalize both the quote and the passage text: collapse whitespace (`" ".join(text.split())`), lowercase both.
3. Check if normalized quote is a substring of normalized passage text.
4. If valid, keep the claim. If invalid, add an error message to the errors list: `f"Quote not found in passage {claim.citation.passage_id}: '{claim.citation.quote[:50]}...'"`.
5. Return (valid_claims, errors).

**Function 3: `_build_passage_map`**
```python
def _build_passage_map(citations: list[Citation]) -> dict[str, str]:
```
Helper that builds a dict mapping `f"{citation.file_path or citation.title}:{citation.index}"` -> citation.text for all citations. This is used to look up passage text for validation. The key format matches how passage_ids are constructed in the synthesis prompt.

Add imports at top:
```python
from __future__ import annotations
import logging
import re
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from objlib.models import Citation
from objlib.search.models import Claim, SynthesisOutput, CitationRef
```
  </action>
  <verify>
Run `python -c "
from objlib.models import Citation
from objlib.search.synthesizer import apply_mmr_diversity, validate_citations
from objlib.search.models import Claim, CitationRef

# Test MMR diversity
citations = [
    Citation(1, 'file1.txt', None, 'text1', None, 0.9, 'path1', {'course': 'OPAR'}),
    Citation(2, 'file1.txt', None, 'text2', None, 0.8, 'path1', {'course': 'OPAR'}),
    Citation(3, 'file1.txt', None, 'text3', None, 0.7, 'path1', {'course': 'OPAR'}),
    Citation(4, 'file2.txt', None, 'text4', None, 0.6, 'path2', {'course': 'ITOE'}),
]
diverse = apply_mmr_diversity(citations)
file_counts = {}
for c in diverse:
    k = c.file_path or c.title
    file_counts[k] = file_counts.get(k, 0) + 1
print(f'Max per file: {max(file_counts.values())}')  # Should be <= 2
print(f'Diverse count: {len(diverse)}')

# Test citation validation
claims = [
    Claim(claim_text='Test claim', citation=CitationRef(file_id='f1', passage_id='p1', quote='exact text from the passage here')),
]
passages = {'p1': 'This is the exact text from the passage here in context'}
valid, errors = validate_citations(claims, passages)
print(f'Valid: {len(valid)}, Errors: {len(errors)}')  # Should be 1, 0
"` — MMR should cap at 2 per file; validation should match exact substrings.
  </verify>
  <done>MMR diversity filter caps at 2 passages per file and prefers distinct courses. Citation validation checks exact substring match with whitespace normalization.</done>
</task>

<task type="auto">
  <name>Task 2: Gemini Flash synthesis pipeline with re-prompt on validation failure</name>
  <files>src/objlib/search/synthesizer.py</files>
  <action>
Add the main `synthesize_answer` function to `src/objlib/search/synthesizer.py` (same file as Task 1).

```python
def synthesize_answer(
    client,  # genai.Client
    query: str,
    citations: list[Citation],
    model: str = "gemini-2.0-flash",
) -> SynthesisOutput | None:
```

Implementation per locked decisions (Q2, Q3):

1. **Minimum check:** If len(citations) < 5, return None (caller shows "Insufficient sources for synthesis" and falls back to excerpts).

2. **Build passage map:** Create a dict mapping passage_id -> passage_text. Use `f"{i}"` as passage_id (0-based index into the citations list). Build the prompt context listing each passage:
   ```
   Passage 0 [file: "filename.txt", course: "OPAR", difficulty: "introductory"]:
   "passage text here..."

   Passage 1 [file: "filename2.txt", course: "ITOE"]:
   ...
   ```

3. **System instruction:** "You are a scholarly synthesizer specializing in Objectivist philosophy. Given a query and numbered source passages, produce a synthesis of 150-300 words structured as a list of factual claims. Each claim must be a single sentence making one factual assertion, supported by a verbatim quote (20-60 words) from one of the provided passages. The quote must be copied EXACTLY from the passage text — do not paraphrase, rephrase, or modify it in any way. You may include one optional bridging_intro sentence and one optional bridging_conclusion sentence that contain no factual assertions. When sources disagree, attribute explicitly: 'Source A states X; Source B states Y.' Never silently merge conflicting claims."

4. **First attempt:** Call `client.models.generate_content()` with:
   - model=model
   - contents=f"Query: {query}\n\nSources:\n{passage_context}"
   - config=GenerateContentConfig(response_mime_type="application/json", response_schema=SynthesisOutput)

5. **Parse response:** Parse JSON into SynthesisOutput.

6. **Validate citations:** Call `validate_citations(output.claims, passage_map)`.

7. **If validation errors and this is first attempt:** Re-prompt once with error feedback:
   - Append to the prompt: f"\n\nPREVIOUS ATTEMPT HAD CITATION ERRORS. The following quotes were NOT exact substrings of the source passages:\n{errors}\n\nPlease re-generate with EXACT verbatim quotes copied directly from the passages."
   - Call generate_content again.
   - Validate again.

8. **If second attempt also fails:** Log warning, return None (caller falls back to excerpts per Q3b).

9. **On any API/parse exception:** Log warning, return None (graceful degradation per Q9).

Map each claim's citation.passage_id to the actual passage index used in the prompt so validation can look up the right text.

Important: Use `from google.genai import types` for `GenerateContentConfig`. Import `SynthesisOutput` from `objlib.search.models`.
  </action>
  <verify>
Run `python -c "from objlib.search.synthesizer import synthesize_answer, apply_mmr_diversity, validate_citations; print('All synthesizer exports OK')"` — should import without error.
  </verify>
  <done>synthesize_answer calls Gemini Flash with SynthesisOutput schema, validates citations as exact substrings, re-prompts once on failure, returns None for graceful fallback to excerpts.</done>
</task>

</tasks>

<verification>
- `python -c "from objlib.search.synthesizer import synthesize_answer, apply_mmr_diversity, validate_citations; print('OK')"` passes
- MMR diversity: given 5 citations from 2 files, returns max 2 per file
- Citation validation: exact substring passes, modified quote fails
- synthesize_answer returns None when fewer than 5 citations (insufficient sources)
</verification>

<success_criteria>
- apply_mmr_diversity limits to max 2 passages per file, prefers distinct courses
- validate_citations checks exact substring match (whitespace-normalized, case-insensitive)
- synthesize_answer uses Gemini Flash with SynthesisOutput Pydantic schema
- Re-prompts once on validation failure, returns None on second failure
- Returns None for fewer than 5 passages (graceful degradation)
- All API failures caught and logged (graceful degradation)
</success_criteria>

<output>
After completion, create `.planning/phases/04-quality-enhancements/04-03-SUMMARY.md`
</output>
