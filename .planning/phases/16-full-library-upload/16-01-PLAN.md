---
phase: 16-full-library-upload
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/objlib/upload/orchestrator.py
  - src/objlib/cli.py
  - src/objlib/tui/__init__.py
autonomous: true

must_haves:
  truths:
    - "run_fsm() processes all ~1,658 untracked .txt files (not capped at 50)"
    - "RecoveryCrawler runs at startup before new uploads begin"
    - "429 RateLimitError triggers in-place retry with exponential backoff, not immediate FAILED transition"
    - "All ~1,748 .txt files reach gemini_state='indexed' after full remediation loop"
    - "check_stability.py T=0 reports STABLE (exit 0) with --sample-count 20"
    - "store-sync dry-run confirms ~1,748 canonical docs and 0 orphaned docs"
    - "store-sync default store name is objectivism-library (not objectivism-library-test)"
  artifacts:
    - path: "src/objlib/upload/orchestrator.py"
      provides: "FSM upload with 429 retry, RecoveryCrawler, uncapped limit"
      contains: "MAX_429_RETRIES"
    - path: "src/objlib/cli.py"
      provides: "store-sync default store name fix"
      contains: "objectivism-library"
    - path: "src/objlib/tui/__init__.py"
      provides: "TUI default store name fix"
      contains: "objectivism-library"
  key_links:
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/recovery.py"
      via: "RecoveryManager.run() + RecoveryCrawler.recover_all() at start of run_fsm()"
      pattern: "RecoveryManager|RecoveryCrawler"
    - from: "src/objlib/upload/orchestrator.py"
      to: "asyncio.sleep"
      via: "429 in-place retry with exponential backoff"
      pattern: "RateLimitError.*retry|MAX_429_RETRIES"
---

<objective>
Fix three critical bugs in FSMUploadOrchestrator, fix store name defaults, then execute the full ~1,658-file production upload with post-upload remediation and T=0 stability check.

Purpose: This is the core execution plan for Phase 16 -- scaling from 50-file proxy to full 1,748-file library. Three blocking code bugs (limit cap, missing RecoveryCrawler, 429 handling) must be fixed before the upload can succeed at scale.

Output: All ~1,748 .txt files at gemini_state='indexed', T=0 STABLE, store-sync clean.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-full-library-upload/16-CONTEXT.md
@.planning/phases/16-full-library-upload/16-RESEARCH.md
@src/objlib/upload/orchestrator.py
@src/objlib/upload/recovery.py
@src/objlib/cli.py
@src/objlib/tui/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix three critical bugs and store name defaults</name>
  <files>
    src/objlib/upload/orchestrator.py
    src/objlib/cli.py
    src/objlib/tui/__init__.py
  </files>
  <action>
Four surgical code changes, all in existing files:

**Bug 1: run_fsm() limit cap (orchestrator.py line 1045)**
Change `limit = self._file_limit if self._file_limit > 0 else 50` to `limit = self._file_limit if self._file_limit > 0 else 10000`. This ensures `--limit 0` processes all untracked files instead of capping at 50. The parent classes already use `else 10000`.

**Bug 2: run_fsm() missing RecoveryCrawler (orchestrator.py, after line 1027)**
After `self.setup_signal_handlers()` and BEFORE "Step 1: Ensure store exists" (line 1029), add recovery calls:
```python
# Step 0: Run crash recovery (RecoveryManager + RecoveryCrawler)
from objlib.upload.recovery import RecoveryManager, RecoveryCrawler
recovery = RecoveryManager(self._client, self._state, self._config)
recovery_result = await recovery.run()
if recovery_result.recovered_operations > 0 or recovery_result.reset_to_pending > 0:
    logger.info(
        "Recovery: %d ops recovered, %d files reset to pending",
        recovery_result.recovered_operations,
        recovery_result.reset_to_pending,
    )

# RecoveryCrawler for FSM-specific write-ahead intents
crawler = RecoveryCrawler(self._state, self._client)
recovered, occ_failures = await crawler.recover_all()
if recovered:
    logger.info("RecoveryCrawler recovered %d files", len(recovered))
if occ_failures:
    logger.warning("RecoveryCrawler OCC failures: %d files (will retry next startup)", len(occ_failures))
```

**Bug 3: 429 in-place retry in _upload_fsm_file() (orchestrator.py lines 1294-1356)**
Replace the current upload + RateLimitError handler block with a retry loop per locked decision #1. The current code at lines 1294-1340 does `async with self._upload_semaphore: file_obj, operation = await self._client.upload_and_import(...)`. Wrap this in a retry loop:

```python
import random

MAX_429_RETRIES = 5
BASE_DELAY = 1.0
MAX_DELAY = 60.0

# Upload with 429 in-place retry (locked decision #1)
file_obj = None
operation = None
for attempt in range(MAX_429_RETRIES):
    try:
        async with self._upload_semaphore:
            file_obj, operation = await self._client.upload_and_import(
                upload_path, display_name, custom_metadata
            )
        break  # Success -- exit retry loop
    except RateLimitError as exc:
        if attempt == MAX_429_RETRIES - 1:
            # Budget exhausted -- fall through to the existing generic Exception handler
            logger.error(
                "429 retry budget exhausted for %s after %d attempts: %s",
                file_path, MAX_429_RETRIES, exc,
            )
            raise  # Will be caught by the Exception handler below
        delay = min(BASE_DELAY * (2 ** attempt), MAX_DELAY)
        jittered = random.random() * delay  # full jitter
        logger.warning(
            "429 rate limit on %s (attempt %d/%d), retrying in %.1fs",
            file_path, attempt + 1, MAX_429_RETRIES, jittered,
        )
        await asyncio.sleep(jittered)
```

Then REMOVE the separate `except RateLimitError` handler at lines 1347-1356 entirely (it is now handled inside the retry loop; exhausted retries raise into the generic `except Exception` handler which already transitions to FAILED correctly).

Define `MAX_429_RETRIES`, `BASE_DELAY`, `MAX_DELAY` as module-level constants at the top of orchestrator.py (near other constants/imports), and add `import random` to the imports.

**Store name fix 1: store-sync default (cli.py line 419)**
Change the `store_sync` command's default store name from `"objectivism-library-test"` to `"objectivism-library"`.

**Store name fix 2: TUI default (tui/__init__.py line 11)**
Change `DEFAULT_STORE_NAME = "objectivism-library-test"` to `DEFAULT_STORE_NAME = "objectivism-library"`.

Run `pytest tests/ -x -q` after all changes to verify no regressions.
  </action>
  <verify>
1. `grep -n "else 10000" src/objlib/upload/orchestrator.py` confirms limit fix
2. `grep -n "RecoveryCrawler" src/objlib/upload/orchestrator.py` shows recovery in run_fsm()
3. `grep -n "MAX_429_RETRIES" src/objlib/upload/orchestrator.py` shows retry constant
4. `grep -n "objectivism-library-test" src/objlib/cli.py src/objlib/tui/__init__.py` returns NO matches
5. `pytest tests/ -x -q` passes (all existing tests green)
  </verify>
  <done>
All three critical bugs fixed (limit cap, RecoveryCrawler, 429 handling). Store name defaults updated. Test suite passes. Code is ready for full-scale upload.
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute full library upload and post-upload remediation</name>
  <files></files>
  <action>
This task executes the production upload and remediation loop. No code changes -- purely operational execution.

**Step 1: Record exact file count denominator (locked decision #8)**
```bash
python -m objlib status
```
Record the exact untracked .txt count and total .txt count in the SUMMARY. Expected: ~1,658 untracked, ~90 indexed, ~1,748 total .txt.

**Step 2: Execute full library upload**
```bash
python -m objlib fsm-upload --store objectivism-library --concurrency 10 --limit 2000 --batch-size 10
```
This will process all ~1,658 untracked files. Use `--limit 2000` (not `--limit 0`) as a safety margin above the expected count. Monitor output for:
- Total files found
- Batch progress
- Any 429 retry messages (expected; confirms the fix is working)
- Failed file count
- Upload complete summary

The upload will take approximately 2-4 hours. Record the full summary output.

**Step 3: Post-upload cooldown (locked decision #5)**
```bash
sleep 60
```

**Step 4: store-sync dry-run**
```bash
python -m objlib store-sync --store objectivism-library --dry-run
```
Record output. Expected: ~1,748 canonical docs, 0 orphaned. If orphans found, proceed to step 5.

**Step 5: store-sync actual (if orphans in step 4)**
```bash
python -m objlib store-sync --store objectivism-library --no-dry-run --yes
```

**Step 6: check_stability T=0 (locked decision #4)**
```bash
python scripts/check_stability.py --store objectivism-library --sample-count 20 --verbose
```
Must exit 0 (STABLE). Record FULL output verbatim for next session comparison.

**Step 7: Remediation loop (if any FAILED files remain)**
If check_stability shows non-zero FAILED or the upload summary shows failures:
```bash
# Re-run fsm-upload which picks up untracked files (retry_failed_file already reset FAILED -> UNTRACKED during batch retry pass)
python -m objlib fsm-upload --store objectivism-library --concurrency 10 --limit 2000 --batch-size 10
```
Then repeat steps 3-6. Success criterion: zero non-indexed AFTER this remediation loop (locked decision #2).

**Step 8: Verify no [Unresolved file #N]**
Run DB query to confirm all .txt files are indexed:
```bash
python -c "
import sqlite3
conn = sqlite3.connect('data/library.db')
c = conn.cursor()
c.execute(\"SELECT gemini_state, COUNT(*) FROM files WHERE filename LIKE '%.txt' GROUP BY gemini_state\")
for row in c.fetchall(): print(row)
c.execute(\"SELECT COUNT(*) FROM files WHERE filename LIKE '%.txt' AND gemini_state != 'indexed'\")
print(f'Non-indexed .txt files: {c.fetchone()[0]}')
conn.close()
"
```
Non-indexed count must be 0.
  </action>
  <verify>
1. `objlib status` output recorded with exact file counts
2. Upload summary recorded (succeeded/failed/skipped/total)
3. store-sync dry-run shows ~1,748 canonical, 0 orphaned
4. check_stability T=0 exits 0 (STABLE), full output recorded verbatim
5. DB query confirms 0 non-indexed .txt files
  </verify>
  <done>
All ~1,748 .txt files have gemini_state='indexed' with valid gemini_store_doc_id. check_stability T=0 STABLE. store-sync clean (0 orphans). Full output recorded in SUMMARY for temporal checkpoint comparison.
  </done>
</task>

</tasks>

<verification>
1. Three code bugs confirmed fixed (grep verification)
2. Store name defaults updated to objectivism-library
3. Test suite passes
4. Upload summary: all .txt files processed
5. store-sync: ~1,748 canonical, 0 orphans
6. check_stability T=0: STABLE (7/7 assertions pass, --sample-count 20)
7. DB audit: 0 non-indexed .txt files
</verification>

<success_criteria>
Phase 16 SC1: All ~1,748 .txt files have gemini_state='indexed' with valid gemini_store_doc_id after remediation loop.
Phase 16 SC2 (partial): check_stability STABLE at T=0 (T+4h/T+24h/T+36h are in plan 16-02).
Phase 16 SC3: store-sync dry-run confirms ~1,748 canonical docs, 0 orphaned docs.
Phase 16 SC5: 50-file proxy assumption (A11) validated -- no new failure modes at full scale (or documented and resolved).
</success_criteria>

<output>
After completion, create `.planning/phases/16-full-library-upload/16-01-SUMMARY.md`

CRITICAL: Record ALL check_stability output verbatim in SUMMARY. The T+4h session needs this as the T=0 baseline for delta comparison.
</output>
