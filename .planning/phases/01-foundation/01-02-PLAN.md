---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/objlib/metadata.py
  - src/objlib/scanner.py
autonomous: true

must_haves:
  truths:
    - "Simple filename pattern extracts course, lesson number, and topic correctly"
    - "Complex filename pattern extracts course, year, quarter, week, and topic correctly"
    - "Files that match no pattern get metadata_quality='minimal' or 'none' and are still recorded"
    - "Scanner discovers all .txt files recursively following symlinks with cycle detection"
    - "Scanner skips hidden files, files below 1KB, and non-allowed extensions"
    - "Change detection correctly identifies new, modified, deleted, and unchanged files"
  artifacts:
    - path: "src/objlib/metadata.py"
      provides: "MetadataExtractor class with pre-compiled regex and quality grading"
      exports: ["MetadataExtractor"]
    - path: "src/objlib/scanner.py"
      provides: "FileScanner class with discovery, hashing, change detection"
      exports: ["FileScanner", "ChangeSet"]
  key_links:
    - from: "src/objlib/metadata.py"
      to: "src/objlib/models.py"
      via: "imports MetadataQuality, FileRecord"
      pattern: "from objlib\\.models import"
    - from: "src/objlib/scanner.py"
      to: "src/objlib/database.py"
      via: "uses Database for persistence and change detection"
      pattern: "from objlib\\.database import"
    - from: "src/objlib/scanner.py"
      to: "src/objlib/metadata.py"
      via: "uses MetadataExtractor to parse each discovered file"
      pattern: "from objlib\\.metadata import"
    - from: "src/objlib/scanner.py"
      to: "os.walk"
      via: "directory traversal with followlinks and cycle detection"
      pattern: "os\\.walk.*followlinks"
---

<objective>
Implement the metadata extraction engine and file scanner with change detection for the Objectivism Library.

Purpose: This plan builds the core scanning pipeline -- discovering files, extracting metadata from folder hierarchy and filenames, computing content hashes, and detecting changes between scans. This covers FOUN-02 (file scanner), FOUN-03 (hash-based change detection), FOUN-04 (metadata from folders), FOUN-05 (metadata from filenames), and FOUN-06 (idempotency via content hashes).

Output: Working MetadataExtractor and FileScanner classes that can scan a directory tree, extract rich metadata, compute hashes, and detect new/modified/deleted files against the database.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/CLARIFICATIONS-ANSWERED.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement MetadataExtractor with regex patterns and quality grading</name>
  <files>
    src/objlib/metadata.py
  </files>
  <action>
    Create `MetadataExtractor` class in `src/objlib/metadata.py`:

    **Class-level pre-compiled regex patterns (per research, with CRITICAL corrections):**
    ```python
    # CRITICAL: Use \d+ not \d{2} -- handles single-digit, double-digit, and triple-digit lesson numbers
    SIMPLE_PATTERN = re.compile(
        r'^(?P<course>.+?) - Lesson (?P<lesson>\d+) - (?P<topic>.+?)\.txt$'
    )
    COMPLEX_PATTERN = re.compile(
        r'^(?P<course>.+?) - Year (?P<year>\d+) - Q(?P<quarter>\d+) - Week (?P<week>\d+) - (?P<topic>.+?)\.txt$'
    )
    # Folder detection for complex pattern: Year{N}/Q{N}/ subfolders
    YEAR_FOLDER = re.compile(r'^Year\s*(\d+)$')
    QUARTER_FOLDER = re.compile(r'^Q(\d+)$')
    ```

    **Constructor `__init__(self, metadata_mappings: dict | None = None)`:**
    - Store metadata_mappings (from config/metadata_mappings.json) for course-level enrichment
    - Initialize logging

    **`extract(self, file_path: Path, library_root: Path) -> tuple[dict, MetadataQuality]`:**
    - Main entry point. Returns `(metadata_dict, quality)`.
    - Compute relative path from library_root
    - Extract folder-level metadata: `_extract_folder_metadata(relative_path)`
    - Extract filename-level metadata: `_extract_filename_metadata(file_path.name)`
    - Merge folder + filename metadata
    - Enrich with course-level mappings if available
    - Grade quality based on how many fields were extracted
    - Return `(metadata_dict, quality)`

    **`_extract_folder_metadata(self, relative_path: Path) -> dict`:**
    - Parse folder components between library_root and filename
    - Detect if inside `Courses/` directory
    - For simple pattern: course name = parent folder name (e.g., `Courses/Induction in Physics/file.txt` -> course = "Induction in Physics")
    - For complex pattern: detect Year{N}/Q{N}/ folders, course = grandparent above Year folder
    - Also detect non-course categories (Books, MOTM, Podcasts) from top-level folder name
    - Store `category` field: "course", "book", "motm", "podcast", or "unknown"
    - Return dict with extracted fields (course, category, year, quarter from folders)

    **`_extract_filename_metadata(self, filename: str) -> dict`:**
    - Try COMPLEX_PATTERN first (more specific, avoids false matches)
    - If matches: extract course, year, quarter, week, topic
    - If not: try SIMPLE_PATTERN
    - If matches: extract course, lesson_number, topic
    - If neither matches: return minimal metadata with just the raw filename and `_unparsed_filename` flag
    - Strip `.txt` extension from topic if present
    - Clean up topic: replace underscores with spaces, strip whitespace

    **`_grade_quality(self, metadata: dict) -> MetadataQuality`:**
    - `COMPLETE`: has course + (lesson_number or (year + quarter + week)) + topic
    - `PARTIAL`: has course + topic (missing structural fields)
    - `MINIMAL`: has course OR topic (but not both)
    - `NONE`: no recognizable fields extracted
    - Never returns UNKNOWN (that's only for unprocessed files)

    **`_enrich_from_mappings(self, metadata: dict) -> dict`:**
    - If course name found in metadata_mappings courses dict, merge in difficulty, instructor, etc.
    - Does not overwrite extracted fields, only adds missing ones

    **JSON serialization:**
    - Return metadata as a dict suitable for `json.dumps()` storage in metadata_json column
    - Include all extracted fields: course, lesson_number, topic, year, quarter, week, category, difficulty, instructor
    - Include `_unparsed_filename` and `_unparsed_folder` flags if extraction failed

    IMPORTANT per user decisions:
    - Always create a record even if extraction fails completely
    - Log extraction failures via return value (caller logs to _extraction_failures table)
    - Lesson regex uses `\d+` NOT `\d{2}` (CRITICAL research correction)
  </action>
  <verify>
    Run from project root:
    ```bash
    python -c "
    from objlib.metadata import MetadataExtractor
    from objlib.models import MetadataQuality
    from pathlib import Path

    ext = MetadataExtractor()

    # Test simple pattern (99% of files)
    meta, quality = ext.extract(
        Path('/lib/Courses/Induction in Physics and Philosophy/Induction in Physics and Philosophy - Lesson 01 - The Axioms of Induction, Part 1.txt'),
        Path('/lib')
    )
    assert meta['course'] == 'Induction in Physics and Philosophy', f'Got: {meta.get(\"course\")}'
    assert meta['lesson_number'] == '01' or meta['lesson_number'] == 1, f'Got: {meta.get(\"lesson_number\")}'
    assert 'Axioms of Induction' in meta['topic'], f'Got: {meta.get(\"topic\")}'
    assert quality == MetadataQuality.COMPLETE, f'Got: {quality}'

    # Test complex pattern (1% of files)
    meta2, quality2 = ext.extract(
        Path('/lib/Courses/Objectivism Seminar - Foundations/Year1/Q1/Objectivism Seminar - Foundations - Year 1 - Q1 - Week 1 - Objectivism and Its View of Philosophy.txt'),
        Path('/lib')
    )
    assert meta2['year'] == '1' or meta2['year'] == 1, f'Got: {meta2.get(\"year\")}'
    assert meta2['quarter'] == '1' or meta2['quarter'] == 1, f'Got: {meta2.get(\"quarter\")}'
    assert meta2['week'] == '1' or meta2['week'] == 1, f'Got: {meta2.get(\"week\")}'
    assert quality2 == MetadataQuality.COMPLETE, f'Got: {quality2}'

    # Test unknown file (graceful degradation)
    meta3, quality3 = ext.extract(
        Path('/lib/random_file.txt'),
        Path('/lib')
    )
    assert quality3 in (MetadataQuality.MINIMAL, MetadataQuality.NONE), f'Got: {quality3}'

    print('All metadata extraction tests passed')
    "
    ```
  </verify>
  <done>
    - Simple pattern correctly extracts course, lesson_number, topic from dominant filename format
    - Complex pattern correctly extracts course, year, quarter, week, topic from hierarchical format
    - Folder-level extraction detects course name from parent directory
    - Quality grading assigns COMPLETE/PARTIAL/MINIMAL/NONE based on extracted field count
    - Unrecognized files get MINIMAL or NONE quality (never crash, never skip)
    - Regex uses `\d+` for lesson numbers (not `\d{2}`)
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement FileScanner with discovery, hashing, and change detection</name>
  <files>
    src/objlib/scanner.py
  </files>
  <action>
    Create the `FileScanner` class and `ChangeSet` dataclass in `src/objlib/scanner.py`:

    **`ChangeSet` dataclass:**
    ```python
    @dataclass
    class ChangeSet:
        new: set[str]         # Paths of files not in DB
        modified: set[str]    # Paths where content_hash differs
        deleted: set[str]     # Paths in DB but not on disk
        unchanged: set[str]   # Paths where hash matches
    ```

    **`FileScanner.__init__(self, config: ScannerConfig, db: Database, metadata_extractor: MetadataExtractor)`:**
    - Store config, db, and metadata_extractor
    - Initialize logging

    **`discover_files(self) -> list[Path]`:**
    - Use `os.walk(str(config.library_path), followlinks=config.follow_symlinks)` per research
    - Implement symlink cycle detection with `visited_inodes: set[tuple[int, int]]`:
      - Before walk: add root's (st_dev, st_ino) to visited set
      - For each dirpath: stat it, check (st_dev, st_ino) in visited set
      - If cycle detected: log warning, call `dirnames.clear()`, continue
      - Otherwise: add to visited set
    - Prune directories IN-PLACE (`dirnames[:] = ...`) to skip:
      - Hidden directories (starting with `.`) when `config.skip_hidden` is True
      - Directories matching `config.skip_patterns` (e.g., `__pycache__`)
    - For each filename: filter by:
      - Skip hidden files (starting with `.`)
      - Check extension: `Path(filename).suffix.lower() in config.allowed_extensions` (case-insensitive, per research pitfall 6)
      - Check file size: `os.path.getsize(full_path) >= config.min_file_size`
      - Log skipped files to database via `db.log_skipped_file()` with reason
    - Return list of Path objects for matched files

    **`compute_hash(file_path: Path, buf_size: int = 65536) -> str`:** (static method)
    - SHA-256 streaming hash per research code example
    - Return hex digest as string
    - Handle permission errors gracefully (log warning, return empty string)

    **`scan(self) -> ChangeSet`:**
    - Main orchestration method:
    1. Call `discover_files()` to get list of paths
    2. For each file: compute hash, extract metadata via `self.metadata_extractor.extract()`
    3. Build dict of `{str(file_path): FileRecord}` from scan results
    4. Call `detect_changes()` to compare against DB state
    5. For new + modified files: build FileRecord objects, call `db.upsert_files()`
    6. For deleted files: call `db.mark_deleted()`
    7. For extraction failures: call `db.log_extraction_failure()`
    8. Return ChangeSet with counts
    - Use Rich Progress bar for user feedback during scan (if available, graceful fallback to logging)

    **`detect_changes(self, scan_results: dict[str, FileRecord]) -> ChangeSet`:**
    - Get DB state via `db.get_all_active_files()`
    - Use set operations per research pattern:
      - `new = scan_paths - db_paths`
      - `deleted = db_paths - scan_paths`
      - `common = scan_paths & db_paths`
      - `modified = {p for p in common if scan_results[p].content_hash != db_files[p][0]}`
      - `unchanged = common - modified`
    - Return ChangeSet

    **Idempotency guarantee (FOUN-06):**
    - The UPSERT in database.py + the ChangeSet detection ensures that:
      - Running scan twice on unchanged library produces: new=0, modified=0, deleted=0, unchanged=N
      - Running scan after adding a file: new=1, rest unchanged
      - Running scan after modifying content: modified=1, rest unchanged
      - Running scan after deleting a file: deleted=1, rest unchanged

    IMPORTANT: Use `os.walk()` NOT `Path.rglob()` for directory traversal (per research -- rglob cannot detect symlink cycles).
  </action>
  <verify>
    Run from project root:
    ```bash
    python -c "
    import tempfile, os
    from pathlib import Path
    from objlib.scanner import FileScanner, ChangeSet
    from objlib.database import Database
    from objlib.metadata import MetadataExtractor
    from objlib.config import ScannerConfig

    # Create temp directory tree mimicking library structure
    with tempfile.TemporaryDirectory() as tmpdir:
        root = Path(tmpdir)

        # Create test files
        courses = root / 'Courses' / 'Test Course'
        courses.mkdir(parents=True)
        f1 = courses / 'Test Course - Lesson 01 - Introduction.txt'
        f1.write_text('This is lesson 1 content ' * 100)  # >1KB
        f2 = courses / 'Test Course - Lesson 02 - Foundations.txt'
        f2.write_text('This is lesson 2 content ' * 100)

        # Create a hidden file (should be skipped)
        (courses / '.hidden').write_text('hidden file')

        # Create a tiny file (should be skipped due to min_size)
        (courses / 'tiny.txt').write_text('x')

        # Setup
        db_path = root / 'test.db'
        config = ScannerConfig(library_path=root, db_path=db_path, min_file_size=100)
        db = Database(db_path)
        ext = MetadataExtractor()
        scanner = FileScanner(config, db, ext)

        # First scan: all files are new
        changes = scanner.scan()
        assert len(changes.new) == 2, f'Expected 2 new, got {len(changes.new)}'
        assert len(changes.deleted) == 0
        assert len(changes.unchanged) == 0
        assert db.get_file_count() == 2

        # Second scan (no changes): all unchanged
        changes2 = scanner.scan()
        assert len(changes2.new) == 0, f'Expected 0 new, got {len(changes2.new)}'
        assert len(changes2.unchanged) == 2, f'Expected 2 unchanged, got {len(changes2.unchanged)}'

        # Modify a file
        f1.write_text('MODIFIED content ' * 100)
        changes3 = scanner.scan()
        assert len(changes3.modified) == 1, f'Expected 1 modified, got {len(changes3.modified)}'
        assert len(changes3.unchanged) == 1

        # Delete a file
        f2.unlink()
        changes4 = scanner.scan()
        assert len(changes4.deleted) == 1, f'Expected 1 deleted, got {len(changes4.deleted)}'
        assert len(changes4.unchanged) == 1

        # Check deleted file is marked LOCAL_DELETE
        counts = db.get_status_counts()
        assert counts.get('LOCAL_DELETE', 0) == 1

        db.close()
        print('All scanner tests passed')
    "
    ```
  </verify>
  <done>
    - FileScanner discovers all allowed files recursively using os.walk
    - Hidden files and directories are skipped
    - Files below minimum size are skipped and logged
    - Non-allowed extensions are skipped and logged
    - Symlink cycle detection works (visited_inodes tracking)
    - SHA-256 hashing produces consistent hex digests
    - First scan: all files detected as new, inserted into DB
    - Re-scan on unchanged library: zero changes (idempotent -- FOUN-06)
    - Modified file detected by hash change
    - Deleted file detected and marked LOCAL_DELETE
    - ChangeSet correctly categorizes all four change types
  </done>
</task>

</tasks>

<verification>
1. MetadataExtractor correctly parses both simple (`Lesson NN`) and complex (`Year N - QN - Week N`) patterns
2. FileScanner discovers files, computes hashes, extracts metadata, and persists to DB in a single `scan()` call
3. Idempotency: second scan on unchanged directory produces zero new/modified/deleted
4. Change detection: adding, modifying, deleting files all correctly detected on re-scan
5. Skipped files (hidden, too small, wrong extension) logged to _skipped_files table
6. Extraction failures logged to _extraction_failures table
</verification>

<success_criteria>
- MetadataExtractor handles all three tiers: exact pattern match, partial extraction, graceful failure
- FileScanner implements full discovery pipeline with symlink safety
- Change detection uses set operations for efficient comparison
- All FOUN-02 through FOUN-06 requirements demonstrably met via verification scripts
- Idempotent re-scan produces zero changes on unchanged library
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
