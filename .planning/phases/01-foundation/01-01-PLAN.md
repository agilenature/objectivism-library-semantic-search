---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/objlib/__init__.py
  - src/objlib/__main__.py
  - src/objlib/models.py
  - src/objlib/config.py
  - src/objlib/database.py
  - config/scanner_config.json
  - config/metadata_mappings.json
  - data/.gitkeep
autonomous: true

must_haves:
  truths:
    - "SQLite database can be created at data/library.db with WAL mode enabled"
    - "All tables (files, _processing_log, _extraction_failures, _skipped_files) exist with correct schema"
    - "FileStatus and MetadataQuality enums are importable and compare correctly with SQLite TEXT values"
    - "UPSERT inserts new records and updates existing records without losing created_at timestamps"
    - "Status transition trigger auto-logs changes to _processing_log table"
    - "Package is installable via pip install -e . and importable as objlib"
  artifacts:
    - path: "pyproject.toml"
      provides: "Package metadata, dependencies, CLI entry point"
      contains: "objlib"
    - path: "src/objlib/models.py"
      provides: "FileStatus enum, MetadataQuality enum, FileRecord dataclass"
      exports: ["FileStatus", "MetadataQuality", "FileRecord"]
    - path: "src/objlib/database.py"
      provides: "Database class with schema init, pragmas, CRUD, UPSERT, change detection queries"
      exports: ["Database"]
    - path: "src/objlib/config.py"
      provides: "ScannerConfig dataclass, load_config function"
      exports: ["ScannerConfig", "load_config"]
    - path: "config/scanner_config.json"
      provides: "Scanner configuration (extensions, min size, symlinks, library path)"
    - path: "config/metadata_mappings.json"
      provides: "Course-level metadata mappings (placeholder structure)"
  key_links:
    - from: "src/objlib/database.py"
      to: "src/objlib/models.py"
      via: "imports FileStatus, MetadataQuality, FileRecord"
      pattern: "from objlib\\.models import"
    - from: "src/objlib/config.py"
      to: "config/scanner_config.json"
      via: "json.load reads config file"
      pattern: "json\\.load"
    - from: "src/objlib/database.py"
      to: "SQLite file"
      via: "sqlite3.connect with WAL pragmas"
      pattern: "PRAGMA journal_mode=WAL"
---

<objective>
Create the project scaffolding, data models, configuration layer, and SQLite database layer for the Objectivism Library scanner.

Purpose: Establish the foundational package structure and persistence layer that all subsequent plans build upon. This covers FOUN-01 (SQLite with WAL), FOUN-06 (idempotency via UPSERT), FOUN-07 (upload timestamp columns), FOUN-08 (embedding model version column), and FOUN-09 (status tracking with atomic transitions).

Output: An installable Python package (`objlib`) with working database layer that creates a correctly-schemaed SQLite database with WAL mode, triggers, and UPSERT support.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/CLARIFICATIONS-ANSWERED.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project scaffolding, data models, and configuration</name>
  <files>
    pyproject.toml
    src/objlib/__init__.py
    src/objlib/__main__.py
    src/objlib/models.py
    src/objlib/config.py
    config/scanner_config.json
    config/metadata_mappings.json
    data/.gitkeep
  </files>
  <action>
    Create the Python package structure for `objlib`:

    **pyproject.toml:**
    - Build system: hatchling (lightweight, modern)
    - Package name: `objlib`
    - Version: `0.1.0`
    - Python requires: `>=3.12`
    - Dependencies: `typer>=0.12`, `rich>=13.0`
    - Dev dependencies (optional): `pytest>=8.0`, `pytest-cov>=5.0`
    - CLI entry point: `[project.scripts] objlib = "objlib.cli:app"`
    - Package discovery: `src` layout with `src/objlib`

    **src/objlib/__init__.py:**
    - Package version: `__version__ = "0.1.0"`
    - Public API imports: `from objlib.models import FileStatus, MetadataQuality, FileRecord`

    **src/objlib/__main__.py:**
    - Simple: `from objlib.cli import app; app()` (enables `python -m objlib`)

    **src/objlib/models.py:**
    - `FileStatus(str, Enum)` with values: `PENDING = "pending"`, `UPLOADING = "uploading"`, `UPLOADED = "uploaded"`, `FAILED = "failed"`, `LOCAL_DELETE = "LOCAL_DELETE"`
    - `MetadataQuality(str, Enum)` with values: `COMPLETE = "complete"`, `PARTIAL = "partial"`, `MINIMAL = "minimal"`, `NONE = "none"`, `UNKNOWN = "unknown"`
    - `FileRecord` dataclass with `__slots__ = True`:
      - `file_path: str` (absolute path)
      - `content_hash: str` (SHA-256 hex digest as TEXT, per research recommendation)
      - `filename: str`
      - `file_size: int`
      - `metadata_json: str | None = None` (JSON string)
      - `metadata_quality: MetadataQuality = MetadataQuality.UNKNOWN`
      - `status: FileStatus = FileStatus.PENDING`
    - Use `from __future__ import annotations` for cleaner type hints
    - Use `from dataclasses import dataclass, field, asdict` for serialization support

    **src/objlib/config.py:**
    - `ScannerConfig` dataclass with fields:
      - `library_path: Path`
      - `db_path: Path = Path("data/library.db")`
      - `allowed_extensions: set[str] = {'.txt', '.md', '.pdf', '.epub', '.docx', '.html'}` (per user decision)
      - `min_file_size: int = 1024` (1 KB minimum, per user decision)
      - `skip_hidden: bool = True`
      - `skip_patterns: set[str] = {'.DS_Store', 'Thumbs.db', '.git', '__pycache__'}`
      - `follow_symlinks: bool = True` (per user decision)
    - `load_config(config_path: Path) -> ScannerConfig`: Load from JSON, merge with defaults
    - `load_metadata_mappings(mappings_path: Path) -> dict`: Load metadata_mappings.json

    **config/scanner_config.json:**
    - Content per CLARIFICATIONS-ANSWERED.md: allowed_extensions, min_file_size_bytes, skip_hidden_files, skip_patterns, follow_symlinks, library_path

    **config/metadata_mappings.json:**
    - Placeholder structure with `courses` dict (empty for now, to be populated) and `folder_patterns` dict

    **data/.gitkeep:**
    - Empty file to ensure data directory exists in git (library.db will be gitignored)

    IMPORTANT: Use modern Python 3.12+ syntax throughout:
    - `str | None` instead of `Optional[str]`
    - `list[str]` instead of `List[str]`
    - `dict[str, Any]` instead of `Dict[str, Any]`
  </action>
  <verify>
    Run from project root:
    ```bash
    pip install -e ".[dev]" && python -c "from objlib.models import FileStatus, MetadataQuality, FileRecord; print(FileStatus.PENDING == 'pending'); print(FileRecord(file_path='/test', content_hash='abc', filename='test.txt', file_size=100))"
    ```
    Expected: `True` printed, followed by FileRecord repr. Also verify:
    ```bash
    python -c "from objlib.config import ScannerConfig, load_config; c = ScannerConfig(library_path='/tmp'); print(c.allowed_extensions)"
    ```
  </verify>
  <done>
    - Package installable via `pip install -e .`
    - `objlib.models` exports FileStatus, MetadataQuality, FileRecord with correct values
    - `FileStatus.PENDING == "pending"` evaluates to True (str, Enum pattern works)
    - `objlib.config` exports ScannerConfig with all fields matching user decisions
    - Config JSON files exist and are valid JSON
    - data/.gitkeep exists
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement SQLite database layer with schema, pragmas, and CRUD</name>
  <files>
    src/objlib/database.py
  </files>
  <action>
    Create the `Database` class in `src/objlib/database.py`:

    **Constructor `__init__(self, db_path: str | Path)`:**
    - Store db_path, ensure parent directory exists (`Path(db_path).parent.mkdir(parents=True, exist_ok=True)`)
    - Connect with `sqlite3.connect(str(db_path), autocommit=sqlite3.LEGACY_TRANSACTION_CONTROL)`
    - Set `conn.row_factory = sqlite3.Row` for dict-like access
    - Call `_setup_pragmas()` then `_setup_schema()`

    **`_setup_pragmas(self)`:**
    - Execute all per user decision: `journal_mode=WAL`, `synchronous=NORMAL`, `cache_size=-10000`, `foreign_keys=ON`, `temp_store=MEMORY`
    - Verify WAL mode: `result = self.conn.execute("PRAGMA journal_mode").fetchone()[0]` and log warning if not 'wal'

    **`_setup_schema(self)`:**
    - Use `CREATE TABLE IF NOT EXISTS` for idempotent schema creation
    - Execute the full schema SQL from the RESEARCH.md code example (verified empirically):
      - `files` table with all columns: file_path TEXT PK, content_hash TEXT NOT NULL, filename TEXT, file_size INTEGER, metadata_json TEXT, metadata_quality TEXT with CHECK, status TEXT with CHECK, error_message TEXT, gemini_file_uri TEXT, gemini_file_id TEXT, upload_timestamp TEXT, remote_expiration_ts TEXT, embedding_model_version TEXT, created_at TEXT, updated_at TEXT
      - Timestamps: Use `strftime('%Y-%m-%dT%H:%M:%f', 'now')` for ISO 8601 with milliseconds (per research)
      - Indexes: `idx_content_hash` (regular index, NOT UNIQUE -- CRITICAL per research correction), `idx_status`, `idx_metadata_quality`
      - Trigger `update_files_timestamp`: AFTER UPDATE auto-sets updated_at
      - Trigger `log_status_change`: AFTER UPDATE OF status auto-inserts into _processing_log WHEN OLD.status != NEW.status
      - `_processing_log` table (log_id, file_path, old_status, new_status, timestamp, error_details)
      - `_extraction_failures` table (failure_id, file_path, unparsed_folder_name, unparsed_filename, timestamp)
      - `_skipped_files` table (skip_id, file_path, reason, file_size, timestamp)
    - Set PRAGMA user_version = 1

    **`upsert_file(self, record: FileRecord)`:**
    - Single file UPSERT using `ON CONFLICT(file_path) DO UPDATE`
    - On conflict: update content_hash, file_size, metadata_json, metadata_quality
    - On conflict: set status to 'pending' ONLY if content_hash changed (use CASE expression)
    - Preserve created_at (updated_at handled by trigger)

    **`upsert_files(self, records: list[FileRecord])`:**
    - Batch UPSERT wrapping all records in a single `with self.conn:` transaction
    - Use `executemany` for efficiency

    **`get_all_active_files(self) -> dict[str, tuple[str, int]]`:**
    - Return `{file_path: (content_hash, file_size)}` for all files WHERE status != 'LOCAL_DELETE'
    - Used by change detection algorithm

    **`mark_deleted(self, file_paths: set[str])`:**
    - Set `status = 'LOCAL_DELETE'` for given paths in a single transaction

    **`get_status_counts(self) -> dict[str, int]`:**
    - Return count of files grouped by status (`SELECT status, COUNT(*) ... GROUP BY status`)

    **`get_quality_counts(self) -> dict[str, int]`:**
    - Return count of files grouped by metadata_quality

    **`log_skipped_file(self, file_path: str, reason: str, file_size: int | None = None)`:**
    - Insert into _skipped_files table

    **`log_extraction_failure(self, file_path: str, folder_name: str | None, filename: str | None)`:**
    - Insert into _extraction_failures table

    **`get_file_count(self) -> int`:**
    - Return total count of files (all statuses)

    **`close(self)`:**
    - Close database connection

    **Context manager support:**
    - `__enter__` returns self, `__exit__` calls close

    IMPORTANT CORRECTIONS from research:
    - content_hash index is NOT UNIQUE (allows same content at different paths)
    - content_hash stored as TEXT (hex digest), not BLOB (readable in DB browsers)
    - Use `strftime('%Y-%m-%dT%H:%M:%f', 'now')` for timestamps, not CURRENT_TIMESTAMP
  </action>
  <verify>
    Run from project root:
    ```bash
    python -c "
    from objlib.database import Database
    from objlib.models import FileRecord, MetadataQuality
    import tempfile, os

    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as f:
        db_path = f.name

    db = Database(db_path)

    # Test WAL mode
    mode = db.conn.execute('PRAGMA journal_mode').fetchone()[0]
    assert mode == 'wal', f'Expected wal, got {mode}'

    # Test insert
    rec = FileRecord(file_path='/test/file.txt', content_hash='abc123', filename='file.txt', file_size=5000)
    db.upsert_file(rec)
    assert db.get_file_count() == 1

    # Test idempotent re-insert (same path, same hash)
    db.upsert_file(rec)
    assert db.get_file_count() == 1

    # Test UPSERT with changed hash resets status to pending
    rec2 = FileRecord(file_path='/test/file.txt', content_hash='def456', filename='file.txt', file_size=5100)
    db.upsert_file(rec2)
    row = db.conn.execute('SELECT status, content_hash FROM files WHERE file_path = ?', ('/test/file.txt',)).fetchone()
    assert row['status'] == 'pending'
    assert row['content_hash'] == 'def456'

    # Test mark_deleted
    db.mark_deleted({'/test/file.txt'})
    counts = db.get_status_counts()
    assert counts.get('LOCAL_DELETE', 0) == 1

    # Test status transition log
    log = db.conn.execute('SELECT * FROM _processing_log').fetchall()
    assert len(log) >= 1

    db.close()
    os.unlink(db_path)
    os.unlink(db_path + '-wal') if os.path.exists(db_path + '-wal') else None
    os.unlink(db_path + '-shm') if os.path.exists(db_path + '-shm') else None
    print('All database tests passed')
    "
    ```
  </verify>
  <done>
    - Database creates SQLite file with WAL mode confirmed
    - All 4 tables exist (files, _processing_log, _extraction_failures, _skipped_files)
    - UPSERT inserts new records and updates existing without losing created_at
    - UPSERT resets status to 'pending' only when content_hash changes
    - Status change trigger auto-logs to _processing_log
    - content_hash index is NOT UNIQUE (per research correction)
    - Batch upsert works with executemany in single transaction
    - Context manager (with Database(...) as db) works correctly
  </done>
</task>

</tasks>

<verification>
1. `pip install -e ".[dev]"` succeeds without errors
2. `python -c "import objlib; print(objlib.__version__)"` prints `0.1.0`
3. Database verification script in Task 2 passes all assertions
4. `python -c "from objlib.config import load_config; from pathlib import Path; c = load_config(Path('config/scanner_config.json')); print(c)"` prints config with correct defaults
5. SQLite file at `data/library.db` can be opened with `sqlite3` CLI and shows correct schema via `.schema`
</verification>

<success_criteria>
- Package `objlib` is pip-installable and importable
- Database layer creates WAL-mode SQLite with correct schema matching all FOUN requirements
- UPSERT is idempotent: re-inserting unchanged file produces zero changes
- Status tracking works with atomic transitions logged to audit table
- All columns needed for Phase 2 (gemini_file_id, upload_timestamp, remote_expiration_ts, embedding_model_version) exist as nullable
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
