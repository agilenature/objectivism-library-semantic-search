---
phase: 16.4-metadata-invariant-retrievability-audit
plan: 02
type: execute
wave: 2
depends_on: ["16.4-01"]
files_modified:
  - src/objlib/cli.py
  - scripts/re_upload_phase164.py
  - data/library.db
autonomous: true
user_setup:
  - service: mistral
    why: "Batch extraction API for 41 pending files"
    env_vars:
      - name: MISTRAL_API_KEY
        source: "keyring (already configured)"

must_haves:
  truths:
    - "All 41 pending indexed files complete batch-extract with exactly 8 primary_topics each"
    - "objlib metadata audit exits 0 for all 1,749 indexed files"
    - "Per-series breakdown shows every series at 100% file_primary_topics coverage"
    - "No indexed file has ai_metadata_status other than approved or skipped"
    - "All skipped files are books exceeding BOOK_SIZE_BYTES"
    - "40 re-extracted Episode files are re-uploaded with updated identity headers (Tags field)"
    - "1 re-extracted Book file is re-uploaded with identity header"
  artifacts:
    - path: "src/objlib/cli.py"
      provides: "Extended metadata audit command with per-series breakdown"
      contains: "Per-Series Breakdown"
    - path: "scripts/re_upload_phase164.py"
      provides: "Targeted re-upload script for 41 Phase 16.4 files"
      contains: "re_upload_phase164"
    - path: ".planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-02-audit-output.txt"
      provides: "Captured audit output showing exit 0"
  key_links:
    - from: "src/objlib/cli.py"
      to: "data/library.db"
      via: "SQL queries in metadata audit command"
      pattern: "file_primary_topics"
    - from: "scripts/re_upload_phase164.py"
      to: "Gemini store"
      via: "re-upload of 41 files with updated identity headers"
      pattern: "gemini_store_doc_id"
    - from: "data/library.db"
      to: "Gemini store"
      via: "re-upload of 41 files with updated identity headers"
      pattern: "gemini_store_doc_id"
---

<objective>
Run batch-extract for the 41 pending files (40 Episodes + 1 Book), approve all extracted files, re-upload the 41 files with updated identity headers via a targeted script, extend the metadata audit command with per-series breakdown, and confirm `objlib metadata audit` exits 0 with 100% coverage across all series.

Purpose: Achieve structural metadata quality across the entire indexed corpus. This is the precondition for the retrievability audit (Plan 16.4-03) -- all 3 query strategies require file_primary_topics and topic_aspects to construct queries.

Output: All 1,749 indexed files have file_primary_topics; audit command reports per-series breakdown; 41 files re-uploaded with enriched identity headers.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-CONTEXT.md
@.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-RESEARCH.md
@.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-01-SUMMARY.md
@src/objlib/cli.py
@src/objlib/extraction/batch_orchestrator.py
@src/objlib/upload/header_builder.py
@scripts/re_enrich_retrieval.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Batch-extract 41 pending files</name>
  <files>data/library.db</files>
  <action>
  Run batch-extract for the 41 pending files (40 Episodes + 1 Book reclassified in Plan 16.4-01).

  Run: `python -m objlib metadata batch-extract --db data/library.db`

  This picks up the 41 files with ai_metadata_status='pending'.

  IMPORTANT: The 1 Book file ("Philosophy - Who Needs It", 552KB, ~115K tokens) will likely exceed MAX_DOCUMENT_TOKENS=100,000 and get marked 'skipped' by _mark_oversized(). This is ACCEPTABLE -- it is a book-length file near the Mistral context limit. If this happens:
  - The file remains indexed in Gemini store (it has an identity header from Phase 16.3)
  - It will be skipped legitimately (large file, genuine extraction limit)
  - The audit must treat it as a valid 'skipped' file with error_message explaining why
  - Verify: `SELECT ai_metadata_status, error_message FROM files WHERE filename LIKE '%Philosophy%Who%Needs%'`

  If the batch-extract succeeds for this Book file (Mistral handles it), approve it normally.

  After batch-extract completes, verify:
  ```sql
  SELECT ai_metadata_status, COUNT(*) FROM files WHERE gemini_state='indexed' GROUP BY ai_metadata_status;
  -- Expected: approved ~1708, extracted ~40, skipped 1 (or 2 if Book re-skipped), pending 0, failed_validation 0
  ```

  If any Episode files fail validation again (hard failures), investigate the specific error. The research indicates these files have existing file_metadata_ai records with 5-7 topics that the validator can expand to 8. If batch-extract fails for some Episodes:
  - Check if the failure is a Mistral format issue (JSON array vs object)
  - Consider resetting to pending and retrying once
  - If persistent failures: escalate to checkpoint (do NOT silently exclude)
  </action>
  <verify>
  ```sql
  -- No pending indexed files remain (except possibly the 60 OH files which are addressed in Task 2)
  SELECT COUNT(*) FROM files WHERE ai_metadata_status='pending' AND gemini_state='indexed';
  -- Expected: 0

  -- No failed_validation indexed files
  SELECT COUNT(*) FROM files WHERE ai_metadata_status='failed_validation' AND gemini_state='indexed';
  -- Expected: 0

  -- The 40 Episodes should now be 'extracted' (or 'approved' if auto-approved)
  SELECT ai_metadata_status, COUNT(*) FROM files WHERE gemini_state='indexed' GROUP BY ai_metadata_status;
  ```
  </verify>
  <done>41 pending files processed by batch-extract. 40 Episodes have extracted metadata with 8 primary_topics each. Book file either extracted or legitimately skipped (documented).</done>
</task>

<task type="auto">
  <name>Task 2: Approve files + re-upload 41 files + extend metadata audit CLI</name>
  <files>src/objlib/cli.py, scripts/re_upload_phase164.py, data/library.db</files>
  <action>
  This task has 4 sub-steps: approve, re-upload, store-sync, and audit extension.

  SUB-STEP A -- Approve all extracted/needs_review files via direct SQL:

  The CLI command `approve-all` does not exist. Use a direct SQL UPDATE instead.

  Run via `sqlite3 data/library.db`:
  ```sql
  UPDATE files
  SET ai_metadata_status = 'approved'
  WHERE ai_metadata_status IN ('extracted', 'needs_review')
    AND gemini_state = 'indexed'
    AND EXISTS (SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = files.file_path);
  ```

  This handles BOTH:
  - The ~40 newly batch-extracted Episode files (status='extracted')
  - The 60 OH files that still have ai_metadata_status='needs_review' (they already have file_primary_topics from prior phases)

  Verify:
  ```sql
  SELECT COUNT(*), ai_metadata_status FROM files WHERE gemini_state='indexed' GROUP BY ai_metadata_status;
  -- Expected: 0 rows with 'needs_review', 0 rows with 'extracted'
  -- All non-skipped indexed files should be 'approved'
  ```

  SUB-STEP B -- Create and run targeted re-upload script for the 41 files:

  Create `scripts/re_upload_phase164.py` -- a standalone script that re-uploads the 41 files that were re-extracted/approved in this phase. Do NOT use `scripts/re_enrich_retrieval.py` (it loads from a static Phase 16.3 manifest that does not include these 41 files).

  The script must:
  1. Query DB for the specific files that need re-upload. The 41 files are identifiable as: the 40 files whose ai_metadata_status was changed from 'failed_validation' to 'pending' in Plan 16.4-01 (now 'approved' after Task 1+2A), plus the 1 reclassified Book file. Identify them with:
     ```sql
     SELECT f.file_path, f.filename, f.gemini_store_doc_id, f.gemini_file_id, f.gemini_state
     FROM files f
     WHERE f.ai_metadata_status = 'approved'
       AND f.gemini_state = 'indexed'
       AND f.file_path IN (
         -- The 41 files from Plan 16.4-01 reset
         -- These are the files that had error_message cleared (error_message IS NULL after reset)
         -- and were just batch-extracted in Task 1
         SELECT file_path FROM files
         WHERE gemini_state = 'indexed'
           AND ai_metadata_status = 'approved'
           AND file_path IN (
             -- Identify by: files that had failed_validation or were reclassified from skipped
             -- Since we don't have a direct marker, use file_metadata_ai.updated_at from today
             SELECT fma.file_path FROM file_metadata_ai fma
             WHERE fma.is_current = 1
               AND date(fma.updated_at) = date('now')
           )
       )
     ```
     Alternative simpler approach: hardcode the identification by querying `file_metadata_ai` for records updated today (the batch-extract just ran), which gives exactly the files that were re-extracted.

  2. For each file, follow the SAME upload sequence as `re_enrich_retrieval.py`:
     a. Read file content from disk
     b. Call `build_identity_header()` to generate updated header with new primary_topics in Tags
     c. Prepend identity header to content
     d. Delete old store document (via `gemini_store_doc_id`)
     e. Upload file to Gemini Files API
     f. Import file to Gemini store
     g. Update DB with new `gemini_file_id` and `gemini_store_doc_id`

  3. Accept `--dry-run` flag (prints what would be done without uploading)
  4. Accept `--db` flag (default: `data/library.db`)
  5. Accept `--store` flag (default: `objectivism-library`)

  Reference `scripts/re_enrich_retrieval.py` for the exact Gemini API call patterns (upload, import_to_store, delete_store_document).

  Run the script:
  ```bash
  # Dry run first
  python scripts/re_upload_phase164.py --store objectivism-library --db data/library.db --dry-run

  # Actual re-upload
  python scripts/re_upload_phase164.py --store objectivism-library --db data/library.db
  ```

  SUB-STEP C -- Run store-sync to clean orphans:

  ```bash
  python -m objlib store-sync --store objectivism-library --no-dry-run --yes
  ```

  Verify: `python -m objlib store-sync --store objectivism-library` shows 0 orphans.

  SUB-STEP D -- Extend metadata audit command with per-series breakdown:

  In `src/objlib/cli.py`, the `metadata_audit` command (line ~3302), add a new section AFTER the Phase 16.3 readiness table: a "Per-Series Breakdown" table showing for each series:
  - Series name (ITOE, ITOE AT, ITOE OH, ITOE AT OH, OL, Episodes, MOTM, Other, Books)
  - Total indexed files in series
  - Files with file_primary_topics
  - Files with topic_aspects (from file_metadata_ai where is_current=1)
  - Files with summary (from file_metadata_ai.metadata_json where summary is non-empty)
  - Coverage %

  Series detection logic (derive from file_path patterns):
  ```sql
  CASE
    WHEN file_path LIKE '%/ITOE Addenda/%' AND file_path LIKE '%Office Hour%' THEN 'ITOE AT OH'
    WHEN file_path LIKE '%/ITOE Addenda/%' THEN 'ITOE AT'
    WHEN file_path LIKE '%/ITOE/%' AND file_path LIKE '%Office Hour%' THEN 'ITOE OH'
    WHEN file_path LIKE '%/ITOE/%' THEN 'ITOE'
    WHEN file_path LIKE '%/Objectivist Logic/%' THEN 'OL'
    WHEN file_path LIKE '%/MOTM/%' THEN 'MOTM'
    WHEN file_path LIKE '%Episode%' THEN 'Episodes'
    WHEN file_path LIKE '%/Books/%' THEN 'Books'
    ELSE 'Other'
  END as series
  ```

  The per-series table does NOT affect the exit code. It is informational, committed to the repo as an audit artifact. The existing 4 conditions (+ informational condition 5) remain unchanged and still control exit code.

  Also add a new condition 6 (ENFORCING, affects exit code):
  ```
  Condition 6: Indexed non-book without file_primary_topics
  Query: SELECT COUNT(*) FROM files f
         WHERE f.gemini_state = 'indexed'
           AND f.ai_metadata_status != 'skipped'
           AND NOT EXISTS (SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = f.file_path)
  Expected: 0
  ```
  This catches any indexed file (regardless of ai_metadata_status) that lacks topics and is not a skipped book. It is the structural invariant.

  Run the audit and capture output:
  ```bash
  python -m objlib metadata audit --db data/library.db 2>&1 | tee .planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-02-audit-output.txt
  echo "Exit code: $?"
  ```
  </action>
  <verify>
  1. `python -m objlib metadata audit --db data/library.db; echo "EXIT: $?"` shows "AUDIT PASSED" and "EXIT: 0"
  2. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE ai_metadata_status='failed_validation'"` returns 0
  3. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE ai_metadata_status='pending' AND gemini_state='indexed'"` returns 0
  4. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE ai_metadata_status='needs_review' AND gemini_state='indexed'"` returns 0
  5. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE ai_metadata_status='extracted' AND gemini_state='indexed'"` returns 0
  6. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE gemini_state='indexed' AND ai_metadata_status NOT IN ('approved','skipped')"` returns 0
  7. Per-series breakdown table visible in audit output with all series at 100% topics coverage (except Books which may have 1 skipped file)
  8. `python -m objlib store-sync --store objectivism-library` shows 0 orphans
  </verify>
  <done>
  - All 41 pending files batch-extracted (40 Episodes + 1 Book)
  - All extracted + needs_review files approved via SQL (including 60 OH files)
  - 41 files re-uploaded with updated identity headers via scripts/re_upload_phase164.py
  - Store-sync confirms 0 orphans
  - Metadata audit extended with per-series breakdown and condition 6
  - `objlib metadata audit` exits 0
  - Per-series breakdown committed as audit artifact
  </done>
</task>

</tasks>

<verification>
Phase 16.4-02 structural metadata quality is confirmed when:
1. `objlib metadata audit` exits 0 with all conditions (including new condition 6) passing
2. Per-series breakdown shows 100% file_primary_topics coverage for all non-book series
3. Zero files with failed_validation, pending, extracted, or needs_review status among indexed files
4. Store-sync shows 0 orphans after re-upload
5. Audit output saved as artifact in phase directory
6. scripts/re_upload_phase164.py committed to repo
</verification>

<success_criteria>
- objlib metadata audit exits 0 for all 1,749 indexed files
- Per-series breakdown shows every series at 100% file_primary_topics coverage
- No indexed file has ai_metadata_status other than approved or skipped
- All skipped files have error_message explaining why
- 41 files re-uploaded with updated identity headers (Tags field reflects new primary_topics)
- 0 orphaned store documents
</success_criteria>

<output>
After completion, create `.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-02-SUMMARY.md`
</output>
