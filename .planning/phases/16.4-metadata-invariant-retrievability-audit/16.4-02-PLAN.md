---
phase: 16.4-metadata-invariant-retrievability-audit
plan: 02
type: execute
wave: 2
depends_on: ["16.4-01"]
files_modified:
  - src/objlib/cli.py
  - data/library.db
autonomous: true
user_setup:
  - service: mistral
    why: "Batch extraction API for 41 pending files"
    env_vars:
      - name: MISTRAL_API_KEY
        source: "keyring (already configured)"

must_haves:
  truths:
    - "All 41 pending indexed files complete batch-extract with exactly 8 primary_topics each"
    - "objlib metadata audit exits 0 for all 1,749 indexed files"
    - "Per-series breakdown shows every series at 100% file_primary_topics coverage"
    - "No indexed file has ai_metadata_status other than approved or skipped"
    - "All skipped files are books exceeding BOOK_SIZE_BYTES"
    - "40 re-extracted Episode files are re-uploaded with updated identity headers (Tags field)"
    - "1 re-extracted Book file is re-uploaded with identity header"
  artifacts:
    - path: "src/objlib/cli.py"
      provides: "Extended metadata audit command with per-series breakdown"
      contains: "Per-Series Breakdown"
    - path: ".planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-02-audit-output.txt"
      provides: "Captured audit output showing exit 0"
  key_links:
    - from: "src/objlib/cli.py"
      to: "data/library.db"
      via: "SQL queries in metadata audit command"
      pattern: "file_primary_topics"
    - from: "data/library.db"
      to: "Gemini store"
      via: "re-upload of 41 files with updated identity headers"
      pattern: "gemini_store_doc_id"
---

<objective>
Run batch-extract for the 41 pending files (40 Episodes + 1 Book), extend the metadata audit command with per-series breakdown, re-upload the 41 files with updated identity headers, and confirm `objlib metadata audit` exits 0 with 100% coverage across all series.

Purpose: Achieve structural metadata quality across the entire indexed corpus. This is the precondition for the retrievability audit (Plan 16.4-03) -- all 3 query strategies require file_primary_topics and topic_aspects to construct queries.

Output: All 1,749 indexed files have file_primary_topics; audit command reports per-series breakdown; 41 files re-uploaded with enriched identity headers.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-CONTEXT.md
@.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-RESEARCH.md
@.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-01-SUMMARY.md
@src/objlib/cli.py
@src/objlib/extraction/batch_orchestrator.py
@src/objlib/upload/header_builder.py
@scripts/re_enrich_retrieval.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Batch-extract 41 pending files + approve + extend audit with per-series breakdown</name>
  <files>src/objlib/cli.py, data/library.db</files>
  <action>
  This task has 4 sub-steps: batch-extract, approve, re-upload, and audit extension.

  SUB-STEP A -- Batch-extract the 41 pending files:

  Run: `python -m objlib metadata batch-extract --db data/library.db`

  This picks up the 41 files with ai_metadata_status='pending' (40 Episodes + 1 Book reclassified in Plan 16.4-01).

  IMPORTANT: The 1 Book file ("Philosophy - Who Needs It", 552KB, ~115K tokens) will likely exceed MAX_DOCUMENT_TOKENS=100,000 and get marked 'skipped' by _mark_oversized(). This is ACCEPTABLE -- it is a book-length file near the Mistral context limit. If this happens:
  - The file remains indexed in Gemini store (it has an identity header from Phase 16.3)
  - It will be skipped legitimately (large file, genuine extraction limit)
  - The audit must treat it as a valid 'skipped' file with error_message explaining why
  - Verify: `SELECT ai_metadata_status, error_message FROM files WHERE filename LIKE '%Philosophy%Who%Needs%'`

  If the batch-extract succeeds for this Book file (Mistral handles it), approve it normally.

  After batch-extract completes, verify:
  ```sql
  SELECT ai_metadata_status, COUNT(*) FROM files WHERE gemini_state='indexed' GROUP BY ai_metadata_status;
  -- Expected: approved ~1748 (or 1747 if Book re-skipped), skipped 1 (or 2 if Book re-skipped), pending 0, failed_validation 0
  ```

  If any Episode files fail validation again (hard failures), investigate the specific error. The research indicates these files have existing file_metadata_ai records with 5-7 topics that the validator can expand to 8. If batch-extract fails for some Episodes:
  - Check if the failure is a Mistral format issue (JSON array vs object)
  - Consider resetting to pending and retrying once
  - If persistent failures: escalate to checkpoint (do NOT silently exclude)

  SUB-STEP B -- Approve extracted files:

  Run: `python -m objlib metadata approve-all --db data/library.db`
  (This approves 'extracted' and 'needs_review' files that passed validation.)

  Verify:
  ```sql
  SELECT COUNT(*) FROM files WHERE ai_metadata_status IN ('extracted', 'needs_review');
  -- Expected: 0 (all approved or skipped)
  ```

  SUB-STEP C -- Re-upload 41 files with updated identity headers:

  The 41 re-extracted files need re-upload because their identity headers (Tags field) changed with new/updated primary_topics. Use the existing `scripts/re_enrich_retrieval.py` script with appropriate filtering.

  IMPORTANT: These files are already indexed (gemini_state='indexed'). The re-upload replaces the store document with one containing the updated identity header.

  Create a targeted re-upload by running re_enrich_retrieval.py with a filter for just the 41 files. If the script does not support filtering by specific files, create a small wrapper script that:
  1. Queries DB for the 41 files that were just re-extracted (track their file_paths from the batch-extract output)
  2. For each file: build_identity_header() + read enriched content + upload to Gemini store (replacing old store doc)
  3. Run store-sync after to clean up orphans

  Alternative approach if re_enrich_retrieval.py supports --category or file list: use that.

  After re-upload, verify:
  ```sql
  SELECT COUNT(*) FROM files WHERE gemini_state='indexed';
  -- Expected: 1749 (unchanged count, same files, updated content)
  ```
  Run: `python -m objlib store-sync --store objectivism-library --no-dry-run --yes`
  to clean any orphaned store docs from the re-upload.

  SUB-STEP D -- Extend metadata audit command with per-series breakdown:

  In `src/objlib/cli.py`, the `metadata_audit` command (line ~3302), add a new section AFTER the Phase 16.3 readiness table: a "Per-Series Breakdown" table showing for each series:
  - Series name (ITOE, ITOE AT, ITOE OH, ITOE AT OH, OL, Episodes, MOTM, Other, Books)
  - Total indexed files in series
  - Files with file_primary_topics
  - Files with topic_aspects (from file_metadata_ai where is_current=1)
  - Files with summary (from file_metadata_ai.metadata_json where summary is non-empty)
  - Coverage %

  Series detection logic (derive from file_path patterns):
  ```sql
  CASE
    WHEN file_path LIKE '%/ITOE Addenda/%' AND file_path LIKE '%Office Hour%' THEN 'ITOE AT OH'
    WHEN file_path LIKE '%/ITOE Addenda/%' THEN 'ITOE AT'
    WHEN file_path LIKE '%/ITOE/%' AND file_path LIKE '%Office Hour%' THEN 'ITOE OH'
    WHEN file_path LIKE '%/ITOE/%' THEN 'ITOE'
    WHEN file_path LIKE '%/Objectivist Logic/%' THEN 'OL'
    WHEN file_path LIKE '%/MOTM/%' THEN 'MOTM'
    WHEN file_path LIKE '%Episode%' THEN 'Episodes'
    WHEN file_path LIKE '%/Books/%' THEN 'Books'
    ELSE 'Other'
  END as series
  ```

  The per-series table does NOT affect the exit code. It is informational, committed to the repo as an audit artifact. The existing 4 conditions (+ informational condition 5) remain unchanged and still control exit code.

  Also add a new condition 6 (ENFORCING, affects exit code):
  ```
  Condition 6: Indexed non-book without file_primary_topics
  Query: SELECT COUNT(*) FROM files f
         WHERE f.gemini_state = 'indexed'
           AND f.ai_metadata_status != 'skipped'
           AND NOT EXISTS (SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = f.file_path)
  Expected: 0
  ```
  This catches any indexed file (regardless of ai_metadata_status) that lacks topics and is not a skipped book. It is the structural invariant.

  Run the audit and capture output:
  ```bash
  python -m objlib metadata audit --db data/library.db 2>&1 | tee .planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-02-audit-output.txt
  echo "Exit code: $?"
  ```
  </action>
  <verify>
  1. `python -m objlib metadata audit --db data/library.db; echo "EXIT: $?"` shows "AUDIT PASSED" and "EXIT: 0"
  2. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE ai_metadata_status='failed_validation'"` returns 0
  3. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE ai_metadata_status='pending' AND gemini_state='indexed'"` returns 0
  4. `sqlite3 data/library.db "SELECT COUNT(*) FROM files WHERE gemini_state='indexed' AND ai_metadata_status NOT IN ('approved','skipped')"` returns 0
  5. Per-series breakdown table visible in audit output with all series at 100% topics coverage (except Books which may have 1 skipped file)
  6. `python -m objlib store-sync --store objectivism-library` shows 0 orphans
  </verify>
  <done>
  - All 41 pending files batch-extracted (40 Episodes + 1 Book)
  - All extracted files approved
  - 41 files re-uploaded with updated identity headers
  - Metadata audit extended with per-series breakdown and condition 6
  - `objlib metadata audit` exits 0
  - Per-series breakdown committed as audit artifact
  - Store-sync confirms 0 orphans
  </done>
</task>

</tasks>

<verification>
Phase 16.4-02 structural metadata quality is confirmed when:
1. `objlib metadata audit` exits 0 with all conditions (including new condition 6) passing
2. Per-series breakdown shows 100% file_primary_topics coverage for all non-book series
3. Zero files with failed_validation, pending, or needs_review status among indexed files
4. Store-sync shows 0 orphans after re-upload
5. Audit output saved as artifact in phase directory
</verification>

<success_criteria>
- objlib metadata audit exits 0 for all 1,749 indexed files
- Per-series breakdown shows every series at 100% file_primary_topics coverage
- No indexed file has ai_metadata_status other than approved or skipped
- All skipped files have error_message explaining why
- 41 files re-uploaded with updated identity headers (Tags field reflects new primary_topics)
- 0 orphaned store documents
</success_criteria>

<output>
After completion, create `.planning/phases/16.4-metadata-invariant-retrievability-audit/16.4-02-SUMMARY.md`
</output>
