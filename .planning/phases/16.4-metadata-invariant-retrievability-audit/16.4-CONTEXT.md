# CONTEXT.md ‚Äî Phase 16.4: Metadata Pipeline Invariant + Comprehensive Retrievability Audit

**Generated:** 2026-02-25
**Phase Goal:** Every non-book file satisfies the metadata completeness invariant AND is independently retrievable under a stable query strategy ‚Äî zero exclusions, zero tolerance.
**Synthesis Source:** Multi-provider AI analysis (OpenAI gpt-5.2, Gemini Pro with high reasoning; Perplexity unavailable 502)

---

## Overview

Phase 16.4 enforces a structural invariant that prior phases worked around rather than fixed: every small (.txt/.md) file must complete full AI extraction before upload, with no scanner-only approval path. It also replaces ad-hoc A7 exclusions (Episodes, OH) and tolerance=2 with an audit-confirmed zero-exclusion query strategy.

The phase has two distinct concerns:
1. **Routing correctness** ‚Äî ensure the pipeline structurally prevents scanner-only approval for non-book files
2. **Retrievability correctness** ‚Äî ensure every file is independently findable via a stable, query-strategy using its identity header content

Both providers converged on the same core ambiguities. This document synthesizes them.

**Confidence markers:**
- ‚úÖ **Consensus** ‚Äî Both providers identified this as critical
- ‚ö†Ô∏è **Recommended** ‚Äî One provider identified with strong rationale
- üîç **Needs Clarification** ‚Äî Identified but genuinely uncertain

---

## Gray Areas Identified

### ‚úÖ 1. Corpus Count Discrepancy (1,749 DB vs 1,809 Success Criteria)

**What needs to be decided:**
What constitutes the 1,809 files referenced in Phase 16.4 SC3/SC4? Where do the extra 60 files come from, and is the retrievability audit supposed to run against DB-indexed files or something else?

**Why it's ambiguous:**
Phase 16.3 ended with DB=1749, Store=1749, Orphans=0. Yet Phase 16.4 success criteria say "all 1,809 indexed files." The difference is exactly 60 ‚Äî which matches the ITOE OH batch that was re-uploaded. Two hypotheses:
- H1: After the routing fix (Plan 16.4-01), 60 scanner-approved non-book files are re-routed to pending, extracted, and uploaded ‚Äî bringing total to 1,809.
- H2: The 1,809 figure is a documentation error and the true count is 1,749.

**Provider synthesis:**
- **OpenAI:** Recommends establishing a canonical corpus manifest before running any audits; without a stable denominator, "zero exclusions" is meaningless.
- **Gemini:** Suspects the 60 OH files created duplicates during re-upload, requiring reconciliation.

**Proposed implementation decision:**
H1 is more likely given the routing audit context. The 60 extra files are likely non-book files with scanner-only approval that will be discovered by the routing audit, extracted, and uploaded. Plan 16.4-01's first task should be to identify exactly which files have this status and confirm the expected final count before plans 16.4-02/03 begin. The DB is the source of truth ‚Äî audits run against `gemini_state='indexed'` files.

**Open questions:**
- After the routing fix, how many non-book files currently have `ai_metadata_status='approved'` but lack `primary_topics`? This is the delta.
- Should Plan 16.4-01 begin with a DB query that produces the exact expected count before any code changes?

**Confidence:** ‚úÖ Both providers agreed this must be resolved before audits begin.

---

### ‚úÖ 2. Book-Size Threshold: Units and Value

**What needs to be decided:**
What byte count defines a "book" (too large for Mistral extraction), what variable name and module it lives in, and whether the existing `ai_metadata_status='skipped'` files were correctly classified against this threshold.

**Why it's ambiguous:**
The Mistral Batch API context window is token-based, but file routing happens on disk (bytes). Token count depends on the specific Mistral model used and content encoding. A conservative byte threshold that errs toward "extract" is safest, but too low a value would force extraction attempts that fail. The existing skipped files presumably exceeded some threshold ‚Äî but that threshold was never made explicit.

**Provider synthesis:**
- **OpenAI:** Recommends measuring the upload payload (header + transcript), using a constants module, and having CI grep for any inline number duplicating the constant.
- **Gemini:** Suggests ~80,000 bytes (conservative estimate for Mistral batch) with the caveat that it depends on the specific model.

**Proposed implementation decision:**
Define `BOOK_SIZE_BYTES` in `src/objlib/constants.py` (or equivalent). The value should be derived from the largest successfully-extracted file currently in the DB. If the largest extracted file is X bytes, set threshold to 1.5X to leave headroom. The routing audit (Plan 16.4-01) should verify every current `ai_metadata_status='skipped'` file exceeds this threshold and every `ai_metadata_status IN ('approved','pending')` non-book file is below it.

**Open questions:**
- What is the largest .txt file that successfully completed batch-extract? (This anchors the threshold.)
- Is the Mistral model `open-mistral-7b` or `mistral-small-latest` for batch jobs? (Affects actual context window.)

**Confidence:** ‚úÖ Both providers agreed threshold must be named constant with grep verification.

---

### ‚úÖ 3. Episode Retrievability: Can Zero-Tolerance Be Achieved?

**What needs to be decided:**
Whether the 333 Episode files can achieve max_misses=0 in A7 using the identity headers added in Phase 16.3, or whether Episodes will always fail zero-tolerance due to semantic homogeneity ‚Äî requiring either a fundamentally different approach or an explicit "unanswerable" classification.

**Why it's ambiguous:**
Phase 16.3 excluded Episodes from A7 precisely because they're semantically homogeneous (podcast transcripts with generic topics, no class numbers). The identity headers now include Title/Course/Topic/Tags ‚Äî but if all Episode titles are similarly generic ("Episode 12", "Episode 47"), the headers may not provide enough discrimination for Gemini's vector search.

The ITOE OH files (60) had the same problem and still "fail A7 at zero tolerance due to semantic homogeneity" per STATE.md.

**Provider synthesis:**
- **OpenAI:** Recommends defining "retrievable" as "appears in top-5" (not top-1), which relaxes the constraint while still being operationally meaningful.
- **Gemini:** Recommends hybrid query using the file title in quotes alongside primary topics; questions whether Gemini File Search respects quotation marks as strict keyword constraints.

**Proposed implementation decision:**
Plan 16.4-03 (retrievability audit) must empirically determine whether Episodes can achieve zero misses with any of the 3 query strategies. If no strategy achieves 100% for Episodes, the phase must document "affirmative evidence of what would be needed to fix them" (per ROADMAP SC4) ‚Äî which may mean concluding that Episodes require a different indexing approach (e.g., separate episode-specific store, or accepting that episode retrieval is not achievable via content-based query). Do NOT pre-exclude them ‚Äî let the audit data drive the decision.

**The key insight:** The identity header for an Episode includes its `primary_topics`. If two Episode files share similar topics, the header discriminator becomes the Episode number. "Episode 12 - Topic A" vs "Episode 47 - Topic A" ‚Äî the only differentiator is the number. Does querying "Episode 12" retrieve the right file? This is what the audit must confirm.

**Open questions:**
- Do Episode identity headers include the episode number in the Title field?
- Does querying the exact episode number (e.g., "Episode 12") return that specific file in top results, or does it scatter across multiple episodes?
- If Episodes cannot achieve zero-tolerance, is the project willing to accept a documented "achievability floor" with affirmative evidence?

**Confidence:** ‚úÖ Both providers flagged this as the hardest success criterion; resolution requires empirical audit data.

---

### ‚úÖ 4. Query Strategy Design: Universal vs. Series-Specific

**What needs to be decided:**
Whether A7 should use a single universal query format applied to all files, or per-series query construction. If universal: what is the query template? If per-series: what logic determines which strategy to apply?

**Why it's ambiguous:**
Different file series have different discriminating metadata:
- Class-number files (ITOE, OL): discriminated by class number (e.g., "09-02")
- MOTM: discriminated by topic
- Episodes: discriminated by episode number (if present in title)
- Other-stem: topic == filename stem (low discrimination)

A universal query template using only topics may fail for class-number files (they share topics). A class-number query may fail for Episode files (no class numbers). The retrievability audit (Plan 16.4-03) is meant to resolve this empirically ‚Äî but we need to agree on the 3 strategies being tested.

**Provider synthesis:**
- **OpenAI:** Recommends universal strategy using `course + top-3 primary_topics` as the query; defines "retrievable" as appearing in top-5.
- **Gemini:** Recommends universal strategy using `{filename} + {Top 3 Topics}` (leveraging the unique identity header filename field); prefers this to per-series logic in the stability checker.

**Proposed implementation decision:**
The 3 strategies for Plan 16.4-03 should be:
1. **Stem-only**: query = filename stem (e.g., "Objectivist Logic Class 09-02")
2. **Stem + aspects**: query = `{stem} {top-3 topic_aspects joined by space}`
3. **Topics + course**: query = `{course} {top-3 primary_topics joined by space}`

Retrievability threshold: file appears in **top-5** results (consistent with Phase 15 design). If no single strategy achieves 100%, the minimum viable strategy is the one with the highest hit rate ‚Äî and the audit documents residual failures with the specific files that fail all 3 strategies.

A7 in `check_stability.py` should use the **best-performing strategy confirmed by Plan 16.4-03**, not a hand-picked one. This is the dependency between Plan 16.4-03 and Plan 16.4-04.

**Open questions:**
- Should "retrievable" mean top-1 or top-5? (Top-5 is more lenient but still operationally meaningful for search quality.)
- If strategy B outperforms strategy A for Episodes but A outperforms B for class-number files, should A7 use strategy A+B together (per-series), or pick the globally best single strategy?

**Confidence:** ‚úÖ Both providers agreed query strategy must be empirically determined before A7 is updated.

---

### ‚ö†Ô∏è 5. Retrievability Audit Performance at Scale (1,809 √ó 3 = 5,427 API calls)

**What needs to be decided:**
How to execute 5,427 Gemini File Search queries within rate limits, with resumability on failure, and without blocking development for an unreasonable time window.

**Why it's ambiguous:**
If the Gemini API has a rate limit of ~60 RPM, 5,427 queries would take ~1.5 hours. A retry storm on 429s could extend this further. The audit script is a new piece of infrastructure that must be robust (resumable, rate-limited, result-cached).

**Provider synthesis:**
- **Gemini:** Explicitly flagged this; recommends semaphore-based rate limiting + result caching to a JSON/SQLite file for resumability.
- **OpenAI:** Did not flag separately but implies it in the audit infrastructure recommendation.

**Proposed implementation decision:**
The retrievability audit script (Plan 16.4-03) should:
- Save results progressively to a JSON file (keyed by file_id + strategy)
- Skip already-completed entries on re-run (resumable)
- Use asyncio with a semaphore limiting concurrency
- Implement exponential backoff on 429 responses
- Accept a `--strategy` flag to run one strategy at a time (allows overnight runs per strategy)

The audit does NOT need to be a single continuous run ‚Äî results from 3 separate runs (one per strategy) can be merged.

**Open questions:**
- What is the actual Gemini File Search API rate limit for the current tier?
- Can the audit run strategy 1 on day 1, strategy 2 on day 2, etc., or must all 3 run in sequence before conclusions can be drawn?

**Confidence:** ‚ö†Ô∏è One provider explicitly flagged; clearly important for executability.

---

### ‚ö†Ô∏è 6. A7 Consecutive STABLE Runs: Time Gap and Nondeterminism Handling

**What needs to be decided:**
How long between the two "consecutive" STABLE runs required by SC5, and how to handle Gemini's inherent ranking nondeterminism (same query returning slightly different result sets on consecutive calls).

**Why it's ambiguous:**
Phase 16.3 used "at least 1 hour" separation. The STATE.md temporal log shows Run 1 (19/20) and Run 2 (20/20) at 4-minute separation ‚Äî suggesting some nondeterminism that resolved in 4 minutes. With max_misses=0, a single non-deterministic fluctuation causes a FAIL, which could create an infinite retry situation.

**Provider synthesis:**
- **OpenAI:** Recommends 15-minute minimum separation, up to 3 retries per query on transport errors, and logging query strings + returned top-5 IDs for debugging.

**Proposed implementation decision:**
Maintain the Phase 16.3 approach: "at least 1 hour" between consecutive STABLE runs (aligns with search index propagation). For within-run nondeterminism: allow up to 2 retries per query (on HTTP errors only, not ranking failures). Log the full result set (top-5 IDs, query string, timestamp) for each A7 sample. If a file passes on retry 1 but fails on retry 2, it counts as a MISS (not a pass).

**Open questions:**
- If two consecutive fresh-session runs achieve max_misses=0 but a 3rd run gets 1 miss, does Phase 16.4 gate fail? (Recommendation: no ‚Äî 2 consecutive STABLE is sufficient per ROADMAP.)

**Confidence:** ‚ö†Ô∏è Raised by OpenAI; important for Phase 16.4 gate definition.

---

### üîç 7. Routing Enforcement Mechanism: State Machine Guard vs. Pre-Condition Check

**What needs to be decided:**
Whether preventing non-book files from reaching `ai_metadata_status='approved'` without batch-extract should be enforced at the FSM transition level (guard in python-statemachine) or at the pre-condition check level (the `_get_pending_files()` fix).

**Why it's ambiguous:**
The phase description says to fix `_get_pending_files()` (which is the extraction pipeline entry point). But the FSM controls upload approval. If a file has scanner-approved status but bypassed extraction, the upload FSM may still accept it. The question is: where is the enforcement point?

**Provider synthesis:**
- **OpenAI:** Recommends adding a guard in the FSM transition so that `approve()` requires `batch_extract_status == complete` for non-book files.

**Proposed implementation decision:**
Two-layer enforcement:
1. Fix `_get_pending_files()` to include all non-book files without primary_topics (not just .txt, not just LIKE filters)
2. Add a pre-upload invariant check (not necessarily FSM guard) in `get_fsm_pending_files()` that verifies non-book files have primary_topics before queuing them for upload. This is consistent with Phase 16.2's existing "upload gate enforces scan ‚Üí batch-extract ‚Üí approve ‚Üí upload ordering."

Full FSM guard modification is out of scope ‚Äî the FSM was locked in Phase 13 and adding a new guard risks introducing the `InvalidDefinition` errors seen in Phase 12.

**Open questions:**
- Does `get_fsm_pending_files()` already have a primary_topics check, or does it only check `ai_metadata_status='approved'`?

**Confidence:** üîç Identified by one provider; addressable with existing architecture.

---

## Summary: Decision Checklist

Before planning, confirm:

**Tier 1 (Blocking ‚Äî must resolve before Plan 16.4-01):**
- [ ] What is the expected post-routing-fix corpus count (1,749 or 1,809)?
- [ ] What is the current count of scanner-approved non-book files lacking primary_topics?

**Tier 2 (Important ‚Äî resolve before Plan 16.4-03):**
- [ ] What byte threshold value to use for BOOK_SIZE_BYTES?
- [ ] What are the 3 concrete query strategy templates for the retrievability audit?
- [ ] What rank threshold counts as "retrievable" (top-1, top-5)?

**Tier 3 (Design ‚Äî addressable in each plan):**
- [ ] Audit script resumability mechanism
- [ ] A7 consecutive run time gap
- [ ] Routing enforcement layer (pre-upload check vs FSM guard)

---

*Multi-provider synthesis: OpenAI gpt-5.2, Gemini Pro (Perplexity unavailable ‚Äî 502 error)*
*Generated: 2026-02-25*
