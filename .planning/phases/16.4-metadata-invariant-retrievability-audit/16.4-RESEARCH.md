# Phase 16.4: Metadata Pipeline Invariant + Comprehensive Retrievability Audit - Research

**Researched:** 2026-02-25
**Domain:** SQLite metadata routing, Gemini File Search retrievability, batch extraction pipeline
**Confidence:** HIGH

## Summary

Phase 16.4 fixes two structural violations and then proves the entire indexed corpus is independently retrievable. The routing violation is that 40 Episode files reached `gemini_state='indexed'` with `ai_metadata_status='failed_validation'` and zero `file_primary_topics` entries -- they were uploaded before the Phase 16.2 upload gate was added. Additionally, 1 Book file ("Ayn Rand - Philosophy - Who Needs It.txt", 552KB) was marked `skipped` at ~115K tokens but is actually within extraction range since much larger files (up to 3.5MB) were successfully extracted. The 60 Office Hour files are already batch-extracted (status `needs_review`, all 60 have `file_primary_topics`), awaiting approval.

The retrievability audit needs to test all 1,749 indexed files across 3 query strategies. The existing `check_stability.py` A7 implementation already demonstrates the exact API pattern needed (stem-based query via `generate_content` with `FileSearch` tool, match via `grounding_chunks[].retrieved_context.title` against `store_doc_id` prefix). The audit script is a scaled-up version of A7's sampling logic, run exhaustively with resumability. The critical unknown is whether 40 failed_validation Episodes and the 1 skipped Book can achieve zero-tolerance after re-extraction and re-upload.

**Primary recommendation:** Fix the routing violations first (re-extract 40 Episode files + 1 Book, approve 60 OH files), then run the comprehensive retrievability audit. Define `BOOK_SIZE_BYTES = 830,000` (1.5x the 552KB boundary file).

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

1. **Corpus Count Discrepancy:** Confirm via DB query at start of Plan 16.4-01 before any code changes. Query: `SELECT COUNT(*) FROM files WHERE ai_metadata_status='approved' AND (primary_topics IS NULL OR json_array_length(primary_topics) < 8)`
2. **Episode Zero-Tolerance Achievability:** Let Plan 16.4-03 data decide. Run all 3 strategies empirically. Document affirmative evidence if Episodes cannot achieve zero misses. Do NOT pre-exclude or pre-conclude.
3. **Book-Size Threshold:** Empirically derived -- query largest successfully-extracted file, multiply by 1.5. Define as BOOK_SIZE_BYTES in src/objlib/constants.py (or equivalent).
4. **Query Strategy Templates:**
   - Strategy 1 (Stem-only): query = filename stem
   - Strategy 2 (Stem + aspects): query = "{stem} {top-3 topic_aspects}"
   - Strategy 3 (Topics + course): query = "{course} {top-3 primary_topics}"
   - Threshold: file counted as "found" if it appears in top-5 results
5. **Routing Enforcement:** Two-layer check (no FSM modification):
   - Layer 1: Fix `_get_pending_files()` in batch_orchestrator.py (include .md files, check primary_topics)
   - Layer 2: Pre-upload invariant check in `get_fsm_pending_files()`
6. **Audit Resumability:** JSON progress file keyed by `{file_id}_{strategy_num}`. Saved to phase directory.
7. **A7 Time Gap:** At least 1 hour between consecutive STABLE runs, fresh sessions required.

### Claude's Discretion
None explicitly listed -- all decisions were locked.

### Deferred Ideas (OUT OF SCOPE)
None listed.
</user_constraints>

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| google-genai | current | Gemini File Search API (query + grounding chunks) | Already used in check_stability.py and search client |
| aiosqlite | current | Async SQLite for DB queries | Already used project-wide |
| sqlite3 | stdlib | Sync SQLite for scripts | Already used in check_stability.py |
| keyring | current | API key retrieval | Already used project-wide |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| asyncio | stdlib | Semaphore-based rate limiting for audit script | Rate-limit concurrent Gemini API calls |
| json | stdlib | Progress file persistence for audit resumability | Save/load audit results |
| pathlib | stdlib | File path manipulation | Stem extraction for query construction |
| rich | current | Console output, tables, progress bars | Already used in CLI |
| typer | current | CLI command framework | Already used for `objlib metadata audit` |

### Alternatives Considered
None -- this phase uses exclusively existing project dependencies.

## Architecture Patterns

### Pattern 1: Gemini File Search Query + Grounding Chunk Matching

**What:** Query the Gemini API via `generate_content()` with a `FileSearch` tool, then inspect `response.candidates[0].grounding_metadata.grounding_chunks` to find the target file.

**When to use:** For the retrievability audit (Plan 16.4-03) and updated A7 (Plan 16.4-04).

**Verified implementation from check_stability.py (lines 554-608):**
```python
# Source: scripts/check_stability.py, lines 554-608
response = self.client.models.generate_content(
    model=SEARCH_MODEL,   # "gemini-2.5-flash"
    contents=query,
    config=genai_types.GenerateContentConfig(
        tools=[genai_types.Tool(
            file_search=genai_types.FileSearch(
                file_search_store_names=[self.store_resource_name]
            )
        )]
    ),
)

# Match target file in grounding chunks
if response.candidates:
    gm = getattr(response.candidates[0], "grounding_metadata", None)
    if gm:
        chunks = getattr(gm, "grounding_chunks", []) or []
        for chunk in chunks[:10]:  # top-10 (Phase 16.4 uses top-5)
            rc = getattr(chunk, "retrieved_context", None)
            if not rc:
                continue
            title_in_result = getattr(rc, "title", "") or ""
            # Match via store_doc_id prefix (covers all 1,749 files)
            store_doc_prefix = store_doc_id.split("-")[0] if store_doc_id else ""
            if store_doc_prefix and title_in_result == store_doc_prefix:
                found = True
                break
```

**Key fields in `retrieved_context`:**
- `title`: Returns the 12-char file resource ID (NOT the display_name). Matches `store_doc_id.split("-")[0]`.
- `uri`: File URI
- `text`: The matched text chunk
- `document_name`: Full store document resource name

**Matching strategy (Phase 11 identity contract):** `retrieved_context.title` == first segment of `gemini_store_doc_id` (split by `-`). This covers all 1,749 indexed files including 1,075 with NULL `gemini_file_id`.

### Pattern 2: Batch Extraction Pipeline Entry Point

**What:** `_get_pending_files()` in `batch_orchestrator.py` is the gate for which files enter the Mistral batch extraction pipeline.

**Current implementation (line 369-383):**
```python
def _get_pending_files(self, max_files: int | None) -> list[dict]:
    """Get pending files from database for batch extraction."""
    query = """
        SELECT file_path, metadata_json
        FROM files
        WHERE ai_metadata_status = 'pending'
          AND (file_path LIKE '%.txt' OR file_path LIKE '%.md')
        ORDER BY file_path
    """
```

**Routing violation:** This function only picks up files with `ai_metadata_status='pending'`. The 40 Episode files with `failed_validation` status will NOT be picked up for re-extraction unless their status is first reset to `'pending'`.

### Pattern 3: Upload Gate (FSM Pending Files)

**What:** `get_fsm_pending_files()` in `state.py` is the gate for which files enter the upload pipeline.

**Current implementation (line 694-722):**
```python
async def get_fsm_pending_files(self, limit: int = 50) -> list[dict]:
    """Return files in untracked gemini_state ready for FSM upload."""
    db = self._ensure_connected()
    cursor = await db.execute(
        """SELECT file_path, content_hash, filename, file_size,
                  metadata_json, version, gemini_state
           FROM files
           WHERE gemini_state = 'untracked'
             AND (filename LIKE '%.txt' OR filename LIKE '%.md')
             AND ai_metadata_status = 'approved'
             AND EXISTS (SELECT 1 FROM file_primary_topics pt
                         WHERE pt.file_path = files.file_path)
           ORDER BY file_path
           LIMIT ?""",
        (limit,),
    )
```

**Status (Phase 16.2 verified):** This gate already checks `ai_metadata_status = 'approved'` AND `EXISTS(file_primary_topics)`. The gate is sound. A file cannot reach upload without approved status AND topic entries. The violation happened BEFORE this gate was added.

### Pattern 4: Identity Header Builder

**What:** `build_identity_header()` in `header_builder.py` constructs metadata headers prepended to file content before upload.

**Header structure:**
```
--- DOCUMENT METADATA ---
Title: {filename stem}
Course: {parent directory name}
Class: Class {XX-XX}   (if class-number file)
Topic: {scanner topic}
Tags: {primary_topics space-separated}
Aspects: {topic_aspects semicolon-separated}
--- END METADATA ---
```

**Relevance to retrievability:** The identity header fields are what make files discriminable in semantic search. Strategy 1 (stem-only) queries the Title field. Strategy 2 (stem + aspects) queries Title + Aspects. Strategy 3 (topics + course) queries Tags + Course.

### Pattern 5: Metadata Audit Command

**What:** `objlib metadata audit` checks 4 invariant conditions (exit code 1 if any fail).

**Current conditions (lines 3302-3529 of cli.py):**
1. Zero approved files without `file_primary_topics` entries
2. Zero skipped files without `error_message`
3. Zero pending non-enrichable extensions (only .txt/.md may be pending)
4. Phase 16.3 readiness (MOTM + Other-stem at 100% coverage)
5. (Informational) Count of `needs_review` files

**What's missing for Phase 16.4:**
- Per-series breakdown (Courses, MOTM, Episodes, OH, Books, Other)
- Check for `failed_validation` files that are indexed (the 40 Episodes)
- Check that ALL indexed non-book files have `file_primary_topics`

### Anti-Patterns to Avoid
- **Modifying the FSM:** The FSM was locked in Phase 13. Adding new guards risks `InvalidDefinition` errors. Use pre-condition checks instead.
- **Using `_get_pending_files()` for re-extraction without resetting status:** The function only queries `ai_metadata_status = 'pending'`. Files must be reset to `pending` before re-extraction.
- **Hardcoding book size thresholds:** Use a named constant `BOOK_SIZE_BYTES` with empirical derivation, not inline magic numbers.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Rate limiting for audit | Custom token bucket | `asyncio.Semaphore(N)` | Simple, proven, already used in upload orchestrator |
| Retry on 429/5xx | Custom retry loop | `tenacity` with exponential backoff | Already project dependency, used in search client |
| Progress persistence | Custom file format | JSON dict keyed by `{file_id}_{strategy}` | Simple, human-readable, resumable |
| Store resolution | Manual API listing | `GeminiSearchClient.resolve_store_name()` or check_stability.py pattern | Already implemented |
| Citation matching | Custom logic | Store doc prefix matching from check_stability.py A7 | Verified correct for all 1,749 files |

**Key insight:** The retrievability audit is essentially check_stability.py's Assertion 7 run exhaustively instead of on a sample. The same API pattern, matching logic, and query construction apply -- scaled to all 1,749 files with 3 strategies and resumability.

## Common Pitfalls

### Pitfall 1: Confusing `file_primary_topics` with `metadata_json.primary_topics`
**What goes wrong:** A file may have `primary_topics` in its `file_metadata_ai.metadata_json` but zero rows in `file_primary_topics` table. The upload gate checks `file_primary_topics` (the table), not the JSON.
**Why it happens:** `file_primary_topics` is only populated by `_save_extracted_metadata()` which runs on validation SUCCESS. The 40 `failed_validation` Episode files have AI metadata records with topic arrays, but validation failed, so topics were never written to the table.
**How to avoid:** Always check `EXISTS(SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = f.file_path)` for topic presence. The JSON is not authoritative.
**Warning signs:** A file has `file_metadata_ai` records but `SELECT COUNT(*) FROM file_primary_topics WHERE file_path = ?` returns 0.

### Pitfall 2: The 40 Episode `failed_validation` Files Are Indexed
**What goes wrong:** These files were uploaded to Gemini store BEFORE the Phase 16.2 upload gate was added. They are `gemini_state='indexed'` but have `ai_metadata_status='failed_validation'` and zero `file_primary_topics` entries.
**Why it happens:** The upload pipeline ran before the metadata completeness gate existed.
**How to avoid:** Plan 16.4-01 must reset these 40 files to `ai_metadata_status='pending'`, re-extract them, and validate. They do NOT need re-upload since they already have identity headers (Phase 16.3).
**Warning signs:** `SELECT COUNT(*) FROM files WHERE gemini_state='indexed' AND ai_metadata_status='failed_validation'` returns 40.

### Pitfall 3: Book Threshold Boundary Case
**What goes wrong:** "Ayn Rand - Philosophy - Who Needs It.txt" (552,798 bytes, ~115K tokens) was marked `skipped` because it exceeded `MAX_DOCUMENT_TOKENS = 100_000`. But larger .txt books (up to 3.5MB) were successfully extracted -- they just weren't skipped by this check. The skipped file IS indexed but has no `file_primary_topics`.
**Why it happens:** The `_mark_oversized()` check in `batch_orchestrator.py` correctly skipped this file (it genuinely exceeds 100K token estimate), but the file was then uploaded to Gemini without enriched metadata. The question is: is it a "book" (legitimately skipped) or should it be extracted?
**How to avoid:** Define `BOOK_SIZE_BYTES` empirically. At 552KB, this file is near the boundary. The next-smallest approved .txt book is 568KB ("Principles of Grammar"). A threshold of ~830KB (1.5x 552KB) would classify this as a book. But note: the 3.5MB Atlas Shrugged .txt WAS successfully extracted, meaning the token estimation is very conservative.
**Warning signs:** `MAX_DOCUMENT_TOKENS = 100_000` hardcoded in `batch_orchestrator.py` without a named constant.

### Pitfall 4: Needs-Review Files Block Zero-Tolerance
**What goes wrong:** The 60 OH files have `ai_metadata_status='needs_review'`, not `'approved'`. The upload gate requires `'approved'`. If these files need re-upload after approval, they'd need to go through `reset_existing` flow.
**Why it happens:** Batch extraction assigned `needs_review` due to low confidence scores and filename coherence mismatches.
**How to avoid:** These 60 files are already indexed with identity headers. They have `file_primary_topics`. The `needs_review` status is about metadata confidence, not routing. For the retrievability audit, they should be included since they ARE indexed.
**Warning signs:** `objlib metadata audit` condition 5 shows 60 `needs_review` files.

### Pitfall 5: Query Strategies Require Different Metadata
**What goes wrong:** Strategy 3 (`{course} {top-3 primary_topics}`) requires `file_primary_topics` entries. The 40 `failed_validation` Episodes and 1 skipped Book don't have these. Strategy 2 (`{stem} {top-3 topic_aspects}`) requires `topic_aspects` from `file_metadata_ai` -- same problem.
**Why it happens:** Strategies 2 and 3 depend on AI-extracted metadata that the 41 files lack.
**How to avoid:** Fix the routing violations BEFORE running the retrievability audit. The audit should only run after Plan 16.4-02 confirms `objlib metadata audit` exits 0 for all indexed files.
**Warning signs:** The audit script would need fallback logic for files without topics/aspects, defeating the purpose of the invariant.

### Pitfall 6: Rate Limits for 5,247+ API Calls
**What goes wrong:** 1,749 files x 3 strategies = 5,247 Gemini API calls. At ~60 RPM, this takes ~87 minutes minimum.
**Why it happens:** Gemini API rate limits.
**How to avoid:** Use `asyncio.Semaphore` for concurrency, exponential backoff on 429s, JSON progress file for resumability, `--strategy` flag to run one strategy at a time.
**Warning signs:** 429 errors mid-audit causing result loss.

## Code Examples

### Current Database State (verified 2026-02-25)
```
Indexed files by series:
  Courses:      702 (702 with topics, 0 issues)
  MOTM:         468 (468 with topics, 0 issues)
  Episodes:     334 (294 approved+topics, 40 failed_validation NO topics)
  Other:        140 (140 with topics, 0 issues)
  Office Hours:  78 (78 with topics, 18 approved + 60 needs_review)
  Books:         27 (26 with topics, 1 skipped NO topics)
  TOTAL:       1,749

  ai_metadata_status breakdown:
    approved:         1,648
    needs_review:        60  (ITOE OH files, all have file_primary_topics)
    failed_validation:   40  (Episodes, all indexed, NO file_primary_topics)
    skipped:            138  (137 pdf/epub + 1 oversized .txt)

  gemini_state breakdown:
    indexed:    1,749
    untracked:    136  (all skipped non-txt/md files)
```

### Resetting Failed Validation Files for Re-Extraction
```python
# Source: pattern from batch_orchestrator.py _mark_failed()
# To re-route failed_validation Episodes back to pending:
UPDATE files
SET ai_metadata_status = 'pending',
    error_message = NULL
WHERE ai_metadata_status = 'failed_validation'
  AND gemini_state = 'indexed'
  AND (file_path LIKE '%.txt' OR file_path LIKE '%.md');
-- Expected: 40 rows affected
```

### BOOK_SIZE_BYTES Derivation
```python
# Source: DB query results 2026-02-25
# The single skipped .txt file:
#   Ayn Rand - Philosophy - Who Needs It.txt: 552,798 bytes (~115K tokens)
# The next-smallest approved .txt book:
#   Leonard Peikoff - Principles of Grammar.txt: 568,564 bytes
# Largest approved .txt:
#   La rebelion de Atlas - Ayn Rand.txt: 3,475,970 bytes
#
# Note: The token-based skip (MAX_DOCUMENT_TOKENS=100_000) in batch_orchestrator.py
# caught this file. The byte-based BOOK_SIZE_BYTES is a DIFFERENT concept:
# it defines "this file is too large to be a course/episode transcript, it's a book."
# All .txt books in /Books/ range from 221KB to 3.5MB.
# The smallest non-book .txt file that would need extraction is Episode-sized (~55KB max).
# BOOK_SIZE_BYTES should separate "transcripts" from "books."
#
# Empirical anchor: The skipped file is 552KB. 1.5x = 829,197 bytes.
# This safely classifies all files < 553KB as non-books (requiring extraction)
# and all files >= 830KB as books (allowed to skip extraction).
BOOK_SIZE_BYTES = 830_000  # ~810KB
```

### Retrievability Audit Query Construction (3 Strategies)
```python
# Source: derived from check_stability.py A7 + CONTEXT.md decisions

def build_query(strategy: int, filename: str, conn: sqlite3.Connection) -> str:
    """Build query string for a given strategy."""
    stem = Path(filename).stem

    if strategy == 1:
        # Stem-only: matches Title field in identity header
        return f"What is '{stem}' about?"

    elif strategy == 2:
        # Stem + top-3 topic_aspects
        row = conn.execute(
            "SELECT metadata_json FROM file_metadata_ai "
            "WHERE file_path = (SELECT file_path FROM files WHERE filename = ?) "
            "AND is_current = 1",
            (filename,),
        ).fetchone()
        aspects = []
        if row and row[0]:
            meta = json.loads(row[0])
            aspects = (meta.get("topic_aspects") or [])[:3]
        aspect_str = " ".join(aspects)
        return f"What is '{stem} {aspect_str}' about?"

    elif strategy == 3:
        # Course + top-3 primary_topics
        file_row = conn.execute(
            "SELECT file_path FROM files WHERE filename = ?",
            (filename,),
        ).fetchone()
        if not file_row:
            return f"What is '{stem}' about?"  # fallback
        file_path = file_row[0]
        course = Path(file_path).parent.name
        topics = conn.execute(
            "SELECT topic_tag FROM file_primary_topics "
            "WHERE file_path = ? LIMIT 3",
            (file_path,),
        ).fetchall()
        topic_str = " ".join(r[0] for r in topics)
        return f"What is '{course} {topic_str}' about?"
```

### Audit Progress File Format
```json
{
  "metadata": {
    "store": "objectivism-library",
    "started_at": "2026-02-26T10:00:00Z",
    "total_files": 1749
  },
  "results": {
    "/path/to/file.txt_1": {
      "filename": "file.txt",
      "strategy": 1,
      "found": true,
      "rank": 3,
      "query": "What is 'file stem' about?",
      "timestamp": "2026-02-26T10:01:23Z"
    }
  }
}
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| A7: tolerance=2, excludes Episodes+OH | A7: max_misses=0, no exclusions | Phase 16.4 (target) | Zero-tolerance retrievability |
| Scanner-only approval path for small files | Mandatory batch-extract for all non-book txt/md | Phase 16.2 (upload gate) | But 40 pre-existing violations remain |
| Token-based oversized check (MAX_DOCUMENT_TOKENS) | BOOK_SIZE_BYTES named constant | Phase 16.4-01 (target) | Explicit, greppable threshold |
| A7 samples 5 random files | Full corpus audit with 3 strategies | Phase 16.4-03 (target) | Exhaustive proof of retrievability |

**Deprecated/outdated:**
- `MAX_DOCUMENT_TOKENS = 100_000` in `batch_orchestrator.py`: Still functional but should be supplemented with `BOOK_SIZE_BYTES` for routing decisions. Token estimation vs byte threshold serve different purposes.

## Open Questions

1. **Should the 40 failed_validation Episodes be re-extracted or repaired?**
   - What we know: All 40 have `file_metadata_ai` records with valid-looking metadata (primary_topics with 5-7 items, topic_aspects, summaries). The failures were Mistral response format issues ("JSON array instead of object", "no valid JSON").
   - What's unclear: Can the existing `file_metadata_ai` records be "repaired" (validated + written to `file_primary_topics`) without re-running extraction? The validator can expand/reduce topics to exactly 8.
   - Recommendation: Try repair first via `validate_extraction()` + `_save_extracted_metadata()` on the existing metadata. If repair succeeds (gets to exactly 8 topics), it avoids a full batch re-extraction. If not, reset to pending and re-extract.

2. **Should the 1 skipped Book file be re-classified?**
   - What we know: "Ayn Rand - Philosophy - Who Needs It.txt" (552KB) is indexed, skipped, no topics. It's in the Books directory.
   - What's unclear: Should books legitimately skip extraction and remain without topics? The current audit would fail for this file.
   - Recommendation: If it's a book, it's a legitimate skip. The audit condition should be: "all indexed files are either (a) books (`ai_metadata_status='skipped'` AND exceeds `BOOK_SIZE_BYTES`) or (b) have `file_primary_topics`." At 552KB this file IS below the proposed 830KB threshold, meaning Plan 16.4-01 would re-classify it as non-book and extract it.

3. **What Gemini API rate limit applies?**
   - What we know: The project uses the standard Gemini API tier. check_stability.py makes 5-10 API calls per run without issues.
   - What's unclear: Exact RPM limit for `generate_content` with FileSearch tool.
   - Recommendation: Start with `Semaphore(10)` (10 concurrent), back off to `Semaphore(3)` if 429s occur. The `--strategy` flag lets the audit run one strategy at a time across multiple sessions.

4. **What happens to the audit when re-extracted Episodes get re-uploaded?**
   - What we know: The 40 Episodes are already indexed with identity headers. Re-extraction changes metadata but not the stored file content.
   - What's unclear: Do these files need re-upload after metadata repair? The identity header includes Tags (from `file_primary_topics`). If topics change, the header in the store is stale.
   - Recommendation: After re-extraction, these 40 files need re-upload with updated identity headers containing the new Tags. This is a `--reset-existing` operation for these 40 files only.

## Sources

### Primary (HIGH confidence)
- `src/objlib/extraction/batch_orchestrator.py` lines 369-383 -- `_get_pending_files()` implementation, routing gate
- `src/objlib/upload/state.py` lines 694-722 -- `get_fsm_pending_files()` implementation, upload gate with Phase 16.2 enforcement
- `scripts/check_stability.py` lines 444-634 -- A7 implementation, query pattern, matching logic, current exclusion state
- `src/objlib/cli.py` lines 3302-3529 -- `metadata audit` command, 4 invariant conditions
- `src/objlib/upload/header_builder.py` lines 1-141 -- Identity header structure (Title/Course/Class/Topic/Tags/Aspects)
- `data/library.db` -- Direct queries (2026-02-25) for all corpus counts and distributions

### Secondary (MEDIUM confidence)
- `src/objlib/search/client.py` lines 51-85 -- Gemini `generate_content` with FileSearch tool configuration including `top_k` parameter
- `src/objlib/search/citations.py` lines 28-83 -- Grounding chunk extraction pattern (`retrieved_context.title`, `.text`, `.uri`)

### Tertiary (LOW confidence)
- Gemini API rate limits: Assumed ~60 RPM based on project experience; actual limit not verified against official documentation.

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- all existing project dependencies, no new libraries needed
- Architecture: HIGH -- all patterns verified from actual source code and DB queries
- Pitfalls: HIGH -- identified from direct DB evidence (40 failed_validation, 1 skipped book, 60 needs_review)
- Retrievability audit design: MEDIUM -- API pattern verified from check_stability.py but rate limits and full-corpus behavior unverified

**Research date:** 2026-02-25
**Valid until:** 2026-03-07 (DB state changes with each plan execution)

**Critical DB state snapshot (2026-02-25):**
| Metric | Value |
|--------|-------|
| Total indexed | 1,749 |
| Indexed with file_primary_topics | 1,709 |
| Indexed WITHOUT file_primary_topics | 40 (failed_validation Episodes) + 0 (1 Book has no topics but IS skipped) |
| Untracked | 136 (all non-txt/md skipped) |
| needs_review (indexed, have topics) | 60 |
| Largest extracted .txt | 3,475,970 bytes (La rebelion de Atlas) |
| Skipped .txt threshold | 552,798 bytes (~115K tokens) |
| Proposed BOOK_SIZE_BYTES | 830,000 |
