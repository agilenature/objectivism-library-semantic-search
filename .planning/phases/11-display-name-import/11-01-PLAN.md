---
phase: 11-display-name-import
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - spike/phase11_spike/__init__.py
  - spike/phase11_spike/sdk_inspector.py
  - spike/phase11_spike/test_corpus.py
  - spike/phase11_spike/lag_measurement.py
  - spike/phase11_spike/spike.py
  - spike/phase11_spike/RESULTS.md
autonomous: true
user_setup: []

must_haves:
  truths:
    - "SDK source file path and line number for display_name serialization are documented"
    - "Round-trip test confirms submitted display_name equals returned display_name for 12+ files (or documents the normalization rule)"
    - "Import-to-visible lag P50/P95/P99 is measured empirically from 12+ test file imports"
    - "Test store is created, used, and deleted without polluting production store"
    - "Both File.display_name and Document.display_name are compared against submitted values"
  artifacts:
    - path: "spike/phase11_spike/sdk_inspector.py"
      provides: "Programmatic SDK source evidence collection"
      exports: ["collect_sdk_evidence"]
    - path: "spike/phase11_spike/test_corpus.py"
      provides: "Test file generation with edge-case display_names"
      exports: ["create_test_corpus"]
    - path: "spike/phase11_spike/lag_measurement.py"
      provides: "Import-to-visible lag measurement with exponential backoff polling"
      exports: ["measure_visibility_lag", "compute_percentiles"]
    - path: "spike/phase11_spike/spike.py"
      provides: "Combined spike runner: SDK inspection + round-trip + lag measurement + cleanup"
      exports: ["main"]
    - path: "spike/phase11_spike/RESULTS.md"
      provides: "Phase 11 SC1+SC2 empirical results document"
      contains: "P50"
  key_links:
    - from: "spike/phase11_spike/spike.py"
      to: "google.genai SDK"
      via: "genai.Client for upload_file, import_file, list_store_documents"
      pattern: "client\\.aio\\.(files\\.upload|file_search_stores)"
    - from: "spike/phase11_spike/spike.py"
      to: "spike/phase11_spike/sdk_inspector.py"
      via: "collect_sdk_evidence() called in Phase 1"
      pattern: "collect_sdk_evidence"
    - from: "spike/phase11_spike/spike.py"
      to: "spike/phase11_spike/lag_measurement.py"
      via: "measure_visibility_lag() called per file in Phase 3"
      pattern: "measure_visibility_lag"
---

<objective>
SDK source inspection for display_name contract + import lag measurement spike

Purpose: Produce HOSTILE-distrust-grade evidence that (1) display_name is caller-controlled (SDK source + round-trip), and (2) import-to-visible lag is empirically characterized with P50/P95/P99 across 12+ test files. This is the measurement-and-evidence phase -- no production code changes.

Output: spike/phase11_spike/ directory with SDK evidence, round-trip results, latency data, and RESULTS.md summarizing all findings.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-display-name-import/11-CONTEXT.md
@.planning/phases/11-display-name-import/11-RESEARCH.md
@.planning/phases/11-display-name-import/CLARIFICATIONS-ANSWERED.md
@src/objlib/upload/client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: SDK source inspector + test corpus generator + lag measurement module</name>
  <files>
    spike/phase11_spike/__init__.py
    spike/phase11_spike/sdk_inspector.py
    spike/phase11_spike/test_corpus.py
    spike/phase11_spike/lag_measurement.py
  </files>
  <action>
Create the spike/phase11_spike/ package with three modules:

**spike/phase11_spike/__init__.py:**
Standard package init, brief docstring ("Phase 11 spike: display_name stability and import lag measurement").

**spike/phase11_spike/sdk_inspector.py — `collect_sdk_evidence() -> dict`:**
Programmatic inspection of the installed google-genai SDK source to document display_name serialization.
1. Use `inspect.getfile()` on `google.genai.files`, `google.genai.types`, `google.genai._common` to get absolute file paths.
2. Read each file and search for the specific patterns:
   - In files.py: find the line containing `display_name` assignment in the async upload method (approximately line 1066 per research, but VERIFY by reading the actual file -- line numbers may differ from research estimates).
   - In types.py: find `class File` definition and its `display_name` field, AND `class Document` definition and its `display_name` field, AND `class UploadFileConfig` and its `display_name` field.
   - In _common.py: find `alias_generator` or `to_camel` to confirm Pydantic serializes `display_name` -> `displayName` in JSON.
3. Return a dict with structure:
   ```python
   {
       "sdk_version": str,  # from google.genai.__version__ or package metadata
       "files_py": {"path": str, "lines": [{"line_no": int, "content": str, "description": str}]},
       "types_py": {"path": str, "lines": [{"line_no": int, "content": str, "description": str}]},
       "common_py": {"path": str, "lines": [{"line_no": int, "content": str, "description": str}]},
       "conclusion": str,
   }
   ```
4. The function must find ACTUAL line numbers by reading the source files, not hardcoded line numbers from research. The research numbers are approximate guides only.

**spike/phase11_spike/test_corpus.py — `create_test_corpus(base_dir: str) -> list[dict]`:**
Generate 14 small .txt test files with deliberately tricky display_names to stress-test normalization:
- 4 basic cases: simple name, lowercase_only, UPPERCASE_ONLY, MiXeD CaSe
- 4 special character cases: parentheses, dashes, ampersand (Q&A), period in middle (Ch.1)
- 2 whitespace edge cases: leading spaces, trailing spaces
- 2 realistic library filenames: "Ayn Rand - Atlas Shrugged (1957).txt", "OCON 2023 - Harry Binswanger - Q&A.txt"
- 1 long name: 500 chars + ".txt" (near 512 limit)
- 1 internal multiple spaces: "Multiple   Internal   Spaces.txt"

Each file should have content varying in size to test size correlation with lag:
- 4 files at ~1KB (short paragraph)
- 4 files at ~10KB (repeated paragraphs)
- 4 files at ~50KB (larger content)
- 2 files at ~100KB (substantial content)

Return list of dicts: `[{"local_path": str, "display_name": str, "size_bytes": int, "index": int}]`

Local filenames should be sanitized (test_file_00.txt etc) but display_names should be the tricky originals.

**spike/phase11_spike/lag_measurement.py — two functions:**

`measure_visibility_lag(client, store_name, document_name, max_wait=300.0, initial_interval=0.5, backoff_factor=1.5, max_interval=10.0) -> dict`:
- Uses `client.aio.file_search_stores.documents.get(name=document_name)` FIRST (O(1) check per research finding #2 -- document_name is available from ImportFileOperation.response).
- If documents.get() raises (404 or similar), falls back to listing via `client.aio.file_search_stores.documents.list(parent=store_name)` and scanning for the document_name.
- Records BOTH timings: lag_to_get (when documents.get() succeeds) and lag_to_list (when document appears in list). These may differ due to eventual consistency.
- Uses `time.perf_counter()` for timing.
- Polling loop with exponential backoff per locked decision: 0.5s start, 1.5x factor, 10s max interval, 300s absolute timeout.
- Returns dict: `{"lag_to_get_seconds": float|None, "lag_to_list_seconds": float|None, "document_state": str|None, "timed_out": bool, "polls_count": int}`

`compute_percentiles(latencies: list[float]) -> dict`:
- Uses `statistics.quantiles(data, n=100)` for N>=4, falls back to median for smaller N.
- Returns: `{"n": int, "min": float, "p50": float, "p95": float, "p99": float, "max": float, "mean": float, "stdev": float}`
- Rounds all values to 3 decimal places.
- Handles edge case of empty list (returns error dict).

IMPORTANT: Do NOT import from src/objlib/upload/client.py. This spike uses the raw google-genai SDK directly (same pattern as Phase 8/9/10 spikes -- independent of objlib).
  </action>
  <verify>
    python -c "from spike.phase11_spike.sdk_inspector import collect_sdk_evidence; e = collect_sdk_evidence(); assert 'files_py' in e and 'types_py' in e and 'common_py' in e; print('SDK inspector: OK')"
    python -c "import tempfile; from spike.phase11_spike.test_corpus import create_test_corpus; files = create_test_corpus(tempfile.mkdtemp()); assert len(files) == 14; print(f'Test corpus: {len(files)} files created')"
    python -c "from spike.phase11_spike.lag_measurement import compute_percentiles; r = compute_percentiles([1.0, 2.0, 3.0, 4.0, 5.0]); assert 'p50' in r and 'p95' in r and 'p99' in r; print(f'Percentiles: {r}')"
  </verify>
  <done>
    SDK inspector returns evidence dict with actual file paths and line numbers from installed google-genai package. Test corpus generates 14 files with tricky display_names across 4 size buckets. compute_percentiles produces P50/P95/P99 from a list of floats. All three modules import and run without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Combined spike runner -- execute full measurement and produce RESULTS.md</name>
  <files>
    spike/phase11_spike/spike.py
    spike/phase11_spike/RESULTS.md
  </files>
  <action>
Create spike/phase11_spike/spike.py as the combined spike runner, executable with `python -m spike.phase11_spike.spike`. The script has 5 phases:

**Phase 1: SDK Source Inspection**
- Call `collect_sdk_evidence()` from sdk_inspector.py.
- Print a structured summary of the evidence (file paths, line numbers, conclusion).
- Store the evidence dict for the RESULTS.md report.

**Phase 2: Test Store Setup**
- Create a Gemini API client using `genai.Client(api_key=...)`. Get the API key from the system keyring: `import keyring; api_key = keyring.get_password("objlib-gemini", "api-key")`.
- Create a dedicated test store: `client.aio.file_search_stores.create(config={"display_name": "phase11-spike-test"})`. Store the resource name.
- Generate the test corpus using `create_test_corpus()` into a temp directory.
- Print: store name, number of test files, total size.

**Phase 3: Round-Trip + Lag Measurement**
For each of the 14 test files, sequentially (NOT concurrently -- avoid rate limits):
1. Record the submitted `display_name`.
2. Call `client.aio.files.upload(file=local_path, config={"display_name": display_name})`.
3. Poll `client.aio.files.get(name=file.name)` until file.state is ACTIVE (exponential backoff, 60s timeout). This is the Files API activation, not store visibility.
4. Call `client.aio.file_search_stores.import_file(file_search_store_name=store_name, file_name=file.name)`.
5. Poll the import operation until `done==True` using `client.aio.operations.get(operation)`.
6. Extract `document_name` from the completed operation's response (operation.response.get("documentName") or similar -- check the actual response structure).
7. Start the visibility timer (`time.perf_counter()`).
8. Call `measure_visibility_lag()` to measure lag until the document appears.
9. Once visible, retrieve the document (via documents.get or documents.list) and compare:
   - `file.display_name` (File API) vs submitted display_name
   - `document.display_name` (Store Document) vs submitted display_name
   - Record: exact_match (bool), case_match (bool), any normalization observed.
10. Collect the measurement record: `{"index": int, "display_name_submitted": str, "file_display_name": str, "doc_display_name": str, "exact_match": bool, "lag_to_get": float|None, "lag_to_list": float|None, "size_bytes": int, "document_state": str}`.
11. Add a 1-second delay between files to avoid rate limits.

Print per-file results as they complete (Rich table or simple print).

IMPORTANT on extracting document_name from operation response:
- The operation object from `import_file()` is an `ImportFileOperation`.
- When done==True, the response may be in `operation.response` as a dict or object.
- Try: `operation.response.document_name` (attribute access), or `operation.response.get("documentName")` (dict access), or inspect the raw response.
- If document_name is not available from the operation, fall back to listing all store documents and matching by display_name. This fallback is acceptable -- the research said document_name SHOULD be there but it is not certain.

**Phase 4: Statistics + Report**
- Separate latencies into overall and by size bucket (1KB, 10KB, 50KB, 100KB).
- Compute P50/P95/P99 for each group using `compute_percentiles()`.
- Check display_name round-trip: count exact matches, count case-only matches, count mismatches.
- Print a summary table.

**Phase 5: Cleanup**
- Delete the test store with `force=True`: `client.aio.file_search_stores.delete(name=store_name, config={"force": True})`.
- Delete all uploaded files from Files API: `client.aio.files.delete(name=file.name)` for each file.
- Use try/finally to ensure cleanup runs even if Phase 3/4 fail.
- Delete the temp directory with test files.

**After the spike completes, write spike/phase11_spike/RESULTS.md:**
This is a structured report (NOT generated by the spike script itself -- written by Claude after the spike runs). It must contain:
1. **SDK Evidence**: exact file paths, line numbers, and conclusion about display_name serialization path.
2. **Round-Trip Results**: table showing submitted vs returned display_name for all 14 files, with exact_match column.
3. **Latency Data**: P50/P95/P99 overall and by size bucket, with sample size noted.
4. **Key Observations**: any normalization detected, any correlation between file size and lag, any unexpected behavior.
5. **Implications for Phase 12**: recommended polling parameters, any risks identified.

The spike script must use `asyncio.run()` as the entry point (same pattern as Phase 9/10 spikes). All Gemini API calls go through the raw SDK client, not through objlib's GeminiFileSearchClient.

ERROR HANDLING: If any single file fails (upload error, import error, timeout), log the error and continue with the next file. Do not abort the entire spike. Record failures in the results. A spike with 10/14 successful measurements still produces valid P50/P95/P99 data.
  </action>
  <verify>
    python -m spike.phase11_spike.spike
    # Verify the spike runs to completion (this is a LIVE API test -- takes 5-15 minutes)
    # Check that RESULTS.md is created after the spike completes
    test -f spike/phase11_spike/RESULTS.md && echo "RESULTS.md exists"
    # Verify RESULTS.md contains the required sections
    grep -c "SDK Evidence" spike/phase11_spike/RESULTS.md
    grep -c "P50" spike/phase11_spike/RESULTS.md
    grep -c "Round-Trip" spike/phase11_spike/RESULTS.md
  </verify>
  <done>
    Spike runs to completion against live Gemini API. Test store is created, 12+ files are uploaded and measured, test store is deleted. RESULTS.md contains: (1) SDK source file paths and line numbers, (2) round-trip display_name comparison for all files, (3) P50/P95/P99 latency data overall and by size bucket. All data needed for Plan 11-02 trigger strategy decision is captured.
  </done>
</task>

</tasks>

<verification>
Phase 11 Plan 01 verification:
1. spike/phase11_spike/sdk_inspector.py returns evidence dict with real file paths and line numbers from installed SDK
2. spike/phase11_spike/test_corpus.py creates 14 files across 4 size buckets with tricky display_names
3. spike/phase11_spike/lag_measurement.py computes percentiles correctly
4. spike/phase11_spike/spike.py runs to completion against live Gemini API without errors
5. Test store is created and deleted (no pollution of production store)
6. RESULTS.md documents SDK evidence, round-trip results, and P50/P95/P99 latency data
7. At least 10 of 14 files produce successful measurements (allowing for transient failures)
</verification>

<success_criteria>
- SDK source evidence collected: file paths and line numbers for display_name in files.py, types.py, _common.py
- Round-trip verification complete: 12+ files tested, exact string comparison recorded
- Import-to-visible lag measured: P50/P95/P99 computed from 10+ successful measurements
- Both File.display_name and Document.display_name compared against submitted values
- Test store fully cleaned up (deleted with force=True)
- RESULTS.md committed with all evidence for Phase 11 gate assessment
</success_criteria>

<output>
After completion, create `.planning/phases/11-display-name-import/11-01-SUMMARY.md`
</output>
