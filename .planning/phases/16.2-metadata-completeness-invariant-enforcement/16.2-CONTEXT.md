# CONTEXT.md â€” Phase 16.2: Metadata Completeness Invariant Enforcement

**Generated:** 2026-02-24
**Phase Goal:** Every file in the library either has `primary_topics` populated (AI-enriched) or has `ai_metadata_status='skipped'` with a named reason â€” no file silently bypasses enrichment because of an extension filter or missing routing branch.
**Synthesis Source:** Multi-provider AI analysis (OpenAI gpt-5.2, Gemini Pro, Perplexity Sonar Deep Research) + live DB queries

---

## Live DB State (2026-02-24 baseline)

```
Total files in DB:    1885  (NOT 1749 â€” includes non-txt formats)
  .txt:   1748  (all uploaded to Gemini, form the "library")
  .pdf:     59  (pending â€” need to be marked skipped)
  .other:   51  (docx etc â€” pending â€” need to be marked skipped)
  .epub:    26  (already skipped â€” but error_message=None, missing named reason)
  .md:       1  (Bernstein Heroes book â€” pending, excluded by LIKE '%.txt' filter)

ai_metadata_status distribution:
  approved:  1748
  pending:   111   â† includes all .pdf, .docx, .md files
  skipped:    26   â† all .epub â€” but error_message=NULL (named reason missing!)

Primary topics coverage:
  Files with file_primary_topics entries: 1727
  Approved .txt files WITHOUT primary_topics rows: 21  â† SILENT PENDING (book .txts)
  These 21 have file_metadata_ai rows but no file_primary_topics rows
  They are .txt exports of the same books as the epub duplicates
```

**Key finding:** The 26 "silent pending" books mentioned in STATE.md have ALREADY been moved to `skipped` status (they're the 26 epubs). But: (1) their `error_message` is NULL, and (2) there are 21 DIFFERENT approved .txt book files without primary_topics â€” the text exports of those same books.

---

## Overview

Phase 16.2 enforces a completeness invariant across all 1885 DB-tracked files. The invariant is: every file is either AI-enriched (has primary_topics) or explicitly skipped with a documented reason. No file may be silently excluded by an extension filter without explicit classification.

**Confidence markers:**
- âœ… **Consensus** â€” All 3 providers identified this as critical
- âš ï¸ **Recommended** â€” 2 providers identified this as important
- ðŸ” **Needs Clarification** â€” 1 provider identified, potentially important

---

## Gray Areas Identified

### âœ… 1. Definition of "primary_topics populated" (Consensus)

**What needs to be decided:**
Is "populated" defined by the `file_primary_topics` junction table, by `metadata_json->>'primary_topics'` in `file_metadata_ai`, or both?

**Why it's ambiguous:**
The system stores primary_topics in TWO places:
- `file_primary_topics` table (rows per topic per file)
- `metadata_json` JSON blob in `file_metadata_ai` (includes `primary_topics` array)

The 21 silent-pending .txt books have `file_metadata_ai` entries (`has_metadata_ai=1`) but zero `file_primary_topics` rows â€” they passed extraction but topic insertion didn't persist. An audit checking only `metadata_json` would falsely pass them; an audit checking `file_primary_topics` would correctly catch them.

**Provider synthesis:**
- **OpenAI:** At least 1 non-empty topic element; null/empty array counts as not-populated
- **Gemini:** `approved AND json_array_length(primary_topics) > 0` â€” still using metadata_json
- **Perplexity:** Confirm which table is authoritative; choose one and enforce consistently

**Proposed implementation decision:**
The `file_primary_topics` table is authoritative. "Populated" = `EXISTS (SELECT 1 FROM file_primary_topics WHERE file_path = f.file_path)`. This catches the 21 broken .txt books that have metadata_ai but no topic rows. Audit SQL:
```sql
SELECT COUNT(*) FROM files f
WHERE f.ai_metadata_status = 'approved'
  AND NOT EXISTS (SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = f.file_path)
```
This currently returns 21 violations that must be remediated before audit can exit 0.

**Open questions:**
None â€” `file_primary_topics` is the authoritative join table for topics.

**Confidence:** âœ… All 3 providers agreed this definition is critical to get right

---

### âœ… 2. Scope of the Audit Invariant â€” DB has 1885 files, not 1749 (Consensus)

**What needs to be decided:**
Does the audit cover only the 1748 `.txt` files (the Gemini-indexed set), or all 1885 DB-tracked files?

**Why it's ambiguous:**
STATE.md references "1,749 files" throughout, but DB has 1885. The extra 136 files are .pdf (59), .docx/other (51), .epub (26 â€” already skipped), and .md (1). These non-txt files are tracked in the DB but not in the Gemini store.

**Provider synthesis:**
- **OpenAI:** DB rows are the source of truth for the invariant; missing-on-disk is its own category
- **Gemini:** Use DB rows, not filesystem, as authoritative list
- **Perplexity:** Audit all DB-tracked files; non-enrichable formats must be explicitly classified

**Proposed implementation decision:**
Audit ALL 1885 DB-tracked files. Each must satisfy exactly one of:
1. `ai_metadata_status='approved'` AND `file_primary_topics` entries exist â†’ **AI-enriched**
2. `ai_metadata_status='approved'` AND no `file_primary_topics` entries â†’ **scanner-only** (VIOLATION)
3. `ai_metadata_status='skipped'` AND `error_message IS NOT NULL` â†’ **explicitly skipped**
4. `ai_metadata_status='skipped'` AND `error_message IS NULL` â†’ **missing named reason** (VIOLATION)
5. `ai_metadata_status='pending'` â†’ **not yet processed** (VIOLATION if non-enrichable format)

Audit exits non-zero if ANY file is in a violation state.

**Confidence:** âœ… All 3 providers agreed audit scope must cover all DB-tracked files

---

### âœ… 3. Skip Reason Format for .epub/.pdf/.docx Files (Consensus)

**What needs to be decided:**
Where to store the named reason (existing `error_message` column vs. new column), and what the reason strings should be.

**Why it's ambiguous:**
Requirement 3 says "named reason in `error_message`". The 26 epubs are already `skipped` but `error_message=NULL`. The schema already has `error_message` in the files table (used by `_mark_failed()` and `_mark_skipped()`). OpenAI proposed a new `ai_metadata_skip_reason` column; all other providers used `error_message`.

**Provider synthesis:**
- **OpenAI:** Separate `ai_metadata_skip_reason` column with enum values (more rigorous)
- **Gemini:** Use `error_message` (no schema change)
- **Perplexity:** Use `error_message` (no schema change)

**Proposed implementation decision:**
Use existing `error_message` column â€” no schema migration needed. Standardized reason strings (not a DB enum, but fixed code constants):
```python
SKIP_REASONS = {
    ".epub": "epub format: text extraction not supported in v2.0",
    ".pdf":  "pdf format: text extraction not supported in v2.0",
    ".docx": "docx format: text extraction not supported in v2.0",
    ".other": "unsupported format: text extraction not supported in v2.0",
}
```
The audit validates that `error_message IS NOT NULL AND error_message != ''` for all skipped files.

**Confidence:** âœ… Consensus: use error_message; 2/3 providers explicitly recommended no schema change

---

### âœ… 4. Pending Non-Enrichable Files (.pdf, .docx â€” 110 files) (Consensus)

**What needs to be decided:**
The 110 non-.txt, non-.md pending files (.pdf=59, .docx/other=51) are currently excluded from `batch-extract` by `LIKE '%.txt'` but remain in `pending` state. How are they remediated?

**Why it's ambiguous:**
The LIKE filter silently excludes them without marking them as skipped â€” exactly the "silent bypass" the phase is designed to eliminate. These files must be explicitly classified before the audit can pass.

**Provider synthesis:**
- **OpenAI:** Capability-based routing; files that can't be enriched are explicitly skipped
- **Gemini:** Implement a one-time migration command to mark all .pdf/.epub as skipped
- **Perplexity:** Read-only audit + separate remediation commands

**Proposed implementation decision:**
Plan 16.2-01 includes a backfill command: `objlib metadata mark-unsupported` (or similar) that:
1. Marks all files with `ai_metadata_status='pending'` AND extension NOT IN (.txt, .md) as `skipped` with the standardized reason string
2. Updates the 26 existing skipped epubs to add the missing `error_message`
3. Is idempotent (re-runnable without side effects)

This is a one-time remediaton, not a schema change. The `_get_pending_files()` LIKE fix handles `.md` inclusion; the backfill handles everything else.

**Confidence:** âœ… All 3 providers agreed read-only audit + explicit remediation command is the right pattern

---

### âœ… 5. The 21 Silent-Pending .txt Book Files (Consensus)

**What needs to be decided:**
21 approved .txt files have `file_metadata_ai` rows but no `file_primary_topics` rows â€” they went through AI extraction but primary topics were not persisted to the junction table. How are they fixed without violating "sacred metadata" rules?

**Why it's ambiguous:**
These are .txt exports of the same books as the epub duplicates (Atlas Shrugged, Fountainhead, Objectivism PAOR, etc.). They have `metadata_json` in `file_metadata_ai` but the `file_primary_topics` insert step was skipped (likely a v1-era pipeline gap). The sacred metadata rule forbids deleting `metadata_json`, but these files need topics extracted.

**Provider synthesis:**
- **OpenAI:** Re-run enrichment; the sacred rule applies to metadata_json, not preventing re-extraction
- **Gemini:** Check if metadata_json has primary_topics array; if yes, insert from there (no API call needed)
- **Perplexity:** Decide whether to re-extract or backfill from existing metadata_json

**Proposed implementation decision:**
Two-step remediation:
1. **Check metadata_json first**: For each of the 21 files, inspect `metadata_json->>'primary_topics'`. If the array is present and non-empty, INSERT rows into `file_primary_topics` from the existing JSON (no API call, no re-extraction, sacred data preserved).
2. **If metadata_json has no primary_topics**: Reset `ai_metadata_status='pending'` and include in next `batch-extract` run (full re-extraction needed â€” these books were too large or had extraction failures).

This is part of the 16.2-01 plan tasks, executed before the audit can exit 0.

**Confidence:** âœ… All 3 providers agreed this needs investigation and fix before audit can pass

---

### âœ… 6. Phase 16.3 Readiness Row â€” MOTM and Other-stem Identification (Consensus)

**What needs to be decided:**
How to identify "MOTM" and "Other-stem" files for the readiness coverage row in audit output. The audit must confirm both categories are 100% primary_topics-covered before Phase 16.3 proceeds.

**Why it's ambiguous:**
- "MOTM" (Month of the Month) is an organizational category not present as a `category` value in `metadata_json` (top categories are: course_transcript, qa_session, cultural_commentary, etc.)
- "Other-stem" = files where `topic==filename stem` â€” but a simple SQL query returns 0 matches, suggesting the stem comparison logic is non-trivial
- It's unclear whether MOTM is identified by folder path, filename pattern, or metadata category

**Provider synthesis:**
- **OpenAI:** Use path-based grouping (files whose path contains `/MOTM/`)
- **Gemini:** Rely on `category` column if it exists, else use path pattern
- **Perplexity:** Use file path pattern or explicit folder enumeration

**Proposed implementation decision:**
Use file_path pattern matching (authoritative, reproducible, no metadata dependency):
- `MOTM` = files where `file_path LIKE '%/MOTM/%'` (or the folder name used in the library)
- `Other-stem` = the set of non-MOTM files where topic approximately matches filename stem â€” this requires replicating the scanner's categorization logic; for the readiness row, use the 440-file count identified in Phase 16.1 triage as the denominator

The audit readiness row reports:
```
Phase 16.3 readiness:
  MOTM files with primary_topics:    {n}/{total_motm}  ({pct}%)  [READY if 100%]
  Other-stem files with primary_topics: {n}/{total_other_stem}  ({pct}%)  [READY if 100%]
```

**Open questions:**
- What is the exact folder name used for MOTM in the library path? (Must be confirmed in 16.2-01 implementation.)
- For Other-stem: does the scanner store a "stem match" flag, or must we recompute it?

**Confidence:** âœ… All 3 providers agreed readiness row needs path/category-based identification

---

### âš ï¸ 7. Audit Command Read/Write Boundary (Recommended)

**What needs to be decided:**
Should `objlib metadata audit` be strictly read-only, or should it have an `--apply-fixes` mode that automatically remediates violations?

**Why it's ambiguous:**
The phase requires BOTH an audit gate and remediation of existing violations. If audit is read-only, users must run separate commands. If it has an apply mode, it risks accidentally modifying state.

**Provider synthesis:**
- **OpenAI:** Strict read-only audit; separate explicit remediation commands
- **Gemini:** Not explicitly addressed (implies read-only from context)
- **Perplexity:** Read-only audit + separate remediation commands (strongly recommended)

**Proposed implementation decision:**
Strictly read-only. Audit never modifies DB. Separate commands handle remediation:
- `objlib metadata mark-unsupported` â€” marks .pdf/.docx/.other as skipped with reason
- `objlib metadata batch-extract` â€” handles remaining pending .txt/.md files
- No `--apply-fixes` mode on the audit command itself.

**Confidence:** âš ï¸ 2 providers explicitly recommended read-only; 1 implied it

---

### âš ï¸ 8. Approved-AI vs Approved-Scanner-Only Breakdown in Audit (Recommended)

**What needs to be decided:**
How the audit command reports the breakdown between "approved by AI enrichment" and "approved by scanner filename-parsing only" â€” and whether this distinction requires schema changes.

**Why it's ambiguous:**
`ai_metadata_status='approved'` is set by `bulk_approve_metadata()` for both scanner-approved and AI-enriched files. The distinction is only visible by checking `file_primary_topics`. If schema changes are made (e.g., a new `enrichment_route` column), reporting becomes trivial; if not, audit must use the compound query.

**Provider synthesis:**
- **OpenAI:** Two-tier approved states; separate column for enrichment route
- **Gemini:** Keep single enum; compound query at audit time
- **Perplexity:** Use compound logic; add a `approved_source` column only if readability demands it

**Proposed implementation decision:**
No schema change. Audit uses compound queries to derive the breakdown:
- **AI-enriched**: `approved AND EXISTS (file_primary_topics)`
- **Scanner-only**: `approved AND NOT EXISTS (file_primary_topics)` â€” VIOLATION
- Report both counts prominently in audit output.

**Confidence:** âš ï¸ 2 providers recommended no schema change; 1 (OpenAI) proposed separate column

---

### ðŸ” 9. Gemini API Safety â€” Audit Must Not Trigger Network Calls (Single Provider)

**What needs to be decided:**
Whether the `metadata audit` command must explicitly guard against accidentally importing modules that initialize Gemini clients.

**Why it's ambiguous:**
OpenAI noted that importing batch_orchestrator or related modules for audit purposes could inadvertently initialize Gemini API clients. This is a low risk given how the current CLI is structured (Typer commands with lazy initialization), but worth confirming.

**Provider synthesis:**
- **OpenAI:** Add `OBJLIB_NO_NETWORK=1` environment variable guard; audit must never initialize Gemini client
- **Gemini:** Not raised
- **Perplexity:** Not raised

**Proposed implementation decision:**
The audit command is a pure SQLite query command â€” it only reads `data/library.db`. It does not import upload/, search/, or Gemini client modules. This is naturally satisfied by implementing audit in `extraction/` or as a standalone CLI command. No special guard needed; just ensure the command's imports stay clean.

**Confidence:** ðŸ” Single-provider concern; low implementation risk given existing architecture

---

### âœ… 10. MOTM Discriminating Subject â€” Generic primary_topics Cannot Support Phase 16.3 Header Injection (Consensus â€” APPENDED)

**What needs to be decided:**
MOTM files pass the primary_topics completeness check (468/469 have 8 topics). But ALL topics are generic Objectivism framework terms (`epistemology | ethics | metaphysics | reason | values`) that apply to nearly every file in the library. The specific session subject ("Immigration", "Libertarianism", "Masculinity and Femininity") is absent from primary_topics AND absent from `metadata_json->>'topic'` â€” confirmed NULL for all 469 MOTM files. The discriminating content exists ONLY in the filename slug.

**Live DB evidence:**
```
MOTM_2015-01-11_Immigration.txt    â†’ metadata_json->>'topic': NULL
MOTM_2015-09-11_Libertarianism.txt â†’ metadata_json->>'topic': NULL
MOTM_2024-02-18_Axioms.txt         â†’ metadata_json->>'topic': NULL
(469 MOTM files checked â€” ALL have NULL topic field)

Slug extraction (reliable MOTM_YYYY-MM-DD_Slug.txt pattern):
  filename.replace('.txt','').split('_', 2)[2].replace('-', ' ')
  â†’ "Immigration", "Libertarianism", "Axioms", "Taming Christianity Part II" etc.
```

**Why it matters for Phase 16.2:**
Phase 16.3 will inject a metadata header into content sent to Gemini File Search. If the header only has generic primary_topics, it provides zero discrimination between MOTM sessions â€” all 469 would get functionally identical headers. Phase 16.2 must populate `metadata_json->>'topic'` from the filename slug before Phase 16.3 can proceed. Phase 16.2's readiness row must gate on this field being present, not just primary_topics count.

**Provider synthesis:**
- **OpenAI:** Backfill `metadata_json.topic` from filename slug in Phase 16.2 (append-only, no AI call); treat missing topic as a readiness violation; Phase 16.3 injects `Subject:` from this field
- **Gemini:** Use `metadata_json->>'topic'` as separate `Subject:` line in header (alongside `primary_topics` as `Philosophical Frameworks:`); Phase 16.2 is responsible for ensuring the field exists
- **Perplexity:** Minimum fix = leverage `metadata_json->>'topic'`; since it's NULL, Phase 16.2 must populate via append-only JSON update from filename slug; zero metadata changes needed in Phase 16.3 itself

**Proposed implementation decision (REVISED â€” slug-backfill rejected):**
The correct fix is **structural metadata quality check + Mistral batch re-extraction**, not heuristic slug injection.

Governing principle: all enrichable non-book files receive metadata exclusively through `batch-extract` (Mistral Batch API). Writing `metadata_json->>'topic'` from a filename slug violates extraction provenance â€” it produces fields that look AI-extracted but are not, silently creating a two-tier metadata quality that downstream consumers cannot distinguish.

MOTM files with NULL `topic` and generic-only `primary_topics` represent a **Mistral extraction quality failure** â€” either the extraction ran but produced boilerplate, or the `topic` field was never populated in the first place. Either way, the fix is re-extraction:

1. Phase 16.2 implements a **structural quality check** on all approved `.txt`/`.md` files:
   - Signal A: `metadata_json->>'topic'` is NULL or empty
   - Signal B: all `file_primary_topics` entries are drawn exclusively from a "generic boilerplate" set (calibrated from corpus frequency â€” terms like `ethics`, `epistemology`, `metaphysics` that appear in >50% of files)
   - Either signal = quality failure = reset to `pending` for re-extraction
2. All quality-failed files (expected: all 469 MOTM + possibly others) are reset to `ai_metadata_status='pending'` and run through `batch-extract` â€” identical to how any unextracted file is handled. Mistral extracts the actual session topic from transcript content.
3. The Phase 16.2 audit readiness row gates on `metadata_json->>'topic'` being populated **from Mistral extraction**, not from a heuristic. The Phase 16.3 header injection uses whatever Mistral produces.

```
Phase 16.3 readiness:
  MOTM files with topic (Mistral-extracted) populated: {n}/469  [REQUIRED: 100%]
  MOTM files with primary_topics:                      {n}/469  [REQUIRED: 100%]
  Other-stem files with primary_topics:                {n}/440  [REQUIRED: 100%]
```

**Open questions:** How to define the "generic boilerplate" threshold for primary_topics quality check â€” calibration required against actual corpus frequency data during 16.2-01.

**Confidence:** âœ… All 3 providers confirmed topic=NULL; extraction-provenance principle is explicit in MEMORY.md; re-extraction path is the established pattern for all files

---

## Summary: Decision Checklist

**Tier 1 (Blocking â€” must decide before planning):**
- [x] "Primary_topics populated" = EXISTS in `file_primary_topics` table (not metadata_json alone)
- [x] Audit scope = all 1885 DB-tracked files (not just 1749 txt)
- [x] Skip reason stored in `error_message` column (no schema change)
- [x] 110 pending non-enrichable files need explicit `skipped` status (backfill command)
- [x] 21 approved .txt books need primary_topics backfill from metadata_json (or re-extraction)
- [x] Phase 16.3 readiness row: MOTM via file_path pattern; Other-stem via Phase 16.1 categorization
- [x] MOTM + quality-check: structural quality gate (NULL topic OR generic-only primary_topics) triggers Mistral re-extraction via `batch-extract` â€” slug injection rejected; Mistral is the sole authorized metadata source

**Tier 2 (Important):**
- [x] Audit is strictly read-only; separate `mark-unsupported` command handles remediation
- [x] Approved breakdown (AI-enriched vs scanner-only) via compound query, no schema change

**Tier 3 (Polish):**
- [x] Audit must not initialize Gemini clients (natural consequence of clean imports)

---

## Codebase Reference

Key files for Phase 16.2:
- `src/objlib/extraction/batch_orchestrator.py:355` â€” `_get_pending_files()` â€” LIKE '%.txt' filter to fix
- `src/objlib/extraction/batch_orchestrator.py:464` â€” `_mark_failed()` â€” uses `error_message` column
- `src/objlib/extraction/batch_orchestrator.py:490` â€” `_mark_skipped()` â€” uses `error_message` column
- `src/objlib/database.py:1416` â€” `set_ai_metadata_status()` â€” direct status setter
- `src/objlib/database.py:1336` â€” `get_ai_metadata_status_counts()` â€” distribution query
- `src/objlib/cli.py` â€” add new `metadata audit` subcommand here
- `data/library.db` â€” 1885 files, file_primary_topics junction table

---

*Multi-provider synthesis by: OpenAI gpt-5.2, Gemini Pro, Perplexity Sonar Deep Research*
*Generated: 2026-02-24*
