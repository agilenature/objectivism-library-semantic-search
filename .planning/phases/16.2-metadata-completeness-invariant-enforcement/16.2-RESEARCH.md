# Phase 16.2: Metadata Completeness Invariant Enforcement - Research

**Researched:** 2026-02-24
**Domain:** SQLite metadata invariant enforcement, Typer CLI subcommand patterns, Mistral Batch API re-extraction
**Confidence:** HIGH (all findings verified against live DB and codebase)

## Summary

Phase 16.2 enforces a structural invariant: every file in the library either has `primary_topics` rows in the `file_primary_topics` junction table (AI-enriched) or has `ai_metadata_status='skipped'` with a named reason. The live DB audit confirms 21 approved `.txt` book files with AI metadata in JSON but no `file_primary_topics` rows (backfillable from existing JSON), 110 non-txt files in `pending` status that need explicit skip/enrich classification, 26 `.epub` files already `skipped` but with NULL `error_message`, and 1 `.md` file excluded from batch-extract by the `LIKE '%.txt'` filter.

All required changes are surgical: one SQL filter fix (1 line), one new CLI command (`metadata mark-unsupported`), one new CLI command (`metadata audit`), one backfill script for the 21 book files, and a structural metadata quality check to identify files needing re-extraction for Phase 16.3 readiness.

**Primary recommendation:** Implement the four invariant conditions as a single `metadata audit` command with Rich text output and exit 0/1. The backfill of `file_primary_topics` from existing JSON is a safe read-then-insert operation requiring no API calls.

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

**Q1:** "Primary_topics populated" = file_primary_topics junction table (NOT metadata_json)
- Audit SQL: `EXISTS (SELECT 1 FROM file_primary_topics WHERE file_path = f.file_path)`
- Catches the 21 approved .txt books that have file_metadata_ai but no topic rows
- Current violations: 21 files (all approved .txt book exports)

**Q2:** Audit scope = all 1885 DB-tracked files
- Every file must satisfy: AI-enriched OR explicitly skipped with reason OR pending (enrichable only)
- Four invariant conditions for exit 0:
  1. Zero approved files without file_primary_topics entries
  2. Zero skipped files with error_message IS NULL OR ''
  3. Zero pending files with extensions NOT IN (.txt, .md) -- all non-enrichable must be classified
  4. Phase 16.3 readiness: MOTM and Other-stem at 100% primary_topics coverage

**Q3:** Remediation for 21 approved .txt books without primary_topics
- Step 1: Check metadata_json->>'primary_topics' per file
- Step 2: If non-empty array in JSON -> INSERT rows into file_primary_topics (no API call)
- Step 3: If JSON has no primary_topics -> reset to pending, include in next batch-extract
- Sacred metadata rule: we're reading existing data, not replacing/deleting

**Q4:** MOTM/Other-stem identification = file_path LIKE '%/MOTM/%'
- MOTM: file_path LIKE '%/MOTM/%' (confirm exact folder name during implementation)
- Other-stem: use Phase 16.1 triage 440-file count as denominator

**Q5:** Single backfill pass for all non-enrichable formats
- One mark-unsupported command: pdf + docx + other -> skipped with named reason
- Also updates 26 epub skip reasons (error_message was NULL)
- Bernstein .md is the ONLY non-.txt file that gets batch-extracted, not skipped

**Q6:** Audit output = Rich text only (no JSON mode for Phase 16.2)

**Q7:** MOTM discriminating subject = structural quality check + Mistral re-extraction (NOT slug backfill)
GOVERNING PRINCIPLE: All enrichable non-book files receive metadata exclusively through batch-extract (Mistral Batch API). Slug injection violates extraction-provenance invariant.

DECISION:
1. Structural quality check (deterministic, no API calls):
   - Signal A: metadata_json->>'topic' is NULL or empty AND file is .txt or .md (enrichable)
   - Signal B: all file_primary_topics entries exclusively from generic boilerplate set (top-N corpus frequency -- calibrate against corpus data)
   - Either signal = quality failure
2. Reset quality-failed files to ai_metadata_status='pending'
3. Run batch-extract -- Mistral extracts real topic from transcript content
4. Phase 16.3 readiness gates on metadata_json->>'topic' populated from Mistral (not heuristics)
5. Books follow established skip path (epub/pdf/docx -> skipped + named reason)

Expected: all 469 MOTM files + possibly other quality-failed files will be reset to pending

### Implementation Sequence (locked):
16.2-01:
1. Confirm MOTM folder name via SELECT DISTINCT on file_path patterns
2. Implement objlib metadata mark-unsupported command
3. Fix _get_pending_files() in batch_orchestrator.py to include LIKE '%.md'
4. Define + implement structural metadata quality check
5. Reset quality-failed files to pending (expected: all 469 MOTM + others)
6. Run batch-extract on all newly-pending enrichable files (Bernstein .md + quality-failed .txt)
7. Implement objlib metadata audit command

16.2-02:
1. Run metadata audit, verify exit 0
2. Confirm Bernstein .md has primary_topics + topic field (from Mistral)
3. Confirm no silent-pending files remain
4. Confirm Phase 16.3 readiness row: MOTM topic 100%, MOTM primary_topics 100%, Other-stem primary_topics 100%

### Claude's Discretion
None listed -- all implementation decisions are locked.

### Deferred Ideas (OUT OF SCOPE)
None listed.
</user_constraints>

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| Typer | (existing) | CLI framework for new subcommands | Already used; `metadata_app` Typer group at cli.py:122 |
| Rich | (existing) | Console output, Tables, Panels | Already used throughout CLI |
| SQLite | 3.x (stdlib) | All queries, updates, backfill | Direct `conn.execute()` pattern per codebase convention |
| Mistral Batch API | via `MistralBatchClient` | Re-extraction of quality-failed files | Locked decision: sole authorized extraction source |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| json (stdlib) | -- | Parse metadata_json for backfill | Reading primary_topics from file_metadata_ai |
| pathlib (stdlib) | -- | File extension checks | Extension-based routing in mark-unsupported |

### Alternatives Considered
None -- all decisions locked. No new libraries needed.

**Installation:**
No new dependencies. All tools already in the project.

## Architecture Patterns

### Existing CLI Command Pattern
All `metadata` subcommands follow the same pattern (verified from `batch-extract`, `approve`, `stats`):

```python
@metadata_app.command("command-name")
def command_name(
    db_path: Annotated[
        Path,
        typer.Option("--db", "-d", help="Path to SQLite database"),
    ] = Path("data/library.db"),
) -> None:
    """Docstring with examples."""
    if not db_path.exists():
        console.print(f"[red]Error:[/red] Database not found: {db_path}")
        raise typer.Exit(code=1)

    with Database(db_path) as db:
        # ... work with db.conn.execute() ...
        pass
```

**Key pattern: `raise typer.Exit(code=N)` for exit codes.** The audit command must exit 0 (all invariants pass) or exit 1 (violations found).

### Database Query Pattern
The codebase uses raw SQL with `db.conn.execute()` directly, NOT ORM methods. The `Database` class wraps SQLite connection management but most queries are inline SQL:

```python
# Source: batch_orchestrator.py:357-369
cursor = self._db.conn.execute(query)
rows = cursor.fetchall()
```

### Existing Status Update Pattern
```python
# Source: database.py:1416-1427
def set_ai_metadata_status(self, file_path: str, status: str) -> None:
    with self.conn:
        self.conn.execute(
            "UPDATE files SET ai_metadata_status = ? WHERE file_path = ?",
            (status, file_path),
        )
```

### Existing Primary Topics Insert Pattern
```python
# Source: batch_orchestrator.py:447-461
valid_topics = metadata.get("primary_topics", [])
if valid_topics:
    self._db.conn.execute(
        "DELETE FROM file_primary_topics WHERE file_path = ?",
        (file_path,),
    )
    for topic in valid_topics:
        self._db.conn.execute(
            "INSERT INTO file_primary_topics (file_path, topic_tag) "
            "VALUES (?, ?)",
            (file_path, topic),
        )
```

### Anti-Patterns to Avoid
- **Do NOT add new Python dependencies.** Everything needed is already available.
- **Do NOT modify the ExtractedMetadata Pydantic schema** to add a `topic` field. The `topic` field in `files.metadata_json` comes from the scanner (`metadata.py`), not from AI extraction. These are separate data sources.
- **Do NOT use `db.set_ai_metadata_status()` for bulk updates** -- use raw SQL with `WHERE` clauses for efficiency (e.g., bulk mark-unsupported).
- **Do NOT reset files with `--reset-existing`** -- use `ai_metadata_status='pending'` reset only.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| CLI subcommand structure | Custom argument parsing | `@metadata_app.command()` with Typer | Consistent with 11 existing metadata subcommands |
| Rich table output | Print statements | `rich.table.Table` + `console.print()` | Consistent with existing stats/review commands |
| Exit code signaling | `sys.exit()` | `raise typer.Exit(code=N)` | Typer convention used throughout codebase |
| Primary topics validation | Custom vocabulary check | `CONTROLLED_VOCABULARY` frozenset from schemas.py | Already used in batch_orchestrator.py:737 |
| Bulk status update | Loop with individual updates | Single SQL UPDATE with WHERE clause | Already pattern in approve_files_by_confidence() |

**Key insight:** Every component needed for Phase 16.2 already exists in the codebase. No novel patterns required -- only composition of existing patterns into two new CLI commands and one SQL fix.

## Common Pitfalls

### Pitfall 1: The Two metadata_json Columns
**What goes wrong:** Confusing `files.metadata_json` (scanner metadata with `topic`, `course`, `series` etc.) with `file_metadata_ai.metadata_json` (AI extraction with `category`, `primary_topics`, `semantic_description` etc.)
**Why it happens:** Both columns have the same name in different tables.
**How to avoid:** Always qualify: `f.metadata_json` for scanner metadata, `m.metadata_json` for AI metadata. The `topic` field exists ONLY in scanner metadata. The `primary_topics` array exists ONLY in AI metadata.
**Warning signs:** Query returns NULL when you expected data -- you're probably querying the wrong table.

### Pitfall 2: CONTROLLED_VOCABULARY Filtering During Backfill
**What goes wrong:** Inserting primary_topics from AI metadata JSON without validating against CONTROLLED_VOCABULARY, introducing invalid tags into file_primary_topics.
**Why it happens:** The AI metadata JSON may contain topics that were valid at extraction time but aren't in the current vocabulary.
**How to avoid:** Filter through `CONTROLLED_VOCABULARY` before INSERT, exactly as batch_orchestrator.py:737 does: `valid_topics = [t for t in primary_topics if t in CONTROLLED_VOCABULARY]`.
**Warning signs:** Foreign key won't fail (no FK to a vocabulary table), but downstream queries expecting vocabulary membership will produce wrong counts.

### Pitfall 3: Quality Check Signal A Scope Ambiguity
**What goes wrong:** The CONTEXT.md Q7 decision specifies `metadata_json->>'topic'` but doesn't clarify which `metadata_json` column. The scanner `topic` is populated for ALL MOTM files (468/468). The AI `metadata_json` has NO `topic` field at all.
**Why it happens:** Two different metadata systems use the same column name.
**How to avoid:** See Open Questions section. The planner must resolve this ambiguity before implementing the quality check.
**Warning signs:** If Signal A references `files.metadata_json`, it catches 0 MOTM files. If it references `file_metadata_ai.metadata_json`, it catches ALL files (since AI extraction never produces a `topic` field).

### Pitfall 4: MOTM Folder Name Assumption
**What goes wrong:** Hardcoding `LIKE '%/MOTM/%'` when the actual folder might use different casing or structure.
**Why it happens:** The CONTEXT.md says "confirm exact folder name during implementation."
**How to avoid:** The folder IS `/MOTM/` based on live DB query (469 files match `LIKE '%/MOTM/%'`). But note: 1 of those 469 is a PDF file (`Replacing LLM Components with Claude SDK and Embracing Context Graphs.pdf`) with status=pending, not a MOTM transcript.
**Warning signs:** Count mismatch between expected 468 MOTM transcripts and the 469 MOTM folder files.

### Pitfall 5: Backfill Violating Sacred Metadata Rule
**What goes wrong:** Accidentally overwriting or deleting existing AI metadata during the backfill.
**Why it happens:** Copy-pasting the batch_orchestrator pattern which does `DELETE FROM file_primary_topics` before INSERT.
**How to avoid:** For the 21 book backfill, these files have ZERO existing `file_primary_topics` rows, so INSERT-only (no DELETE) is sufficient. Verify with `NOT EXISTS` check before insert.
**Warning signs:** If `DELETE` is used and any of the 21 files somehow already have partial topics, those would be destroyed.

### Pitfall 6: mark-unsupported Affecting Already-Skipped epub Files
**What goes wrong:** The UPDATE for epub files changes `ai_metadata_status` when it should ONLY update `error_message`.
**Why it happens:** Using a single UPDATE that sets both status and error_message for all file types.
**How to avoid:** For epub files (already skipped), update ONLY `error_message`. For pdf/docx/html, update both `ai_metadata_status` and `error_message`.
**Warning signs:** epub files losing their `skipped` status or getting a different status.

## Code Examples

### Example 1: Invariant Check SQL (Condition 1 -- approved without topics)
```sql
-- Source: verified against live DB (returns 21 rows)
SELECT f.file_path, f.filename
FROM files f
WHERE f.ai_metadata_status = 'approved'
  AND NOT EXISTS (
    SELECT 1 FROM file_primary_topics pt
    WHERE pt.file_path = f.file_path
  )
```

### Example 2: Invariant Check SQL (Condition 2 -- skipped without reason)
```sql
-- Source: verified against live DB (returns 26 rows -- all epub)
SELECT f.file_path, f.filename, f.ai_metadata_status
FROM files f
WHERE f.ai_metadata_status = 'skipped'
  AND (f.error_message IS NULL OR f.error_message = '')
```

### Example 3: Invariant Check SQL (Condition 3 -- pending non-enrichable)
```sql
-- Source: verified against live DB (returns 110 files: 58 pdf, 42 html, 9 docx, 1 pdf-with-msg)
SELECT f.file_path, f.filename
FROM files f
WHERE f.ai_metadata_status = 'pending'
  AND f.file_path NOT LIKE '%.txt'
  AND f.file_path NOT LIKE '%.md'
```

### Example 4: Phase 16.3 Readiness Breakdown SQL
```sql
-- Source: verified against live DB
SELECT
  CASE
    WHEN f.file_path LIKE '%Episode %' THEN 'Episode'
    WHEN f.file_path LIKE '%/MOTM/%' THEN 'MOTM'
    WHEN json_extract(f.metadata_json, '$._unparsed_filename') = 1 THEN 'Other-stem'
    ELSE 'Other-discriminating'
  END as category,
  COUNT(*) as total,
  SUM(CASE WHEN EXISTS (
    SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = f.file_path
  ) THEN 1 ELSE 0 END) as has_topics,
  SUM(CASE WHEN NOT EXISTS (
    SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = f.file_path
  ) THEN 1 ELSE 0 END) as no_topics
FROM files f
WHERE f.ai_metadata_status = 'approved' AND f.file_path LIKE '%.txt'
GROUP BY category
```

Live output (2026-02-24):
| Category | Total | Has Topics | No Topics |
|----------|-------|-----------|-----------|
| Episode | 333 | 333 | 0 |
| MOTM | 468 | 468 | 0 |
| Other-discriminating | 503 | 502 | 1 |
| Other-stem | 444 | 424 | 20 |

### Example 5: Backfill primary_topics from AI JSON
```python
# Pattern: read AI metadata JSON, insert into file_primary_topics
# Source: adapted from batch_orchestrator.py:447-461
import json
from objlib.extraction.schemas import CONTROLLED_VOCABULARY

# For each approved file without file_primary_topics
rows = db.conn.execute("""
    SELECT f.file_path, m.metadata_json
    FROM files f
    JOIN file_metadata_ai m ON f.file_path = m.file_path AND m.is_current = 1
    WHERE f.ai_metadata_status = 'approved'
      AND NOT EXISTS (
        SELECT 1 FROM file_primary_topics pt WHERE pt.file_path = f.file_path
      )
""").fetchall()

backfilled = 0
reset_to_pending = 0
for row in rows:
    file_path = row["file_path"]
    meta = json.loads(row["metadata_json"]) if row["metadata_json"] else {}
    topics = meta.get("primary_topics", [])
    valid_topics = [t for t in topics if t in CONTROLLED_VOCABULARY]

    if valid_topics:
        for topic in valid_topics:
            db.conn.execute(
                "INSERT OR IGNORE INTO file_primary_topics (file_path, topic_tag) VALUES (?, ?)",
                (file_path, topic),
            )
        backfilled += 1
    else:
        # No valid topics in JSON -- reset to pending for re-extraction
        db.conn.execute(
            "UPDATE files SET ai_metadata_status = 'pending' WHERE file_path = ?",
            (file_path,),
        )
        reset_to_pending += 1

db.conn.commit()
```

### Example 6: mark-unsupported Bulk Update
```python
# Bulk mark non-enrichable formats as skipped with named reasons
extension_reasons = {
    '%.pdf': 'PDF format - text extraction not supported; use .txt export',
    '%.epub': 'EPUB format - text extraction not supported; .txt export exists for book content',
    '%.docx': 'DOCX format - text extraction not supported; use .txt export',
    '%.html': 'HTML format - structural content (course navigation/menus), not transcript text',
}

for ext_pattern, reason in extension_reasons.items():
    db.conn.execute("""
        UPDATE files
        SET ai_metadata_status = 'skipped',
            error_message = ?,
            updated_at = strftime('%Y-%m-%dT%H:%M:%f', 'now')
        WHERE file_path LIKE ?
          AND ai_metadata_status IN ('pending', 'skipped')
    """, (reason, ext_pattern))

db.conn.commit()
```

### Example 7: _get_pending_files Fix
```python
# Current (batch_orchestrator.py:357-362):
query = """
    SELECT file_path, metadata_json
    FROM files
    WHERE ai_metadata_status = 'pending'
      AND file_path LIKE '%.txt'
    ORDER BY file_path
"""

# Fixed (add .md support):
query = """
    SELECT file_path, metadata_json
    FROM files
    WHERE ai_metadata_status = 'pending'
      AND (file_path LIKE '%.txt' OR file_path LIKE '%.md')
    ORDER BY file_path
"""
```

### Example 8: Audit Exit Code Pattern
```python
@metadata_app.command("audit")
def metadata_audit(
    db_path: Annotated[
        Path,
        typer.Option("--db", "-d", help="Path to SQLite database"),
    ] = Path("data/library.db"),
) -> None:
    """Check metadata completeness invariant. Exits 0 if all conditions pass."""
    # ... run checks ...
    violations = 0
    # ... accumulate violations ...

    if violations > 0:
        console.print(f"\n[red bold]FAIL[/red bold]: {violations} invariant violation(s)")
        raise typer.Exit(code=1)
    else:
        console.print(f"\n[green bold]PASS[/green bold]: All invariants satisfied")
        raise typer.Exit(code=0)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Scanner topic only | Scanner topic + AI 4-tier metadata | Phase 6 | Files have both scanner metadata and AI metadata |
| `LIKE '%.txt'` filter | Should be `LIKE '%.txt' OR LIKE '%.md'` | Phase 16.2 (pending) | Bernstein .md book blocked from extraction |
| No completeness invariant | `metadata audit` command | Phase 16.2 (pending) | Structural guarantee for Phase 16.3 |

## Live Database State (Verified 2026-02-24)

These counts are from direct DB queries, not assumptions:

| Metric | Count | Source |
|--------|-------|--------|
| Total files in DB | 1885 | `SELECT COUNT(*) FROM files` |
| `.txt` files | 1748 | `LIKE '%.txt'` |
| `.pdf` files | 59 | `LIKE '%.pdf'` (58 pending + 1 with error_message) |
| `.epub` files | 26 | `LIKE '%.epub'` (all skipped, error_message=NULL) |
| `.html` files | 42 | `LIKE '%.html'` (all pending) |
| `.docx` files | 9 | `LIKE '%.docx'` (all pending) |
| `.md` files | 1 | Bernstein Heroes book (pending) |
| Approved without topics | 21 | All `.txt` books with AI metadata JSON containing primary_topics |
| MOTM folder files | 469 | 468 approved `.txt` + 1 pending `.pdf` |
| MOTM approved `.txt` | 468 | All have primary_topics rows |
| Other-stem (approved `.txt`) | 444 | 424 have topics, 20 missing (all books) |
| Other-discriminating (approved `.txt`) | 503 | 502 have topics, 1 missing (book) |
| Episode (approved `.txt`) | 333 | All have primary_topics rows |

### Invariant Violations (Pre-Fix)
1. **Condition 1:** 21 approved files without `file_primary_topics` rows
2. **Condition 2:** 26 skipped files with NULL `error_message`
3. **Condition 3:** 110 pending files with non-enrichable extensions (59 pdf, 42 html, 9 docx)
4. **Condition 4 (Phase 16.3):** Other-stem missing 20 files, Other-discriminating missing 1 file

### Post-Fix Expected State
After backfill from JSON: 21 books gain `file_primary_topics` rows (0 violations for Condition 1).
After mark-unsupported: 110 non-enrichable marked skipped with reasons, 26 epub error_messages filled.
After _get_pending_files fix + batch-extract: Bernstein `.md` gains AI metadata.
Phase 16.3 readiness: all categories at 100% primary_topics coverage.

## Open Questions

### 1. Signal A Scope for Quality Check (CRITICAL)

**What we know:** The CONTEXT.md Q7 decision defines Signal A as `metadata_json->>'topic'` is NULL or empty. But there are two `metadata_json` columns:
- `files.metadata_json` (scanner) -- ALL MOTM files have `topic` populated (468/468)
- `file_metadata_ai.metadata_json` (AI) -- NO files have a `topic` field (not in ExtractedMetadata schema)

**What's unclear:** Which `metadata_json` does Q7 reference? If scanner, Signal A catches 0 MOTM files. If AI, Signal A catches ALL files (including non-MOTM).

**The "Expected: all 469 MOTM" claim:** This suggests the intent is to catch all MOTM files. But neither interpretation of Signal A achieves this:
- Scanner topic: catches 0 MOTM (all have topics)
- AI topic: catches all 1748 approved files (none have AI `topic`)

**Evidence of intended behavior:** Q7 says "Phase 16.3 readiness gates on `metadata_json->>'topic'` populated from **Mistral**." Currently Mistral extraction does NOT produce a `topic` field. The ExtractedMetadata schema would need modification to include one. But Q7 also says "Slug injection violates extraction-provenance invariant" and emphasizes Mistral as the sole source.

**Recommendation:** The planner should interpret Q7 as follows: the quality check should use a **different, simpler criterion** for MOTM files. Since the CONTEXT explicitly says "Expected: all 469 MOTM files will be reset to pending," the simplest implementation is:
- For MOTM files: ALL MOTM `.txt` files in `approved` status are quality failures (reset to pending for re-extraction)
- For non-MOTM files: Signal A (scanner topic NULL/empty) + Signal B (all-boilerplate primary_topics)
- This matches the stated expectation and avoids the ambiguous `metadata_json->>'topic'` reference

Alternatively, the ExtractedMetadata schema may need a `topic` field added, and the Mistral prompt updated to extract one. This would be a larger change. The planner should confirm with the user if this interpretation is correct.

### 2. Mistral Re-extraction Scope for MOTM

**What we know:** All 468 MOTM `.txt` files already have approved AI metadata with primary_topics, topic_aspects, and semantic_description. Resetting them all to pending and re-running batch-extract will re-extract all of them.

**What's unclear:** The batch_orchestrator `_save_extracted_metadata` method does `DELETE FROM file_primary_topics WHERE file_path = ?` before re-inserting. This is safe for existing topics but will replace them entirely. The sacred metadata rule says "no operation may delete/reset metadata_json or entity tables." The `file_primary_topics` table is a junction table derived from AI metadata, and replacing it with fresh extraction output may be considered acceptable (it's regeneration, not deletion).

**Recommendation:** This is likely acceptable since batch-extract is the authorized extraction source and the result is a merge (new extraction replaces old). But the planner should note the risk: if Mistral re-extraction for some files produces fewer/different valid topics than the original extraction, those files will have different coverage. The `_save_extracted_metadata` pattern already handles this by deleting and re-inserting.

### 3. topic Field in Mistral Extraction

**What we know:** The current `ExtractedMetadata` Pydantic schema has NO `topic` field. It produces: `category`, `difficulty`, `primary_topics`, `topic_aspects`, `semantic_description`, `confidence_score`. The `semantic_description.summary` field is a rich 1-2 sentence overview that could serve as a discriminating topic.

**What's unclear:** Q7 says "Phase 16.3 readiness gates on `metadata_json->>'topic'` populated from Mistral." If this means the AI `metadata_json` needs a `topic` field, the extraction schema and prompt must be updated. But updating the Pydantic schema (`ExtractedMetadata`) is a significant change that affects validation, batch_orchestrator, and the entire extraction pipeline.

**Recommendation:** The planner should either: (a) interpret `topic` as `semantic_description.summary` (already extracted, no schema change), or (b) add `topic` as a new optional field to `ExtractedMetadata` and update the Mistral prompt. Option (a) is safer and avoids schema changes. The planner should clarify with the user.

## Sources

### Primary (HIGH confidence)
- Live database queries against `data/library.db` (all counts verified 2026-02-24)
- `src/objlib/extraction/batch_orchestrator.py` -- `_get_pending_files()` at line 355, `_save_extracted_metadata()` at line 406
- `src/objlib/extraction/schemas.py` -- `CONTROLLED_VOCABULARY` at line 54, `ExtractedMetadata` at line 121
- `src/objlib/extraction/validator.py` -- `_filter_primary_topics()` at line 117
- `src/objlib/cli.py` -- `metadata_app` at line 122, existing command patterns
- `src/objlib/database.py` -- schema at line 17, `set_ai_metadata_status()` at line 1416
- `src/objlib/metadata.py` -- scanner `topic` extraction patterns
- `src/objlib/upload/content_preparer.py` -- enriched content header format
- `src/objlib/upload/metadata_builder.py` -- enriched metadata builder (no `topic` field used)
- `.planning/phases/16.1-stability-instrument-correctness-audit/16.1-T0-BASELINE.md` -- corpus breakdown

### Secondary (MEDIUM confidence)
- `.planning/phases/16.2-metadata-completeness-invariant-enforcement/16.2-CONTEXT.md` -- locked decisions
- `.planning/STATE.md` -- accumulated context and standing constraints

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- all libraries already in use, no new dependencies
- Architecture: HIGH -- all patterns verified from existing codebase
- Database state: HIGH -- all counts from live DB queries
- Quality check signals: MEDIUM -- Signal A scope is ambiguous (see Open Question 1)
- Re-extraction behavior: MEDIUM -- sacred metadata rule interpretation for file_primary_topics replacement

**Research date:** 2026-02-24
**Valid until:** 2026-03-10 (stable -- no external API changes expected, DB state may shift if batch-extract runs)
