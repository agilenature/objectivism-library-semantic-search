---
phase: 06-ai-powered-metadata
plan: 03
type: execute
wave: 3
depends_on: ["06-02"]
files_modified:
  - src/objlib/extraction/report.py
  - src/objlib/extraction/quality_gates.py
  - src/objlib/cli.py
autonomous: false

must_haves:
  truths:
    - "User can run Wave 1 discovery on 20 test files and see extraction results from all 3 competing strategies"
    - "User can resume Wave 1 from a checkpoint after credit exhaustion without losing completed work"
    - "User can compare strategies side-by-side with per-strategy metrics (avg tokens, avg latency, validation pass rate, confidence)"
    - "User receives a clear Wave 2 go/no-go decision based on quality gate evaluation"
    - "User can select a winning strategy (or declare hybrid needed) and proceed to production processing"
  artifacts:
    - path: "src/objlib/extraction/report.py"
      provides: "Wave 1 comparison report generator (Rich terminal + CSV export)"
      contains: "def generate_wave1_report"
    - path: "src/objlib/extraction/quality_gates.py"
      provides: "Quality gate evaluation for Wave 2 transition"
      contains: "def evaluate_quality_gates"
    - path: "src/objlib/cli.py"
      provides: "Wave 1 CLI commands (extract-wave1, wave1-report, wave1-select)"
      contains: "extract_wave1"
  key_links:
    - from: "src/objlib/cli.py"
      to: "src/objlib/extraction/orchestrator.py"
      via: "extract_wave1 command calls orchestrator.run_wave1()"
      pattern: "orchestrator\\.run_wave1"
    - from: "src/objlib/extraction/report.py"
      to: "src/objlib/database.py"
      via: "Reads wave1_results table for comparison data"
      pattern: "wave1_results"
---

<objective>
Add CLI commands to execute Wave 1 discovery (run 3 strategies on 20 test files), generate comparison reports, and evaluate quality gates for Wave 2 transition.

Purpose: This is the execution and review step for Wave 1. After running, the user reviews results and selects the winning strategy (or hybrid) before proceeding to Wave 2 production processing.
Output: Working Wave 1 CLI workflow with comparison reports and quality gate evaluation. Includes human-verify checkpoint for strategy selection.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-ai-powered-metadata/06-CONTEXT.md
@.planning/phases/06-ai-powered-metadata/CLARIFICATIONS-ANSWERED.md
@.planning/phases/06-ai-powered-metadata/06-01-SUMMARY.md
@.planning/phases/06-ai-powered-metadata/06-02-SUMMARY.md

@src/objlib/extraction/orchestrator.py
@src/objlib/extraction/strategies.py
@src/objlib/extraction/sampler.py
@src/objlib/extraction/checkpoint.py
@src/objlib/cli.py
@src/objlib/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wave 1 report generator and quality gates</name>
  <files>
    src/objlib/extraction/report.py
    src/objlib/extraction/quality_gates.py
  </files>
  <action>
**1. Create `src/objlib/extraction/report.py`** with:

- `def generate_wave1_report(db: Database) -> dict`:
  - Query `wave1_results` table grouped by strategy
  - For each strategy compute:
    - `total_files`: count of results
    - `avg_tokens`: mean token_count
    - `avg_latency_ms`: mean latency_ms
    - `avg_confidence`: mean confidence_score
    - `validation_pass_rate`: percentage where metadata_json parses and validates against ExtractedMetadata
    - `failed_count`: count of NULL or empty metadata_json entries
  - Return structured dict: {strategy_name: {metrics_dict}}

- `def display_wave1_report(report: dict, console: Console) -> None`:
  - Display Rich Table with columns: Strategy, Files, Avg Tokens, Avg Latency, Avg Confidence, Validation %, Failed
  - Color-code: best metric per column in green, worst in red
  - Below table, show recommendation: strategy with highest validation_pass_rate * avg_confidence (product)

- `def display_file_comparison(db: Database, file_path: str, console: Console) -> None`:
  - For a single test file, show side-by-side 3-strategy results
  - Display as 3 Rich Panels (one per strategy) showing: category, difficulty, primary_topics, topic_aspects (first 3), confidence

- `def export_wave1_csv(db: Database, output_path: Path) -> None`:
  - Export all wave1_results to CSV: file_path, strategy, category, difficulty, primary_topics, topic_aspects_count, confidence_score, tokens, latency_ms
  - Parse metadata_json to extract fields
  - Print path to CSV file after export

**2. Create `src/objlib/extraction/quality_gates.py`** with:

- `@dataclass class GateResult`: name (str), threshold (float), actual (float), passed (bool)

- `def evaluate_quality_gates(report: dict) -> tuple[bool, list[GateResult]]`:
  - Find best strategy (highest validation_pass_rate * avg_confidence)
  - Evaluate gates per W1.A7 decision:
    - `tier1_accuracy`: validation_pass_rate >= 0.90 (category and difficulty always valid if parsed)
    - `cost_per_file`: (avg_tokens / 1000 * 0.007) <= 0.30 (magistral pricing: ~$0.007 per 1K tokens combined)
    - `mean_confidence`: avg_confidence >= 0.70
    - `validation_rate`: validation_pass_rate >= 0.85
  - Return (all_passed: bool, gate_results: list[GateResult])

- `def display_gate_results(gates: list[GateResult], console: Console) -> None`:
  - Display Rich Table with columns: Gate, Threshold, Actual, Status
  - Status: green checkmark if passed, red X if failed
  - If all passed: green panel "WAVE 2 APPROVED - Ready for production processing"
  - If any failed: yellow panel "WAVE 1.5 NEEDED - Focused re-discovery required" listing failed gates

- `def recommend_strategy(report: dict) -> str`:
  - Returns the name of the recommended strategy (or "hybrid" if split performance detected)
  - Split performance detected if: one strategy wins on validation but another wins on confidence by >10%
  </action>
  <verify>
```bash
# Verify report generator
python -c "
from objlib.extraction.report import generate_wave1_report, display_wave1_report, export_wave1_csv
print('Report imports OK')
"

# Verify quality gates
python -c "
from objlib.extraction.quality_gates import evaluate_quality_gates, GateResult, recommend_strategy
# Test with mock report
mock_report = {
    'minimalist': {'total_files': 20, 'avg_tokens': 2000, 'avg_latency_ms': 5000, 'avg_confidence': 0.75, 'validation_pass_rate': 0.90, 'failed_count': 2},
    'teacher': {'total_files': 20, 'avg_tokens': 2500, 'avg_latency_ms': 6000, 'avg_confidence': 0.82, 'validation_pass_rate': 0.95, 'failed_count': 1},
    'reasoner': {'total_files': 20, 'avg_tokens': 3500, 'avg_latency_ms': 8000, 'avg_confidence': 0.88, 'validation_pass_rate': 0.85, 'failed_count': 3},
}
all_passed, gates = evaluate_quality_gates(mock_report)
print(f'All gates passed: {all_passed}')
for g in gates:
    print(f'  {g.name}: {g.actual:.2f} vs {g.threshold:.2f} -> {\"PASS\" if g.passed else \"FAIL\"}')
rec = recommend_strategy(mock_report)
print(f'Recommended: {rec}')
print('Quality gates OK')
"
```
  </verify>
  <done>
Wave 1 report generator computes per-strategy metrics from wave1_results table. Quality gates evaluate Wave 2 readiness with clear pass/fail display. Strategy recommendation handles both clear winners and split-performance hybrid cases.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wave 1 CLI commands for execution and review</name>
  <files>src/objlib/cli.py</files>
  <action>
Add Wave 1 commands to the existing `metadata_app` Typer group in `src/objlib/cli.py`:

**1. `@metadata_app.command("extract-wave1")`:**
- Options: `--resume` (flag, resume from checkpoint), `--db` (Path), `--debug-store-raw` (flag, store raw API responses)
- Implementation:
  1. Load Mistral API key via `get_mistral_api_key()`
  2. If `--resume` and checkpoint exists: load checkpoint, display resume info
  3. Else: call `select_test_files(db, n=20)` to get stratified test set, display selection summary
  4. Create `MistralClient(api_key)`
  5. Create `CheckpointManager()`, `ExtractionConfig()`, `ExtractionOrchestrator(client, db, checkpoint, config)`
  6. Run `asyncio.run(orchestrator.run_wave1(test_files, WAVE1_STRATEGIES))`
  7. Display summary: files processed per strategy, total tokens, total cost estimate, time elapsed
  8. Print next step: "Run 'objlib metadata wave1-report' to compare results"

**2. `@metadata_app.command("wave1-report")`:**
- Options: `--db` (Path), `--export-csv` (Path, optional), `--file` (str, optional -- show single file comparison)
- Implementation:
  1. Open database
  2. If `--file`: call `display_file_comparison(db, file, console)`
  3. Else: call `generate_wave1_report(db)`, then `display_wave1_report(report, console)`
  4. Call `evaluate_quality_gates(report)`, then `display_gate_results(gates, console)`
  5. Call `recommend_strategy(report)` and display recommendation
  6. If `--export-csv`: call `export_wave1_csv(db, export_path)`
  7. Print next step based on gate results:
     - If passed: "Quality gates PASSED. Select strategy: objlib metadata wave1-select <strategy>"
     - If failed: "Quality gates FAILED. Consider Wave 1.5 or manual adjustments."

**3. `@metadata_app.command("wave1-select")`:**
- Args: `strategy` (str -- "minimalist", "teacher", "reasoner", or "hybrid")
- Options: `--db` (Path)
- Implementation:
  1. Validate strategy name is valid
  2. If "hybrid": print message that hybrid template will be constructed from best elements of each strategy during Wave 2 setup
  3. Save selection to a simple JSON file: `data/wave1_selection.json` with: strategy, timestamp, prompt_version
  4. Print confirmation: "Strategy '{strategy}' selected for Wave 2 production processing."
  5. Print next step: "Run 'objlib metadata extract' to process remaining files with the selected strategy."

Note: Import extraction modules INSIDE command functions (not at module top) to keep CLI startup fast for non-extraction commands. Follow the same pattern as the existing `upload` command.
  </action>
  <verify>
```bash
# Verify CLI commands are registered
python -m objlib metadata --help 2>&1 | grep -q "extract-wave1" && echo "extract-wave1 registered"
python -m objlib metadata --help 2>&1 | grep -q "wave1-report" && echo "wave1-report registered"
python -m objlib metadata --help 2>&1 | grep -q "wave1-select" && echo "wave1-select registered"

# Verify help text
python -m objlib metadata extract-wave1 --help 2>&1 | grep -q "resume" && echo "extract-wave1 --resume option OK"
python -m objlib metadata wave1-report --help 2>&1 | grep -q "export-csv" && echo "wave1-report --export-csv option OK"
python -m objlib metadata wave1-select --help 2>&1 | grep -q "strategy" && echo "wave1-select strategy arg OK"
```
  </verify>
  <done>
CLI commands extract-wave1 (with --resume), wave1-report (with --export-csv), and wave1-select are registered under metadata subcommand group. Imports are deferred to command functions for fast CLI startup.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Wave 1 discovery infrastructure: competitive strategy execution (3 strategies x 20 test files), comparison report generation, quality gate evaluation, and strategy selection CLI.

Full Wave 1 workflow:
1. `objlib metadata extract-wave1` -- runs 3 strategies against 20 stratified test files
2. `objlib metadata wave1-report` -- compares strategies side-by-side with metrics
3. `objlib metadata wave1-report --export-csv wave1_results.csv` -- exports for detailed review
4. `objlib metadata wave1-select teacher` -- selects winning strategy for Wave 2
  </what-built>
  <how-to-verify>
1. Set Mistral API key: `objlib config set-mistral-key YOUR_KEY`
2. Run Wave 1: `objlib metadata extract-wave1 --db data/library.db`
   - Expect: 60 API calls (20 files x 3 strategies), Rich progress display
   - If credits exhausted: checkpoint saved, resume with `--resume` flag
3. Review results: `objlib metadata wave1-report --db data/library.db`
   - Expect: Table comparing 3 strategies on metrics
   - Expect: Quality gate evaluation (PASS/FAIL per gate)
   - Expect: Strategy recommendation
4. Optionally export: `objlib metadata wave1-report --export-csv data/wave1_export.csv`
5. Select strategy: `objlib metadata wave1-select teacher --db data/library.db`
   - (or whichever strategy won)

**Key verification questions:**
- Do the extracted categories look reasonable for the test files?
- Is the recommended strategy sensible (best validation rate + confidence)?
- Did all quality gates pass? If not, is the failure expected?
  </how-to-verify>
  <resume-signal>Type "approved" with selected strategy name, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
```bash
# Pre-execution verification (no API calls needed)
python -c "
# All CLI commands registered
import subprocess
result = subprocess.run(['python', '-m', 'objlib', 'metadata', '--help'], capture_output=True, text=True)
assert 'extract-wave1' in result.stdout
assert 'wave1-report' in result.stdout
assert 'wave1-select' in result.stdout

# Report and gates modules work
from objlib.extraction.report import generate_wave1_report, display_wave1_report
from objlib.extraction.quality_gates import evaluate_quality_gates, recommend_strategy

print('Plan 03 pre-execution verification passed')
"
```
</verification>

<success_criteria>
- `objlib metadata extract-wave1` runs 20 files x 3 strategies = 60 API calls
- `objlib metadata extract-wave1 --resume` resumes from checkpoint after pause
- `objlib metadata wave1-report` shows comparison table with metrics
- Quality gates evaluated with clear PASS/FAIL per gate
- Strategy recommendation produced (single winner or hybrid)
- Human can review and select strategy via `wave1-select`
- All wave1_results persisted to database for later analysis
</success_criteria>

<output>
After completion, create `.planning/phases/06-ai-powered-metadata/06-03-SUMMARY.md`
</output>
