---
phase: 06-ai-powered-metadata
plan: 04
type: execute
wave: 4
depends_on: ["06-03"]
files_modified:
  - src/objlib/extraction/validator.py
  - src/objlib/extraction/confidence.py
  - src/objlib/extraction/chunker.py
  - src/objlib/extraction/orchestrator.py
  - src/objlib/extraction/prompts.py
autonomous: true

must_haves:
  truths:
    - "Two-level validation engine distinguishes hard failures (reject+retry) from soft warnings (accept+flag)"
    - "Confidence scoring uses multi-dimensional weighted average across all 4 tiers"
    - "Adaptive chunker handles files from 2KB to 7MB using full-text or head-tail windowing"
    - "Production orchestrator processes files in batches with validated prompt template"
    - "Results saved with prompt versioning (model, prompt_version, extraction_config_hash)"
    - "Partial extractions accepted with appropriate status flags"
  artifacts:
    - path: "src/objlib/extraction/validator.py"
      provides: "Two-level validation with hard rules (reject) and soft rules (warn)"
      contains: "def validate_extraction"
    - path: "src/objlib/extraction/confidence.py"
      provides: "Multi-dimensional confidence scoring with tier-specific weighting"
      contains: "def calculate_confidence"
    - path: "src/objlib/extraction/chunker.py"
      provides: "Adaptive context window management for transcripts of varying length"
      contains: "def prepare_transcript"
    - path: "src/objlib/extraction/orchestrator.py"
      provides: "Production run_production() method on ExtractionOrchestrator"
      contains: "async def run_production"
  key_links:
    - from: "src/objlib/extraction/orchestrator.py"
      to: "src/objlib/extraction/validator.py"
      via: "validate_extraction called after each API response"
      pattern: "validate_extraction"
    - from: "src/objlib/extraction/orchestrator.py"
      to: "src/objlib/extraction/confidence.py"
      via: "calculate_confidence adjusts model-reported score"
      pattern: "calculate_confidence"
    - from: "src/objlib/extraction/orchestrator.py"
      to: "src/objlib/extraction/chunker.py"
      via: "prepare_transcript before sending to API"
      pattern: "prepare_transcript"
    - from: "src/objlib/extraction/orchestrator.py"
      to: "src/objlib/database.py"
      via: "Saves metadata to file_metadata_ai and file_primary_topics tables"
      pattern: "file_metadata_ai"
---

<objective>
Build the Wave 2 production pipeline: validation engine, confidence scoring, adaptive chunking, and production orchestrator for processing ~453 remaining files.

Purpose: Transform Wave 1's validated prompt template into a reliable batch processing pipeline that extracts, validates, scores, and persists 4-tier metadata for all unknown-category TXT files.
Output: Complete extraction pipeline ready to be triggered by CLI command (Plan 05).
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-ai-powered-metadata/06-CONTEXT.md
@.planning/phases/06-ai-powered-metadata/CLARIFICATIONS-ANSWERED.md
@.planning/phases/06-ai-powered-metadata/06-01-SUMMARY.md
@.planning/phases/06-ai-powered-metadata/06-02-SUMMARY.md
@.planning/phases/06-ai-powered-metadata/06-03-SUMMARY.md

@src/objlib/extraction/orchestrator.py
@src/objlib/extraction/schemas.py
@src/objlib/extraction/client.py
@src/objlib/extraction/prompts.py
@src/objlib/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Validation engine, confidence scorer, and adaptive chunker</name>
  <files>
    src/objlib/extraction/validator.py
    src/objlib/extraction/confidence.py
    src/objlib/extraction/chunker.py
  </files>
  <action>
**1. Create `src/objlib/extraction/validator.py`** with:

- `@dataclass class ValidationResult`: status (MetadataStatus), hard_failures (list[str]), soft_warnings (list[str]), repaired_fields (list[str])

- `def validate_extraction(raw_data: dict) -> ValidationResult`:
  - **Hard rules** (failure = rejected, retry with schema reminder):
    - `category` must be in Category enum values
    - `difficulty` must be in Difficulty enum values
    - `primary_topics` must have 3-8 items, all in CONTROLLED_VOCABULARY (after filtering invalid ones; if <3 remain after filtering, hard fail)
    - `confidence_score` must be float 0.0-1.0
  - **Soft rules** (warning = needs_review, still accepted):
    - `topic_aspects` should have 3-10 items (warn if outside range)
    - `semantic_description.summary` should be >= 50 chars (warn if shorter)
    - `semantic_description.key_arguments` should have >= 1 item (warn if empty)
  - **Repair logic** (auto-fix before validation):
    - Filter `primary_topics` to only CONTROLLED_VOCABULARY members (record filtered count in repaired_fields)
    - Clamp `confidence_score` to [0.0, 1.0] if outside range
    - If `category` is close match to valid category (Levenshtein or substring), repair (e.g., "course" -> "course_transcript"). Otherwise hard fail.
  - Return ValidationResult:
    - If any hard failure: status = `failed_validation`
    - If soft warnings but no hard failures: status = `needs_review`
    - If clean: status = `extracted`

- `def build_retry_prompt(failures: list[str]) -> str`:
  - Returns a "schema reminder" prompt fragment: "Your previous response had these issues: {failures}. Please fix and return valid JSON."

**2. Create `src/objlib/extraction/confidence.py`** with:

- `def calculate_confidence(model_confidence: float, validation: ValidationResult, transcript_length: int) -> float`:
  - Per W2.A7 decision (multi-dimensional weighted average):
  - `tier1_conf`: 1.0 if category+difficulty valid, 0.3 otherwise
  - `tier2_conf`: len(valid_primary_topics) / 8.0, clamped to [0.0, 1.0]
  - `tier3_conf`: 0.8 if 3 <= len(topic_aspects) <= 10, else 0.5
  - `tier4_conf`: 0.9 if summary >= 50 chars, else 0.6
  - Weighted average: tier1 * 0.30 + tier2 * 0.40 + tier3 * 0.15 + tier4 * 0.15
  - Penalties:
    - -0.25 if any hard validation repair was needed
    - -0.10 per soft warning (max -0.30 total)
    - -0.15 if transcript_length < 800 chars but tier4 confidence > 0.7 (hallucination risk)
  - Final: max(0.0, min(1.0, weighted - penalties))
  - Return rounded to 2 decimal places

**3. Create `src/objlib/extraction/chunker.py`** with:

- `def estimate_tokens(text: str) -> int`:
  - Simple heuristic: len(text) / 4 (approximate 4 chars per token for English)

- `def prepare_transcript(file_path: str, max_tokens: int = 18000) -> str`:
  - Read file from disk (UTF-8 with fallback to latin-1)
  - Call `estimate_tokens()` on full text
  - If <= max_tokens: return full text
  - If <= max_tokens * 1.5 (slightly over): head-tail strategy
    - Extract first 70% tokens worth of text
    - Extract last 30% tokens worth of text
    - Return: "[START]\n{head}\n\n[...CONTENT TRUNCATED...]\n\n[END]\n{tail}"
  - If > max_tokens * 1.5 (very long): windowed sampling
    - Extract first 3000 tokens of text
    - Extract 3 evenly-spaced windows of 600 tokens each from middle
    - Extract last 3000 tokens of text
    - Return: "[START]\n{head}\n\n[EXCERPT 1]\n{w1}\n\n[EXCERPT 2]\n{w2}\n\n[EXCERPT 3]\n{w3}\n\n[END]\n{tail}"
  - Add instruction note if truncated: "\n\n[NOTE: Content was excerpted. Base metadata on provided text only. Lower confidence_score if context seems insufficient.]"
  </action>
  <verify>
```bash
# Verify validator
python -c "
from objlib.extraction.validator import validate_extraction, ValidationResult, build_retry_prompt
from objlib.extraction.schemas import MetadataStatus

# Valid extraction
result = validate_extraction({
    'category': 'course_transcript',
    'difficulty': 'intermediate',
    'primary_topics': ['epistemology', 'concept_formation', 'reason'],
    'topic_aspects': ['measurement omission', 'unit-economy', 'hierarchical concepts'],
    'semantic_description': {
        'summary': 'A lecture on concept formation discussing measurement omission and unit-economy principles.',
        'key_arguments': ['Concepts formed via measurement omission'],
        'philosophical_positions': []
    },
    'confidence_score': 0.85
})
assert result.status == MetadataStatus.EXTRACTED, f'Expected extracted, got {result.status}'
assert len(result.hard_failures) == 0
print(f'Valid: status={result.status.value}, warnings={len(result.soft_warnings)}')

# Hard failure: bad category
result2 = validate_extraction({
    'category': 'invalid_cat',
    'difficulty': 'easy',
    'primary_topics': ['epistemology'],
    'topic_aspects': [],
    'semantic_description': {'summary': 'short', 'key_arguments': [], 'philosophical_positions': []},
    'confidence_score': 0.5
})
assert result2.status == MetadataStatus.FAILED_VALIDATION
print(f'Invalid: status={result2.status.value}, failures={result2.hard_failures}')

# Soft warning: short summary
result3 = validate_extraction({
    'category': 'qa_session',
    'difficulty': 'intro',
    'primary_topics': ['ethics', 'rational_egoism', 'values'],
    'topic_aspects': ['short aspect', 'another', 'third'],
    'semantic_description': {
        'summary': 'A brief Q&A.',  # < 50 chars
        'key_arguments': ['Egoism is rational'],
        'philosophical_positions': []
    },
    'confidence_score': 0.7
})
assert result3.status == MetadataStatus.NEEDS_REVIEW
print(f'Soft warning: status={result3.status.value}, warnings={result3.soft_warnings}')
print('Validator OK')
"

# Verify confidence scorer
python -c "
from objlib.extraction.confidence import calculate_confidence
from objlib.extraction.validator import validate_extraction, ValidationResult
from objlib.extraction.schemas import MetadataStatus

result = ValidationResult(status=MetadataStatus.EXTRACTED, hard_failures=[], soft_warnings=[], repaired_fields=[])
conf = calculate_confidence(model_confidence=0.85, validation=result, transcript_length=15000)
assert 0.0 <= conf <= 1.0
print(f'Confidence: {conf}')
print('Confidence scorer OK')
"

# Verify chunker
python -c "
from objlib.extraction.chunker import estimate_tokens, prepare_transcript
tokens = estimate_tokens('Hello world, this is a test.')
assert tokens > 0
print(f'Token estimate: {tokens}')
print('Chunker OK')
"
```
  </verify>
  <done>
Validation engine applies hard rules (category/difficulty enum, primary_topics from vocabulary, confidence range) and soft rules (aspect count, summary length, key_arguments presence). Confidence scorer computes weighted average across 4 tiers with penalties. Adaptive chunker handles files up to 7MB via head-tail windowing or excerpted sampling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Production orchestrator with versioned metadata persistence</name>
  <files>
    src/objlib/extraction/orchestrator.py
    src/objlib/extraction/prompts.py
  </files>
  <action>
**1. Update `src/objlib/extraction/prompts.py`:**

Add:
- `def build_production_prompt(strategy: str, schema: str) -> str`:
  - Loads the winning strategy template (or hybrid)
  - Uses temperature=1.0 (magistral production requirement)
  - Returns complete system prompt for production use
- `def hash_extraction_config(temperature: float, timeout: int, schema_version: str, vocab_hash: str) -> str`:
  - Creates canonical dict, serializes to sorted JSON, returns sha256 hexdigest truncated to 16 chars
  - Per W2.A10 decision (semantic versioning with config hashing)

**2. Update `src/objlib/extraction/orchestrator.py`:**

Implement `async def run_production(self, files: list[dict], strategy_name: str) -> dict` on ExtractionOrchestrator (replace the NotImplementedError placeholder):

- Load winning strategy config from `data/wave1_selection.json` (or use strategy_name param)
- Build production prompt using `build_production_prompt()`
- Process each file:
  1. Read transcript via `prepare_transcript(file_path)`
  2. Build user prompt with transcript
  3. API call via `self._client.extract_metadata()` with temperature=1.0 (ALWAYS for production, regardless of Wave 1 strategy temperature)
  4. Parse response
  5. Validate via `validate_extraction()`
  6. If hard failure: retry once with `build_retry_prompt(failures)` appended to user message
  7. If still fails after retry: mark as `failed_validation`, save with reduced confidence
  8. Calculate confidence via `calculate_confidence()`
  9. Save to database:
     - Update `files` table: set `ai_metadata_status`, `ai_confidence_score`
     - Insert into `file_metadata_ai`: full metadata_json, model, prompt_version, extraction_config_hash, is_current=1 (mark previous versions as is_current=0 first)
     - Insert into `file_primary_topics`: one row per valid primary_topic tag
  10. Save checkpoint after each file (atomic per-file)

- Handle credit exhaustion (402): same checkpoint + pause as Wave 1
- Handle rate limits (429): exponential backoff with jitter
- Track per-file: status, tokens, latency, confidence

- Return summary dict: {total: int, extracted: int, needs_review: int, failed: int, partial: int, total_tokens: int, estimated_cost: float, avg_latency_ms: float}

Add helper method `def _save_production_result(self, file_path: str, metadata: dict, validation: ValidationResult, confidence: float, tokens: int, config: dict) -> None`:
  - Opens database
  - In a transaction:
    1. `UPDATE files SET ai_metadata_status = ?, ai_confidence_score = ? WHERE file_path = ?`
    2. `UPDATE file_metadata_ai SET is_current = 0 WHERE file_path = ? AND is_current = 1`
    3. `INSERT INTO file_metadata_ai (file_path, metadata_json, model, prompt_version, extraction_config_hash, is_current) VALUES (...)`
    4. `DELETE FROM file_primary_topics WHERE file_path = ?` (clear old topics)
    5. `INSERT INTO file_primary_topics (file_path, topic_tag) VALUES (?, ?)` for each valid primary_topic

Also add `def _get_pending_extraction_files(self, db: Database) -> list[dict]`:
  - Query: `SELECT file_path, filename, file_size FROM files WHERE filename LIKE '%.txt' AND json_extract(metadata_json, '$.category') = 'unknown' AND ai_metadata_status IN ('pending', 'failed_json', 'retry_scheduled') ORDER BY file_path`
  - Exclude files already in wave1_results (they were test files)
  - Return list of dicts
  </action>
  <verify>
```bash
# Verify production orchestrator
python -c "
from objlib.extraction.orchestrator import ExtractionOrchestrator, ExtractionConfig
from objlib.extraction.prompts import build_production_prompt, hash_extraction_config, PROMPT_VERSION

# Verify production prompt builder
prompt = build_production_prompt('teacher', 'test_schema')
assert len(prompt) > 100
print(f'Production prompt: {len(prompt)} chars')

# Verify config hashing
h = hash_extraction_config(1.0, 240, '1.0', 'abcd1234')
assert len(h) == 16
print(f'Config hash: {h}')

# Verify run_production is no longer NotImplementedError
import inspect
source = inspect.getsource(ExtractionOrchestrator.run_production)
assert 'NotImplementedError' not in source
print('Production orchestrator implemented')
print('Production pipeline OK')
"

# Verify database save logic
python -c "
import tempfile, os
from objlib.database import Database
from objlib.extraction.orchestrator import ExtractionOrchestrator, ExtractionConfig

# Create test database
db = Database(os.path.join(tempfile.mkdtemp(), 'test.db'))
# Verify the new tables are queryable
db.conn.execute('SELECT * FROM file_metadata_ai LIMIT 0')
db.conn.execute('SELECT * FROM file_primary_topics LIMIT 0')
print('Database schema OK for production saves')
db.close()
"
```
  </verify>
  <done>
Production orchestrator processes files with validated prompt template (temperature=1.0), two-level validation with retry, multi-dimensional confidence scoring, and versioned metadata persistence to file_metadata_ai and file_primary_topics tables. Checkpoint/resume handles credit exhaustion. Each file's metadata saved atomically with prompt versioning and config hashing.
  </done>
</task>

</tasks>

<verification>
```bash
# Full production pipeline verification (no API calls)
python -c "
from objlib.extraction.validator import validate_extraction
from objlib.extraction.confidence import calculate_confidence
from objlib.extraction.chunker import prepare_transcript, estimate_tokens
from objlib.extraction.orchestrator import ExtractionOrchestrator, ExtractionConfig
from objlib.extraction.prompts import build_production_prompt, hash_extraction_config

# Simulate full validation pipeline
raw = {
    'category': 'concept_exploration',
    'difficulty': 'advanced',
    'primary_topics': ['metaphysics', 'causality', 'identity', 'existence'],
    'topic_aspects': ['causal law', 'entity action', 'primacy of existence'],
    'semantic_description': {
        'summary': 'An advanced exploration of causality in Objectivist metaphysics, examining the relationship between entities and their actions.',
        'key_arguments': ['Causality is entity-based', 'Actions follow from nature of entity'],
        'philosophical_positions': ['Objectivist causality vs Humean skepticism']
    },
    'confidence_score': 0.82
}
validation = validate_extraction(raw)
assert validation.status.value in ('extracted', 'needs_review')
confidence = calculate_confidence(0.82, validation, 15000)
assert 0.0 <= confidence <= 1.0
print(f'Status: {validation.status.value}, Confidence: {confidence}')
print('Full production pipeline verification passed')
"
```
</verification>

<success_criteria>
- Validation engine correctly classifies extractions as extracted/needs_review/failed_validation
- Invalid primary_topics silently filtered; <3 remaining triggers hard failure
- Confidence scorer produces reasonable scores (0.6-0.95 range for typical inputs)
- Adaptive chunker handles full range of file sizes (2KB-7MB)
- Production orchestrator saves versioned metadata with config hashing
- Retry logic sends schema reminder on hard validation failure
- Partial extractions accepted with appropriate status
</success_criteria>

<output>
After completion, create `.planning/phases/06-ai-powered-metadata/06-04-SUMMARY.md`
</output>
