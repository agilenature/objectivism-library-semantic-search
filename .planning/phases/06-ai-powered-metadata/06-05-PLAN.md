---
phase: 06-ai-powered-metadata
plan: 05
type: execute
wave: 5
depends_on: ["06-04"]
files_modified:
  - src/objlib/cli.py
  - src/objlib/extraction/review.py
  - src/objlib/database.py
autonomous: false

must_haves:
  truths:
    - "CLI command 'objlib metadata extract' processes remaining ~453 files with validated prompt"
    - "CLI command 'objlib metadata extract --resume' resumes production processing from checkpoint"
    - "CLI command 'objlib metadata review' displays 4-tier metadata with Rich panels for interactive review"
    - "CLI command 'objlib metadata approve --min-confidence 0.85' auto-approves high-confidence extractions"
    - "CLI command 'objlib metadata review --status needs_review' filters to files needing attention"
    - "Auto-approve threshold: confidence >= 0.85 sets status to 'approved', 0.70-0.84 requires interactive review"
    - "All metadata persisted to SQLite with versioning (model, prompt_version, config_hash)"
  artifacts:
    - path: "src/objlib/extraction/review.py"
      provides: "Rich 4-tier metadata display and interactive review workflow"
      contains: "def display_metadata_panel"
    - path: "src/objlib/cli.py"
      provides: "Production extraction and review CLI commands"
      contains: "extract_production"
    - path: "src/objlib/database.py"
      provides: "Query methods for AI metadata status, confidence filtering, approval"
      contains: "get_ai_metadata_stats"
  key_links:
    - from: "src/objlib/cli.py"
      to: "src/objlib/extraction/orchestrator.py"
      via: "extract command calls orchestrator.run_production()"
      pattern: "orchestrator\\.run_production"
    - from: "src/objlib/extraction/review.py"
      to: "src/objlib/database.py"
      via: "Reads file_metadata_ai table for display"
      pattern: "file_metadata_ai"
    - from: "src/objlib/cli.py"
      to: "src/objlib/extraction/review.py"
      via: "review command uses display_metadata_panel"
      pattern: "display_metadata_panel"
---

<objective>
Add CLI commands for Wave 2 production extraction and the interactive review/approve workflow with Rich 4-tier metadata display.

Purpose: Complete the user-facing Phase 6 experience: run extraction, review results with beautiful Rich panels, approve/reject, and export for analysis.
Output: Full metadata extraction CLI workflow from extraction through review and approval.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-ai-powered-metadata/06-CONTEXT.md
@.planning/phases/06-ai-powered-metadata/CLARIFICATIONS-ANSWERED.md
@.planning/phases/06-ai-powered-metadata/06-01-SUMMARY.md
@.planning/phases/06-ai-powered-metadata/06-02-SUMMARY.md
@.planning/phases/06-ai-powered-metadata/06-03-SUMMARY.md
@.planning/phases/06-ai-powered-metadata/06-04-SUMMARY.md

@src/objlib/extraction/orchestrator.py
@src/objlib/extraction/validator.py
@src/objlib/extraction/confidence.py
@src/objlib/extraction/schemas.py
@src/objlib/cli.py
@src/objlib/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Review module with Rich 4-tier panels and database query methods</name>
  <files>
    src/objlib/extraction/review.py
    src/objlib/database.py
  </files>
  <action>
**1. Create `src/objlib/extraction/review.py`** with:

- `def display_metadata_panel(file_path: str, metadata: dict, confidence: float, status: str, console: Console) -> None`:
  - Display a Rich Panel following the layout from W2.A6 decision:
  ```
  +-- File: filename.txt ----------------------------------------+
  | Category: course_transcript | Difficulty: intermediate        |
  | Confidence: 89% | Status: needs_review                       |
  +-- Primary Topics (Tier 2) -----------------------------------+
  | epistemology  concept_formation  reason                       |
  +-- Topic Aspects (Tier 3) ------------------------------------+
  | * measurement omission principle                              |
  | * unit-economy in concept formation                           |
  | * hierarchical concept organization                           |
  +-- Semantic Description (Tier 4) -----------------------------+
  | Summary: Lecture on how humans form concepts...               |
  | Key Arguments:                                                |
  |   1. Concepts formed by measuring similarities...             |
  | Positions:                                                    |
  |   * Rand's epistemology vs Plato's theory of forms           |
  +--------------------------------------------------------------+
  ```
  - Use Rich styling: category in bold cyan, difficulty color-coded (intro=green, intermediate=yellow, advanced=red), primary_topics as space-separated tags in green, confidence as percentage with color (>0.85 green, 0.70-0.84 yellow, <0.70 red)

- `def display_review_table(files: list[dict], console: Console) -> None`:
  - Display Rich Table with columns: File, Category, Difficulty, Topics (first 3), Confidence %, Status
  - Color-code confidence: green (>=85%), yellow (70-84%), red (<70%)
  - Truncate filename to 40 chars if needed

- `def interactive_review(db: Database, console: Console, status_filter: str | None = None) -> dict`:
  - Query files with `ai_metadata_status` matching filter (or all non-approved)
  - For each file:
    1. Display `display_metadata_panel()`
    2. Prompt: `[A]ccept  [E]dit JSON  [R]erun  [S]kip  [Q]uit`
    3. Accept: set ai_metadata_status = 'approved'
    4. Edit: Open metadata_json in $EDITOR (using `click.edit()` or `typer.edit()` -- check if typer has edit; fallback to subprocess call to $EDITOR on temp file, then parse edited JSON back)
    5. Rerun: mark as `retry_scheduled` (will be picked up by next extract run)
    6. Skip: move to next file
    7. Quit: exit review loop
  - Return summary: {approved: int, edited: int, rerun: int, skipped: int}

**2. Extend `src/objlib/database.py`** with new query methods:

- `def get_ai_metadata_stats(self) -> dict`:
  - Query: `SELECT ai_metadata_status, COUNT(*) as cnt FROM files WHERE ai_metadata_status IS NOT NULL GROUP BY ai_metadata_status`
  - Return dict mapping status -> count

- `def get_files_by_ai_status(self, status: str, limit: int = 50) -> list[dict]`:
  - Query files table joined with file_metadata_ai (where is_current=1)
  - Return list of dicts: file_path, filename, ai_metadata_status, ai_confidence_score, metadata (parsed from file_metadata_ai.metadata_json)

- `def approve_files_by_confidence(self, min_confidence: float) -> int`:
  - `UPDATE files SET ai_metadata_status = 'approved' WHERE ai_metadata_status IN ('extracted', 'needs_review') AND ai_confidence_score >= ?`
  - Return count of updated rows

- `def set_ai_metadata_status(self, file_path: str, status: str) -> None`:
  - `UPDATE files SET ai_metadata_status = ? WHERE file_path = ?`

- `def get_extraction_summary(self) -> dict`:
  - Return comprehensive stats: total unknown TXT files, extracted count, approved count, needs_review count, failed count, avg confidence, min/max confidence
  </action>
  <verify>
```bash
# Verify review module
python -c "
from objlib.extraction.review import display_metadata_panel, display_review_table
print('Review module imports OK')
"

# Verify new database methods
python -c "
import tempfile, os
from objlib.database import Database

db = Database(os.path.join(tempfile.mkdtemp(), 'test.db'))
stats = db.get_ai_metadata_stats()
assert isinstance(stats, dict)
print(f'AI metadata stats: {stats}')

files = db.get_files_by_ai_status('pending')
assert isinstance(files, list)
print(f'Files by status: {len(files)}')

db.close()
print('Database methods OK')
"
```
  </verify>
  <done>
Rich 4-tier metadata panels display category, difficulty, primary_topics, topic_aspects, and semantic_description with appropriate color coding. Interactive review workflow supports Accept/Edit/Rerun/Skip/Quit actions. Database has query methods for AI metadata stats, status filtering, bulk approval, and comprehensive extraction summary.
  </done>
</task>

<task type="auto">
  <name>Task 2: Production extraction and review CLI commands</name>
  <files>src/objlib/cli.py</files>
  <action>
Add production Wave 2 commands to the existing `metadata_app` Typer group in `src/objlib/cli.py`:

**1. `@metadata_app.command("extract")`:**
- Options: `--resume` (flag), `--db` (Path), `--dry-run` (flag), `--set-pending` (flag -- after extraction, set file upload status to pending for re-upload with enriched metadata)
- Implementation:
  1. Load Mistral API key via `get_mistral_api_key()`
  2. Load winning strategy from `data/wave1_selection.json`; if not found, print error: "Run 'objlib metadata extract-wave1' and 'objlib metadata wave1-select' first"
  3. Open database, get count of pending extraction files
  4. If `--dry-run`: show count, estimated cost (count * $0.012), estimated time, exit
  5. Create production orchestrator
  6. Display Rich Panel: "Processing {count} files with strategy '{strategy}' ..."
  7. Run `asyncio.run(orchestrator.run_production(files, strategy))`
  8. Display summary table: Extracted, Needs Review, Failed, Total Tokens, Estimated Cost, Avg Latency
  9. If `--set-pending`: UPDATE files SET status = 'pending' WHERE ai_metadata_status IN ('extracted', 'approved')
  10. Print next steps: "Run 'objlib metadata review' to check results" and "Run 'objlib metadata approve --min-confidence 0.85' to auto-approve"

**2. `@metadata_app.command("review")`:**
- Options: `--db` (Path), `--status` (str, optional -- filter by ai_metadata_status), `--interactive` (flag), `--limit` (int, default 50)
- Implementation:
  1. If `--interactive`: call `interactive_review(db, console, status_filter=status)`
  2. Else: call `get_files_by_ai_status(status or None)`, display with `display_review_table()`
  3. Show stats from `get_ai_metadata_stats()`

**3. `@metadata_app.command("approve")`:**
- Options: `--min-confidence` (float, default 0.85), `--db` (Path), `--yes` (flag, skip confirmation)
- Implementation:
  1. Preview: count how many files would be approved at the given threshold
  2. If not `--yes`: confirm with user
  3. Call `db.approve_files_by_confidence(min_confidence)`
  4. Display: "Approved {count} files with confidence >= {min_confidence}"

**4. `@metadata_app.command("stats")`:**
- Options: `--db` (Path)
- Implementation:
  1. Call `get_extraction_summary(db)` and `get_ai_metadata_stats(db)`
  2. Display Rich Table: Status, Count (with color coding)
  3. Display summary panel: total processed, avg confidence, coverage percentage (extracted+approved / total unknown)

Note: All imports deferred to inside command functions for fast CLI startup.
  </action>
  <verify>
```bash
# Verify all CLI commands registered
python -m objlib metadata --help 2>&1 | grep -q "extract " && echo "extract registered"
python -m objlib metadata --help 2>&1 | grep -q "review" && echo "review registered"
python -m objlib metadata --help 2>&1 | grep -q "approve" && echo "approve registered"
python -m objlib metadata --help 2>&1 | grep -q "stats" && echo "stats registered"

# Verify help text
python -m objlib metadata extract --help 2>&1 | grep -q "resume" && echo "extract --resume OK"
python -m objlib metadata extract --help 2>&1 | grep -q "dry-run" && echo "extract --dry-run OK"
python -m objlib metadata extract --help 2>&1 | grep -q "set-pending" && echo "extract --set-pending OK"
python -m objlib metadata review --help 2>&1 | grep -q "interactive" && echo "review --interactive OK"
python -m objlib metadata approve --help 2>&1 | grep -q "min-confidence" && echo "approve --min-confidence OK"
python -m objlib metadata stats --help 2>&1 | grep -q "db" && echo "stats --db OK"
```
  </verify>
  <done>
Production CLI commands: extract (with --resume, --dry-run, --set-pending), review (with --interactive, --status filter), approve (with --min-confidence threshold), and stats (extraction summary). All integrated into existing metadata subcommand group.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 6 AI-powered metadata extraction system:

**Wave 2 Production Workflow:**
1. `objlib metadata extract --dry-run` -- preview what will be processed and estimated cost
2. `objlib metadata extract` -- process ~453 files with winning strategy
3. `objlib metadata stats` -- see extraction coverage and confidence distribution
4. `objlib metadata approve --min-confidence 0.85` -- auto-approve high-confidence results
5. `objlib metadata review --status needs_review --interactive` -- manually review edge cases
6. `objlib metadata extract --set-pending` -- mark extracted files for re-upload with enriched metadata

**Full Phase 6 Command Suite:**
- `objlib config set-mistral-key` / `get-mistral-key` / `remove-mistral-key`
- `objlib metadata extract-wave1` (Wave 1 discovery)
- `objlib metadata wave1-report` (strategy comparison)
- `objlib metadata wave1-select` (pick winner)
- `objlib metadata extract` (Wave 2 production)
- `objlib metadata review` (4-tier Rich panels)
- `objlib metadata approve` (bulk approval)
- `objlib metadata stats` (extraction summary)
  </what-built>
  <how-to-verify>
1. Run dry-run: `objlib metadata extract --dry-run --db data/library.db`
   - Expect: count of pending files, estimated cost, time estimate
2. Run production extraction: `objlib metadata extract --db data/library.db`
   - Expect: Rich progress display, per-file results
   - If credits exhausted: checkpoint saved, resume with `--resume`
3. Check stats: `objlib metadata stats --db data/library.db`
   - Expect: table showing extracted/needs_review/failed counts
4. Auto-approve: `objlib metadata approve --min-confidence 0.85 --db data/library.db`
   - Expect: count of approved files
5. Interactive review: `objlib metadata review --status needs_review --interactive --db data/library.db`
   - Expect: Rich 4-tier panels with Accept/Edit/Rerun/Skip/Quit actions
6. Optionally set pending for re-upload: `objlib metadata extract --set-pending --db data/library.db`

**Key verification questions:**
- Are extracted categories reasonable? (course_transcript, qa_session, etc.)
- Are primary_topics from the controlled vocabulary?
- Are topic_aspects genuinely derived from file content (not hallucinated)?
- Does confidence scoring correlate with apparent quality?
- Does the review workflow feel efficient?
  </how-to-verify>
  <resume-signal>Type "approved" if extraction quality is acceptable, or describe issues to address</resume-signal>
</task>

</tasks>

<verification>
```bash
# Pre-run verification (no API calls)
python -c "
import subprocess
result = subprocess.run(['python', '-m', 'objlib', 'metadata', '--help'], capture_output=True, text=True)
commands = ['extract-wave1', 'wave1-report', 'wave1-select', 'extract', 'review', 'approve', 'stats']
for cmd in commands:
    assert cmd in result.stdout, f'Missing command: {cmd}'

# Verify all extraction module imports
from objlib.extraction.schemas import ExtractedMetadata, CONTROLLED_VOCABULARY
from objlib.extraction.client import MistralClient
from objlib.extraction.parser import parse_magistral_response
from objlib.extraction.prompts import build_system_prompt, build_production_prompt
from objlib.extraction.strategies import WAVE1_STRATEGIES
from objlib.extraction.checkpoint import CheckpointManager
from objlib.extraction.orchestrator import ExtractionOrchestrator
from objlib.extraction.sampler import select_test_files
from objlib.extraction.validator import validate_extraction
from objlib.extraction.confidence import calculate_confidence
from objlib.extraction.chunker import prepare_transcript
from objlib.extraction.report import generate_wave1_report
from objlib.extraction.quality_gates import evaluate_quality_gates
from objlib.extraction.review import display_metadata_panel

print('All Phase 6 modules and commands verified')
"
```
</verification>

<success_criteria>
- `objlib metadata extract` processes remaining ~453 files with validated prompt
- `objlib metadata extract --dry-run` shows file count and cost estimate
- `objlib metadata review` displays 4-tier Rich panels
- `objlib metadata review --interactive` provides Accept/Edit/Rerun/Skip/Quit workflow
- `objlib metadata approve --min-confidence 0.85` auto-approves high-confidence files
- `objlib metadata stats` shows comprehensive extraction statistics
- All metadata versioned with model, prompt_version, config_hash
- Human review confirms extraction quality is acceptable
</success_criteria>

<output>
After completion, create `.planning/phases/06-ai-powered-metadata/06-05-SUMMARY.md`
</output>
