---
phase: 06.2-metadata-enriched-gemini-upload
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/objlib/database.py
  - src/objlib/upload/metadata_builder.py
  - src/objlib/upload/content_preparer.py
  - src/objlib/upload/state.py
  - src/objlib/upload/__init__.py
autonomous: true

must_haves:
  truths:
    - "Schema migration v5 adds upload_attempt_count and last_upload_hash columns to files table without colliding with existing columns"
    - "build_enriched_metadata() produces valid Gemini CustomMetadata dicts using string_value, numeric_value, and string_list_value for 7-9 fields"
    - "prepare_enriched_content() creates temp files with Tier 4 AI analysis prepended to original file text"
    - "get_enriched_pending_files() returns only files that have BOTH ai_metadata (is_current=1) AND entity_extraction_status='entities_done' AND status='pending'"
  artifacts:
    - path: "src/objlib/upload/metadata_builder.py"
      provides: "build_enriched_metadata function"
      contains: "string_list_value"
    - path: "src/objlib/upload/content_preparer.py"
      provides: "prepare_enriched_content function"
      contains: "AI Analysis"
    - path: "src/objlib/database.py"
      provides: "Schema migration v5"
      contains: "MIGRATION_V5_SQL"
    - path: "src/objlib/upload/state.py"
      provides: "get_enriched_pending_files query"
      contains: "file_metadata_ai"
  key_links:
    - from: "src/objlib/upload/metadata_builder.py"
      to: "src/objlib/upload/client.py"
      via: "build_enriched_metadata returns list[dict] consumed by import_to_store config.custom_metadata"
      pattern: "string_list_value.*values"
    - from: "src/objlib/upload/state.py"
      to: "src/objlib/database.py"
      via: "get_enriched_pending_files JOINs file_metadata_ai and checks entity_extraction_status"
      pattern: "JOIN file_metadata_ai"
    - from: "src/objlib/upload/content_preparer.py"
      to: "src/objlib/upload/orchestrator.py"
      via: "temp file path replaces original file_path for upload_file call"
      pattern: "NamedTemporaryFile"
---

<objective>
Build the data transformation layer for enriched Gemini uploads: schema migration, metadata builder, content preparer, and enriched pipeline query.

Purpose: These components transform raw Phase 6 AI metadata and Phase 6.1 entity data into Gemini-compatible formats. Without them, the upload pipeline cannot carry enriched metadata. This plan creates all the building blocks; Plan 02 wires them into the CLI and orchestrator.

Output: Four production-ready modules (migration, metadata builder, content preparer, enriched query) that the CLI upload command can compose into a complete enriched upload pipeline.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06.2-metadata-enriched-gemini-upload/06.2-RESEARCH.md

@src/objlib/database.py
@src/objlib/upload/client.py
@src/objlib/upload/state.py
@src/objlib/upload/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Schema migration v5 and enriched metadata builder</name>
  <files>
    src/objlib/database.py
    src/objlib/upload/metadata_builder.py
    src/objlib/upload/__init__.py
  </files>
  <action>
**Schema migration v5** in `src/objlib/database.py`:

1. Add `MIGRATION_V5_SQL` constant (empty string -- no new tables needed, just ALTER columns).

2. In `_setup_schema()`, after the `if version < 4:` block, add:
```python
if version < 5:
    for alter_sql in [
        "ALTER TABLE files ADD COLUMN upload_attempt_count INTEGER DEFAULT 0",
        "ALTER TABLE files ADD COLUMN last_upload_hash TEXT",
    ]:
        try:
            self.conn.execute(alter_sql)
        except sqlite3.OperationalError:
            pass  # Column already exists
```
3. Update `PRAGMA user_version` to 5 (change existing `self.conn.execute("PRAGMA user_version = 4")` to `= 5`).

CRITICAL: Do NOT add upload_status, gemini_file_id, gemini_uri, upload_timestamp, or upload_error columns -- these ALREADY EXIST in the schema (as `status`, `gemini_file_id`, `gemini_file_uri`, `upload_timestamp`, `error_message`). See RESEARCH.md "Existing Schema Columns (Audit)" section.

**Enriched metadata builder** in NEW file `src/objlib/upload/metadata_builder.py`:

Create a `build_enriched_metadata()` function that takes three arguments:
- `phase1_metadata: dict` -- from files.metadata_json (parsed JSON)
- `ai_metadata: dict` -- from file_metadata_ai.metadata_json (parsed JSON)
- `entity_names: list[str]` -- canonical names from transcript_entity JOIN person

Returns `list[dict[str, Any]]` in Gemini CustomMetadata format. Fields to include (7-9 total, max 20 allowed):

1. `category` (string_value) -- from `ai_metadata.get("category")`. Fall back to `phase1_metadata.get("category")` if absent.
2. `difficulty` (string_value) -- from `ai_metadata.get("difficulty")`. Fall back to `phase1_metadata.get("difficulty")`.
3. `topics` (string_list_value) -- from `ai_metadata.get("primary_topics", [])`. Format: `{"key": "topics", "string_list_value": {"values": [...]}}`. Max 8 items.
4. `aspects` (string_list_value) -- from `ai_metadata.get("topic_aspects", [])`. Max 10 items.
5. `entities` (string_list_value) -- from `entity_names` parameter. Only if non-empty.
6. `key_themes` (string_list_value) -- from `ai_metadata.get("semantic_description", {}).get("key_arguments", [])`. Truncate each to 200 chars, max 5 items.
7. `source_type` (string_value) -- always `"objectivism_library"`.
8. `course` (string_value) -- from `phase1_metadata.get("course")` if present.
9. `quality_score` (numeric_value) -- from `ai_metadata.get("confidence_score")` as float.

CRITICAL format for string_list_value: `{"values": ["a", "b"]}` NOT `["a", "b"]`. The SDK expects `StringListDict` with a `values` wrapper.

Also create a `compute_upload_hash()` function that takes `phase1_metadata`, `ai_metadata`, `entity_names`, and `file_content_hash` and returns a SHA-256 hex string. This enables idempotency detection -- skip upload if hash matches `last_upload_hash` in DB.

**Update `src/objlib/upload/__init__.py`** to export `build_enriched_metadata`, `compute_upload_hash`.
  </action>
  <verify>
Run: `python -c "from objlib.upload.metadata_builder import build_enriched_metadata, compute_upload_hash; print('imports OK')"`

Run: `python -c "
from objlib.upload.metadata_builder import build_enriched_metadata
result = build_enriched_metadata(
    {'category': 'course', 'course': 'OPAR'},
    {'category': 'course_transcript', 'difficulty': 'intermediate', 'primary_topics': ['epistemology', 'concept_formation'], 'topic_aspects': ['measurement omission'], 'semantic_description': {'key_arguments': ['Knowledge requires conceptual integration']}, 'confidence_score': 0.85},
    ['Leonard Peikoff', 'Ayn Rand']
)
# Check string_list_value format
for entry in result:
    if 'string_list_value' in entry:
        assert 'values' in entry['string_list_value'], f'Missing values wrapper in {entry}'
print(f'OK: {len(result)} metadata entries')
for e in result: print(f'  {e[\"key\"]}: {list(e.keys())}')
"`

Verify schema migration: `python -c "
from objlib.database import Database
db = Database('data/library.db')
v = db.conn.execute('PRAGMA user_version').fetchone()[0]
assert v == 5, f'Expected version 5, got {v}'
# Check new columns exist
db.conn.execute('SELECT upload_attempt_count, last_upload_hash FROM files LIMIT 1')
print('Schema v5 OK')
"`
  </verify>
  <done>
Schema migration v5 adds upload_attempt_count and last_upload_hash to files table. build_enriched_metadata() returns 7-9 Gemini CustomMetadata dicts with correct string_list_value format. compute_upload_hash() returns deterministic SHA-256 for idempotency.
  </done>
</task>

<task type="auto">
  <name>Task 2: Content preparer and enriched pipeline query</name>
  <files>
    src/objlib/upload/content_preparer.py
    src/objlib/upload/state.py
    src/objlib/upload/__init__.py
  </files>
  <action>
**Content preparer** in NEW file `src/objlib/upload/content_preparer.py`:

Create a `prepare_enriched_content()` function:
- Args: `original_file_path: str`, `ai_metadata: dict`
- Returns: `str` (path to temporary file -- caller must clean up with `os.unlink()`)
- Creates a `tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False, encoding="utf-8")`
- Writes header in this format:
```
[AI Analysis]
Category: {category} | Difficulty: {difficulty}

Summary: {tier4_summary}

Key Arguments:
- {argument_1}
- {argument_2}

Philosophical Positions:
- {position_1}

[Original Content]
{original file text}
```
- Extract `summary` from `ai_metadata.get("semantic_description", {}).get("summary", "")`
- Extract `key_arguments` from `ai_metadata.get("semantic_description", {}).get("key_arguments", [])`
- Extract `philosophical_positions` from `ai_metadata.get("semantic_description", {}).get("philosophical_positions", [])`
- If ALL Tier 4 fields are empty (no summary, no arguments, no positions), skip the header entirely and return None (indicating no content injection needed -- use original file as-is).
- Read original file with `Path(original_file_path).read_text(encoding="utf-8")`

Also create a `cleanup_temp_file(path: str | None) -> None` helper that safely unlinks a temp file (handles None, FileNotFoundError).

**Enriched pipeline query** in `src/objlib/upload/state.py`:

Add method `get_enriched_pending_files(self, limit: int = 200, include_needs_review: bool = True) -> list[dict]` to `AsyncUploadStateManager`:

The query must JOIN `files` with `file_metadata_ai` and check entity extraction status:
```sql
SELECT f.file_path, f.content_hash, f.filename, f.file_size,
       f.metadata_json as phase1_metadata_json,
       m.metadata_json as ai_metadata_json,
       f.last_upload_hash
FROM files f
JOIN file_metadata_ai m
    ON f.file_path = m.file_path AND m.is_current = 1
WHERE f.filename LIKE '%.txt'
  AND f.entity_extraction_status = 'entities_done'
  AND f.status = 'pending'
  AND f.ai_metadata_status IN ('extracted', 'approved'{', 'needs_review'' if include_needs_review else ''})
ORDER BY f.file_path
LIMIT ?
```

For each row, also fetch entity canonical names (separate query):
```sql
SELECT p.canonical_name
FROM transcript_entity te
JOIN person p ON te.person_id = p.person_id
WHERE te.transcript_id = ?
ORDER BY te.mention_count DESC
```

Return list of dicts, each containing: file_path, content_hash, filename, file_size, phase1_metadata_json, ai_metadata_json, last_upload_hash, entity_names (list[str]).

Also add `get_files_to_reset_for_enriched_upload(self) -> list[dict]` that returns files with `status IN ('uploaded', 'failed')` that have enriched metadata available (same joins). These are the 17 already-uploaded + 10 failed files that need delete+re-upload.

**Update `src/objlib/upload/__init__.py`** to also export `prepare_enriched_content`, `cleanup_temp_file`.
  </action>
  <verify>
Run: `python -c "from objlib.upload.content_preparer import prepare_enriched_content, cleanup_temp_file; print('imports OK')"`

Run: `python -c "
import os
from objlib.upload.content_preparer import prepare_enriched_content, cleanup_temp_file

# Create a test file
test_path = '/tmp/test_content.txt'
with open(test_path, 'w') as f:
    f.write('Original content here.')

# Test with Tier 4 metadata
tmp = prepare_enriched_content(test_path, {
    'category': 'course_transcript',
    'difficulty': 'intermediate',
    'semantic_description': {
        'summary': 'This lecture discusses concept formation.',
        'key_arguments': ['Concepts require measurement omission'],
        'philosophical_positions': ['Objectivist epistemology']
    }
})
content = open(tmp).read()
assert '[AI Analysis]' in content
assert '[Original Content]' in content
assert 'Original content here.' in content
cleanup_temp_file(tmp)
assert not os.path.exists(tmp)

# Test with empty Tier 4 (should return None)
result = prepare_enriched_content(test_path, {'category': 'unknown'})
assert result is None, 'Expected None for empty Tier 4'
os.unlink(test_path)
print('Content preparer OK')
"`

Run: `python -c "
from objlib.upload.state import AsyncUploadStateManager
import asyncio
async def test():
    async with AsyncUploadStateManager('data/library.db') as state:
        # This will likely return [] since entity extraction hasn't run
        files = await state.get_enriched_pending_files(limit=5)
        print(f'Enriched pending: {len(files)} files')
        resets = await state.get_files_to_reset_for_enriched_upload()
        print(f'Files needing reset: {len(resets)} files')
asyncio.run(test())
"`
  </verify>
  <done>
prepare_enriched_content() creates temp files with Tier 4 AI analysis headers prepended to original text (returns None when no Tier 4 content). cleanup_temp_file() safely removes temp files. get_enriched_pending_files() returns only files with complete AI metadata AND entity extraction via strict JOIN query. get_files_to_reset_for_enriched_upload() identifies already-uploaded files needing re-upload with enriched metadata.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from objlib.upload import build_enriched_metadata, compute_upload_hash, prepare_enriched_content, cleanup_temp_file"` -- all exports resolve
2. `python -c "from objlib.database import Database; db = Database('data/library.db'); v = db.conn.execute('PRAGMA user_version').fetchone()[0]; assert v == 5"` -- schema v5 applied
3. Metadata builder produces correct `string_list_value` format with `{"values": [...]}` wrapper
4. Content preparer creates and cleans up temp files correctly
5. Enriched query enforces strict entity dependency gate (requires entities_done)
</verification>

<success_criteria>
All four components (schema migration, metadata builder, content preparer, enriched query) are importable and functional. Schema v5 adds exactly 2 new columns without colliding with existing ones. Metadata builder produces 7-9 Gemini-compatible CustomMetadata entries. Content preparer handles both Tier 4 present (creates temp file) and absent (returns None) cases. Enriched query correctly JOINs three data sources and enforces the entity gate.
</success_criteria>

<output>
After completion, create `.planning/phases/06.2-metadata-enriched-gemini-upload/06.2-01-SUMMARY.md`
</output>
