# CONTEXT.md ‚Äî Phase 6.2: Metadata-Enriched Gemini Upload

**Generated:** 2026-02-16
**Phase Goal:** User can upload all files to Gemini File Search with enriched 4-tier metadata (category, difficulty, topics, aspects, descriptions) plus entity mentions -- enabling powerful metadata-based filtering and semantic search with full philosophical context
**Synthesis Source:** Multi-provider AI analysis (Gemini Pro, Perplexity Deep Research)

---

## Overview

Phase 6.2 implements concurrent upload of 1,749 transcript files to Google Gemini File Search with enriched metadata from Phase 6 (AI extraction) and Phase 6.1 (entity extraction). This phase transforms the Phase 2 upload pipeline to include 4-tier metadata flattened into Gemini's custom_metadata format, while implementing parallel uploads with rate limiting.

**Current State:**
- Phase 6: 434/1,748 files extracted with AI metadata (24.8% complete)
- Phase 6.1: Entity extraction (15 canonical names) - planned
- Phase 6.2: This phase (concurrent upload) - analyzing now

**Key Technical Challenges:**
- Flattening nested 4-tier metadata into Gemini's flat key-value custom_metadata
- Dependency management: wait for Phase 6.1 entities or upload without?
- Concurrent upload tuning: balance 3-5 parallel uploads with rate limits
- State tracking for mixed processed/unprocessed files (434 done, 1,314 pending)
- Testing strategy before $4.20 indexing investment

**Confidence markers:**
- ‚úÖ **Consensus** ‚Äî Both providers identified this as critical
- ‚ö†Ô∏è **Recommended** ‚Äî One provider identified, strong rationale
- üîç **Needs Clarification** ‚Äî Unique insight, potentially important

---

## Gray Areas Identified

### ‚úÖ 1. Metadata Schema & Tier 4 Flattening Strategy (Consensus)

**What needs to be decided:**
How to map the complex 4-tier metadata structure (especially long-form Tier 4 semantic descriptions) into Gemini's flat custom_metadata key-value format.

**Why it's ambiguous:**
Gemini custom_metadata is designed for filtering (e.g., `WHERE category = 'book'`), not storing prose. Tier 4 contains long-form JSON with summaries and argument structures that don't fit naturally in metadata fields. Requirements say "flatten into Gemini custom_metadata" but don't specify handling of long-form content.

**Provider synthesis:**
- **Gemini:** Proposes split strategy - filtering fields in metadata, long-form content prepended to file text during upload. This ensures embeddings include AI analysis without bloating metadata.
- **Perplexity:** Extensive analysis of metadata flattening approaches, recommends hybrid model with semantic key extraction (5-10 key terms from Tier 4) for metadata, full content in document text.

**Proposed implementation decision:**
Use **split strategy** with separate handling for filtering vs. context:

**Filtering Fields (custom_metadata):**
1. `category` (String) - single value from 17 categories
2. `difficulty` (String) - introductory/intermediate/advanced
3. `topics` (List<String>) - up to 5 from controlled vocabulary
4. `aspects` (List<String>) - up to 5 philosophy domains
5. `entities` (List<String>) - canonical names from Phase 6.1
6. `key_themes` (List<String>) - 5-10 extracted from Tier 4
7. `source_type` (String) - "objectivism_library"

**Context Fields (Content Injection):**
- Tier 4 "summary" and "argument_structure" **prepended to file text** during upload
- Format: `[AI Analysis]\n{summary}\n\n{argument_structure}\n\n[Original Content]\n{file_text}`
- This ensures vector embeddings include AI analysis without metadata bloat

**Rationale:** Separation of concerns - metadata for filtering, content for semantic similarity. Embeddings capture both AI analysis and original text.

**Open questions:**
- Does Gemini API support List<String> for metadata filtering? (Usually yes, needs validation)
- Character limit for metadata values (e.g., 2048 bytes)?
- Should key_themes be extracted algorithmically or use existing Tier 4 data?

**Confidence:** ‚úÖ Both providers agreed on split strategy

---

### ‚úÖ 2. Entity Dependency Management: Phase 6.1 Blocking (Consensus)

**What needs to be decided:**
Whether to block Phase 6.2 upload until Phase 6.1 (entity extraction) completes for each file, or upload without entities and re-upload later.

**Why it's ambiguous:**
Phase 6 extraction is 25% complete (434 files). Phase 6.1 is planned. Gemini doesn't support "patching" metadata - updating requires delete + re-upload, which doubles indexing costs (~$4.20 ‚Üí ~$8.40).

**Provider synthesis:**
- **Gemini:** Recommends strict dependency - file cannot upload until BOTH Phase 6 metadata AND Phase 6.1 entities exist in SQLite. Prevents double-billing and ensures pristine index.
- **Perplexity:** Analyzes stratified upload option (upload now, re-upload later) but notes re-uploading 1,314 files costs ~$3.10 additional. Recommends running Phase 6.1 on 434 ready files immediately before upload.

**Proposed implementation decision:**
Implement **strict dependency with phased execution**:

1. **Phase 6.2a (Immediate):** Upload ONLY files with both Phase 6 metadata AND Phase 6.1 entities
2. **Execution order:**
   - Run Phase 6.1 entity extraction on 434 Phase-6-complete files (estimated: 1-2 hours)
   - Upload those 434 files to Gemini with full metadata + entities
   - Continue as Phase 6 completes more files ‚Üí Phase 6.1 ‚Üí Phase 6.2
3. **Pipeline filter:** Query `WHERE phase_6_complete=true AND entities_extracted=true AND upload_status='pending'`
4. **No re-uploads:** Prevents $3-4 additional cost, ensures consistent metadata quality

**Rationale:** Entity mentions provide significant semantic value (disambiguate "Kant's influence" queries). Re-uploading is expensive. Running Phase 6.1 on 434 files is fast (deterministic-first approach). Better to wait 1-2 hours than pay double.

**Open questions:**
- How fast is Phase 6.1 entity extraction? (Conservative estimate: 5-10 files/min = 43-87 min for 434 files)
- Should we allow manual override to upload without entities? (Proposal: No, strict gate)

**Confidence:** ‚úÖ Both providers agreed strict dependency is optimal

---

### ‚úÖ 3. Concurrency vs. Rate Limiting (Consensus)

**What needs to be decided:**
Precise tuning of parallel upload concurrency (3-5 threads) with rate limiting (0.5-1s delays) to avoid 429 errors while maintaining throughput.

**Why it's ambiguous:**
"3-5 concurrent uploads" with "0.5-1s delay" can be contradictory without central coordination. If 5 threads all wait 0.5s then fire simultaneously, you spike RPM. Gemini API rate limits aren't fully documented.

**Provider synthesis:**
- **Gemini:** Recommends `asyncio.Semaphore(3)` (conservative start), exponential backoff (2s ‚Üí 4s ‚Üí 8s ‚Üí 16s ‚Üí 32s), filesize throttling (reduce concurrency for >10MB files).
- **Perplexity:** Extensive analysis of rate limiting patterns, recommends semaphore-based concurrency control, exponential backoff with jitter, adaptive rate limiting based on response headers (if available).

**Proposed implementation decision:**
Use **conservative configuration with empirical tuning**:

**Initial Configuration:**
- Concurrency: `asyncio.Semaphore(2)` (more conservative than 3-5 estimate)
- Delay between launches: 1 second (conservative)
- Backoff on 429: Exponential with jitter
  - Attempt 1: Immediate
  - Fail 429: Wait 2s + random(0-0.5s)
  - Fail 429: Wait 4s + random(0-1s)
  - Fail 429: Wait 8s + random(0-2s)
  - Fail: Mark `error_upload`, continue batch
- Monitor: Log all 429 errors and response latencies

**Tuning Process:**
1. Upload 50-file test batch with Semaphore(2)
2. If zero 429 errors AND response times <10s ‚Üí increase to Semaphore(3)
3. If 429 errors OR response times >30s ‚Üí keep at Semaphore(2)
4. Document empirical findings for rate limit specs

**Rationale:** Each 429 error requires exponential backoff (up to 32s delay), dramatically increasing total upload time. Configuration that prevents 429s is superior to one that recovers from them.

**Open questions:**
- What is Gemini File Search rate limit (RPM)? (Not documented, needs empirical testing)
- Does API return quota information in response headers?
- Should we implement adaptive throttling based on remaining quota?

**Confidence:** ‚úÖ Both providers agreed on conservative start + empirical tuning

---

### ‚úÖ 4. State Management for Mixed Processing (Consensus)

**What needs to be decided:**
How SQLite tracks lifecycle of files through Phase 6 ‚Üí Phase 6.1 ‚Üí Phase 6.2 pipeline to prevent re-uploads and handle continuous processing (new files being extracted while upload runs).

**Why it's ambiguous:**
Need to differentiate: (1) extracted but not uploaded, (2) uploaded successfully, (3) uploaded but failed indexing (Gemini async processing). Without clear state tracking, files may be skipped or uploaded twice.

**Provider synthesis:**
- **Gemini:** Proposes adding columns: `upload_status` (PENDING/UPLOADED/FAILED), `gemini_file_id`, `gemini_uri`, `last_upload_hash`.
- **Perplexity:** Similar proposal with explicit state machine, plus checkpointing (save progress every 50 uploads) for crash recovery.

**Proposed implementation decision:**
Extend **files table with upload tracking**:

**Schema Extensions:**
```sql
ALTER TABLE files ADD COLUMN upload_status TEXT DEFAULT 'pending'
  CHECK(upload_status IN ('pending', 'uploading', 'uploaded', 'failed'));
ALTER TABLE files ADD COLUMN gemini_file_id TEXT;
ALTER TABLE files ADD COLUMN gemini_uri TEXT;
ALTER TABLE files ADD COLUMN upload_timestamp TEXT;
ALTER TABLE files ADD COLUMN upload_error TEXT;
ALTER TABLE files ADD COLUMN upload_attempt_count INTEGER DEFAULT 0;
```

**State Transitions:**
- `pending` ‚Üí `uploading` (before API call)
- `uploading` ‚Üí `uploaded` (on success, store file_id)
- `uploading` ‚Üí `failed` (on error, store error message)
- `failed` ‚Üí `uploading` (retry with incremented attempt count)

**Pipeline Query:**
```sql
SELECT * FROM files
WHERE phase_6_complete = true
  AND entities_extracted = true
  AND upload_status = 'pending'
ORDER BY file_path
```

**Idempotency:** Track `last_upload_hash` (hash of metadata+content). Skip if hash matches (already uploaded with same data).

**Checkpointing:** Save state every 50 successful uploads (atomic transaction).

**Rationale:** Clear state machine enables crash recovery, prevents duplicate uploads, supports continuous processing.

**Open questions:**
- How to handle files deleted from local disk but still in Gemini? (Out of scope for Phase 6.2)
- Should we track Gemini async indexing completion? (Proposal: Yes, poll operation status)

**Confidence:** ‚úÖ Both providers agreed on state tracking approach

---

### ‚úÖ 5. Testing Strategy & Cost Control (Consensus)

**What needs to be decided:**
How to validate metadata flattening and upload pipeline before committing ~$4.20 to index all 1,749 files.

**Why it's ambiguous:**
If metadata schema is rejected by Gemini API after uploading 1,749 files, must delete and re-upload (doubling cost). Need high-confidence validation before full commitment.

**Provider synthesis:**
- **Gemini:** Proposes "Golden Set" of 10 diverse files (1 book, 1 course, 1 QA, etc.) for initial upload, then verification script to test metadata filtering.
- **Perplexity:** Comprehensive multi-stage testing: Stage 1 (20 docs, metadata validation), Stage 2 (100 docs, semantic quality), Stage 3 (250 docs, upload pipeline). Total testing cost ~$0.06 (1.3% of full deployment).

**Proposed implementation decision:**
Implement **three-stage validation before full upload**:

**Stage 1: Metadata Schema Validation (Cost: <$0.01)**
- Upload 10-20 representative documents with complete 4-tier metadata
- Test queries:
  - `category = "course"`
  - `category = "course" AND difficulty = "introductory"`
  - `topics = "epistemology"`
  - `entities = "Leonard Peikoff"`
- Validate: No schema errors, all filters work correctly
- Success criteria: 100% of test queries return expected results

**Stage 2: Semantic Search Quality (Cost: <$0.05)**
- Upload 50-100 documents from different categories
- Run semantic queries: "epistemological approaches", "Kant's influence"
- Measure precision@10 (fraction of top-10 results that are relevant)
- Success criteria: Precision@10 ‚â• 0.7 for representative queries

**Stage 3: Upload Pipeline Validation (Cost: $0.10-0.20)**
- Upload 200-250 documents (15% of corpus) with full pipeline
- Test concurrency, rate limiting, error recovery, state tracking
- Inject failures (network interrupt) to test resumption
- Success criteria: ‚â§5% 429 errors, all eventually succeed with retry

**Decision Gates:**
- Proceed to next stage ONLY if previous stage passes all criteria
- Total testing cost: ~$0.06 (1.3% of full $4.20 deployment)
- If any stage fails: investigate, fix, re-test

**Rationale:** $0.06 insurance policy against discovering architectural problems after $4.20 commitment.

**Open questions:**
- What precision@10 threshold is acceptable? (Proposal: ‚â•0.7)
- Should we test with real queries from anticipated use cases?
- How to validate Gemini async indexing completion?

**Confidence:** ‚úÖ Both providers agreed on staged testing approach

---

## Summary: Decision Checklist

Before planning, confirm:

**Tier 1 (Blocking - Must Answer):**
- [ ] **Metadata Flattening** - Split strategy (filtering in metadata, content in file text)?
- [ ] **Entity Dependency** - Strict gate (wait for Phase 6.1) or stratified upload?
- [ ] **Concurrency Tuning** - Start with Semaphore(2) or Semaphore(3)?
- [ ] **State Management** - Extend files table with upload_status columns?
- [ ] **Testing Strategy** - Three-stage validation before full upload?

**Tier 2 (Important - Should Answer):**
- [ ] **Tier 4 Handling** - Prepend full summary to file text or extract key terms only?
- [ ] **Rate Limit Discovery** - Empirical testing or contact Google support?
- [ ] **Checkpointing** - Save state every 50 uploads or different frequency?
- [ ] **Retry Policy** - Max 3 attempts with exponential backoff?
- [ ] **Async Indexing** - Poll operation status or assume success after upload?

**Tier 3 (Polish - Can Defer):**
- [ ] **Adaptive Throttling** - Implement quota monitoring if headers available?
- [ ] **Filesize Throttling** - Reduce concurrency for >10MB files?
- [ ] **Deletion Handling** - Clean up orphaned Gemini files?

---

## Recommended Next Step (to unblock planning)

Approve decisions for:
1. **Metadata Flattening Strategy** (Split: filtering fields in metadata, content prepended to text)
2. **Entity Dependency** (Strict gate: wait for Phase 6.1 before upload)
3. **Concurrency Configuration** (Conservative: Semaphore(2), exponential backoff, empirical tuning)
4. **Testing Approach** (Three-stage: schema ‚Üí quality ‚Üí pipeline, $0.06 total)

Those four determine the data flow, pipeline architecture, and validation strategy. Without them, planning will stall or require rework.

---

## Next Steps

**YOLO Mode (current):**
1. Auto-generate CLARIFICATIONS-ANSWERED.md with balanced strategy
2. Proceed to planning when user ready

**Manual Mode:**
1. Review this CONTEXT.md
2. Answer questions in CLARIFICATIONS-NEEDED.md
3. Create CLARIFICATIONS-ANSWERED.md with your decisions
4. Run `/gsd:plan-phase 6.2`

---

*Multi-provider synthesis by: Gemini Pro, Perplexity Deep Research*
*Generated: 2026-02-16*
*OpenAI provider available but not queried (used previous providers)*
