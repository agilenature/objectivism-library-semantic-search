---
phase: 06.2-metadata-enriched-gemini-upload
plan: 02
type: execute
wave: 2
depends_on: ["06.2-01"]
files_modified:
  - src/objlib/upload/orchestrator.py
  - src/objlib/cli.py
autonomous: false

must_haves:
  truths:
    - "User can run `python -m objlib enriched-upload --store <name>` to upload files with enriched metadata through the existing pipeline"
    - "Enriched upload uses build_enriched_metadata for CustomMetadata with string_list_value fields and prepares content with Tier 4 injection"
    - "Already-uploaded files (17) are identified, deleted from Gemini, reset to pending, and re-uploaded with enriched metadata"
    - "Pipeline enforces strict entity gate -- zero files upload without both AI metadata and entity extraction complete"
    - "Upload idempotency via last_upload_hash prevents duplicate uploads when re-running"
  artifacts:
    - path: "src/objlib/cli.py"
      provides: "enriched-upload CLI command"
      contains: "enriched_upload"
    - path: "src/objlib/upload/orchestrator.py"
      provides: "EnrichedUploadOrchestrator or modified _upload_single_file"
      contains: "enriched"
  key_links:
    - from: "src/objlib/cli.py"
      to: "src/objlib/upload/orchestrator.py"
      via: "enriched_upload command creates and runs orchestrator with enriched pipeline"
      pattern: "enriched"
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/metadata_builder.py"
      via: "_upload_single_file calls build_enriched_metadata instead of build_custom_metadata"
      pattern: "build_enriched_metadata"
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/content_preparer.py"
      via: "_upload_single_file calls prepare_enriched_content and cleanup_temp_file"
      pattern: "prepare_enriched_content"
    - from: "src/objlib/upload/state.py"
      to: "src/objlib/upload/orchestrator.py"
      via: "get_enriched_pending_files replaces get_pending_files in enriched run()"
      pattern: "get_enriched_pending_files"
---

<objective>
Wire the enriched metadata components from Plan 01 into a working CLI command that uploads files with 4-tier AI metadata, entity mentions, and content injection through the existing upload pipeline infrastructure.

Purpose: This completes Phase 6.2 by connecting the data transformation layer (metadata builder, content preparer, enriched query) to the proven upload orchestrator (circuit breaker, rate limiter, progress tracking, crash recovery). The user gets a single `enriched-upload` command that handles the full enriched upload lifecycle.

Output: A working `enriched-upload` CLI command. A checkpoint for the user to validate the three-stage testing approach (20 docs -> 100 -> 250) before committing to full deployment.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06.2-metadata-enriched-gemini-upload/06.2-RESEARCH.md
@.planning/phases/06.2-metadata-enriched-gemini-upload/06.2-01-SUMMARY.md

@src/objlib/upload/client.py
@src/objlib/upload/orchestrator.py
@src/objlib/upload/state.py
@src/objlib/upload/metadata_builder.py
@src/objlib/upload/content_preparer.py
@src/objlib/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enriched upload orchestrator and CLI command</name>
  <files>
    src/objlib/upload/orchestrator.py
    src/objlib/cli.py
  </files>
  <action>
**Create `EnrichedUploadOrchestrator`** in `src/objlib/upload/orchestrator.py`:

Subclass `UploadOrchestrator` to override the enriched upload flow. Key overrides:

1. **Override `run()`** (or create `run_enriched()`): Same as parent but:
   - Uses `self._state.get_enriched_pending_files(limit=10000)` instead of `get_pending_files()`
   - Before main loop, check for files needing reset (already uploaded/failed with enriched metadata available). Call `self._state.get_files_to_reset_for_enriched_upload()`. For each:
     - Delete from Gemini via `self._client.delete_file(gemini_file_id)` (if gemini_file_id exists)
     - Reset status to 'pending' in DB: `UPDATE files SET status='pending', error_message=NULL, upload_attempt_count=0 WHERE file_path=?`
   - Add a `--reset-existing` flag (default True) to control this behavior.

2. **Override `_upload_single_file()`**: Same structure as parent but:
   - Parse `phase1_metadata_json` AND `ai_metadata_json` from `file_info` dict (from enriched query)
   - Call `build_enriched_metadata(phase1_metadata, ai_metadata, entity_names)` instead of `self._client.build_custom_metadata(metadata)`
   - Call `prepare_enriched_content(file_path, ai_metadata)` to get temp file path
   - If temp file is not None, use it as `upload_path`; otherwise use original `file_path`
   - In `try/finally`: call `cleanup_temp_file(temp_path)` to always clean up
   - Compute upload hash via `compute_upload_hash(phase1_metadata, ai_metadata, entity_names, content_hash)` and compare with `file_info.get("last_upload_hash")`. If matching, skip upload (idempotent).
   - After successful upload, update `last_upload_hash` and increment `upload_attempt_count` in the files table.

3. **Concurrency per locked decision**: Default `Semaphore(2)` (not 7 like existing). Add 1-second delay between task launches using `asyncio.sleep(1.0)` before semaphore acquire in the batch loop (stagger launches to prevent burst).

Import `build_enriched_metadata`, `compute_upload_hash`, `prepare_enriched_content`, `cleanup_temp_file` at the top of orchestrator.py.

**Create CLI command** `enriched_upload` in `src/objlib/cli.py`:

Add `@app.command("enriched-upload")` with parameters:
- `--store` / `-s`: store display name (default: "objectivism-library-test")
- `--db` / `-d`: database path (default: "data/library.db")
- `--batch-size` / `-b`: files per batch (default: 100)
- `--concurrency` / `-n`: max concurrent uploads (default: 2, per locked decision)
- `--dry-run`: show what would be uploaded
- `--limit`: max files to upload (default: 0 = no limit). Useful for staged testing: `--limit 20` for Stage 1, `--limit 100` for Stage 2.
- `--include-needs-review`: include low-confidence files (default: True)
- `--reset-existing`: delete and re-upload already-uploaded files (default: True)

Command implementation:
1. Validate database exists
2. Deferred imports (for fast CLI startup): `from objlib.upload.orchestrator import EnrichedUploadOrchestrator`, etc.
3. Get API key from keyring (`get_api_key_from_keyring()` from config.py)
4. Dry-run mode: call `get_enriched_pending_files()` and display count, first 20 files in Rich table with AI metadata status
5. Full mode: create circuit breaker, rate limiter, client, state manager, progress tracker, and enriched orchestrator. Call `orchestrator.run_enriched(store_name)`.
6. Display Rich summary panel with succeeded/failed/skipped counts.

**Key details per locked decisions:**
- Concurrency default: 2 (Semaphore(2))
- Rate limiter tier: tier1 (20 RPM)
- Store name default: "objectivism-library-test" (from MEMORY.md)
- API key: system keyring (objlib-gemini service)
  </action>
  <verify>
Run: `python -m objlib enriched-upload --help` -- verify command exists with expected options

Run: `python -m objlib enriched-upload --db data/library.db --dry-run` -- verify it reports enriched pending file count (likely 0 since entity extraction hasn't run, which is expected and correct)

Run: `python -c "from objlib.upload.orchestrator import EnrichedUploadOrchestrator; print('import OK')"` -- verify class exists
  </verify>
  <done>
EnrichedUploadOrchestrator extends UploadOrchestrator with enriched metadata building, content injection, temp file cleanup, upload hash idempotency, and conservative Semaphore(2) concurrency. CLI enriched-upload command provides --limit for staged testing, --dry-run for preview, and --reset-existing for re-uploading the 17 already-uploaded files.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete enriched upload pipeline: schema v5 migration, metadata builder with string_list_value support, content preparer with Tier 4 injection, enriched pipeline query with strict entity gate, EnrichedUploadOrchestrator, and CLI enriched-upload command.

IMPORTANT: Before testing uploads, Phase 6.1 entity extraction must be run first:
```bash
python -m objlib entities extract
```
This populates the transcript_entity table, which is required by the enriched pipeline's strict entity gate.
  </what-built>
  <how-to-verify>
**Pre-flight check (entity gate):**
1. Run `python -m objlib entities stats` -- verify entity extraction has been run on at least some files
2. Run `python -m objlib enriched-upload --dry-run` -- verify it reports files ready for enriched upload

**Stage 1: Metadata Schema Validation (20 docs, cost <$0.01):**
1. Run: `python -m objlib enriched-upload --store objectivism-library-test --limit 20`
2. Verify: All 20 uploads succeed (no INVALID_ARGUMENT errors from string_list_value)
3. Test metadata filtering with Gemini:
   - `python -m objlib --store objectivism-library-test search "epistemology" --filter 'category="course_transcript"'`
   - Verify results return with enriched metadata visible

**Stage 2: Semantic Search Quality (100 docs, cost <$0.05):**
1. Run: `python -m objlib enriched-upload --store objectivism-library-test --limit 100`
2. Run semantic queries and verify relevance:
   - `python -m objlib --store objectivism-library-test search "What is Ayn Rand's view on concept formation?"`
   - Verify results include files about ITOE / concept formation

**Stage 3: Pipeline Validation (250 docs, cost $0.10-$0.20):**
1. Run: `python -m objlib enriched-upload --store objectivism-library-test --limit 250`
2. Verify: All succeed, no 429 errors, progress bars display correctly
3. Verify idempotency: re-run the same command, expect zero new uploads (all skipped via last_upload_hash)

After all stages pass, approve to proceed with full upload.
  </how-to-verify>
  <resume-signal>Type "approved" to proceed with full deployment, or describe issues found during testing</resume-signal>
</task>

</tasks>

<verification>
1. `python -m objlib enriched-upload --help` shows command with --store, --limit, --dry-run, --concurrency options
2. `python -m objlib enriched-upload --dry-run` reports enriched pending files (0 if entities not extracted)
3. After entity extraction: `python -m objlib enriched-upload --limit 20` uploads 20 files with enriched metadata
4. Re-running `--limit 20` uploads zero files (idempotency via last_upload_hash)
5. Uploaded files have string_list_value metadata (topics, aspects, entities, key_themes)
6. Uploaded files have [AI Analysis] header prepended to content
7. Already-uploaded files (17) are detected, deleted from Gemini, and re-uploaded with enriched metadata
</verification>

<success_criteria>
The enriched-upload command successfully uploads files with 4-tier AI metadata and entity mentions to Gemini File Search. Metadata includes string_list_value fields filterable with the `:` operator. Content includes prepended Tier 4 analysis for embedding capture. The strict entity gate prevents uploads without complete metadata. Three-stage testing validates the pipeline before full deployment commitment.
</success_criteria>

<output>
After completion, create `.planning/phases/06.2-metadata-enriched-gemini-upload/06.2-02-SUMMARY.md`
</output>
