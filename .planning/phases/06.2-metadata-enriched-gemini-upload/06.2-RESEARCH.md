# Phase 6.2: Metadata-Enriched Gemini Upload - Research

**Researched:** 2026-02-17
**Domain:** Gemini File Search enriched upload with 4-tier AI metadata, entity mentions, content injection, and concurrent async pipeline
**Confidence:** HIGH (API surface verified via Context7 + official docs; SDK `string_list_value` confirmed locally; existing upload pipeline code fully audited)

## Summary

Phase 6.2 transforms the existing Phase 2 upload pipeline to carry enriched 4-tier AI metadata (from Phase 6) and entity mentions (from Phase 6.1) into Gemini File Search custom_metadata, while prepending Tier 4 semantic descriptions to file content for embedding capture. The primary technical challenge is extending the existing `GeminiFileSearchClient.build_custom_metadata()` method to handle the new field types -- particularly `string_list_value` for topics, aspects, entities, and key_themes -- and building a content injection layer that prepends AI analysis to file text before upload.

A critical discovery during this research: the Gemini `CustomMetadata` type natively supports `string_list_value` (confirmed via SDK introspection: `types.CustomMetadataDict` has `string_list_value: Optional[StringListDict]`). This means topics, aspects, entities, and key_themes can be stored as proper list fields, filterable via AIP-160's `:` (has) operator (e.g., `topics:"epistemology"`). The API documentation confirms a maximum of 20 `CustomMetadata` entries per document, well above the planned 7-9 fields.

The existing upload pipeline in `src/objlib/upload/` is well-architected with separation of concerns (client, orchestrator, state, circuit_breaker, rate_limiter, recovery, progress). The main code changes involve: (1) a new `build_enriched_metadata()` method on the client, (2) a content injection function that prepends Tier 4 analysis to file text, (3) a modified `get_pending_files()` query that joins with `file_metadata_ai` and `transcript_entity` tables, and (4) an enriched upload orchestrator or new CLI command that combines these components.

**Primary recommendation:** Extend the existing `GeminiFileSearchClient.build_custom_metadata()` to accept 4-tier AI metadata and entity data, using `string_list_value` for list fields. Create a new content preparation layer that reads file text, prepends Tier 4 analysis, and writes to a temporary file for upload. Reuse the existing orchestrator, circuit breaker, rate limiter, and state management infrastructure -- do not rebuild these.

<user_constraints>

## User Constraints (from CONTEXT.md)

### Locked Decisions

1. **Metadata Flattening Strategy (Consensus):** Split strategy -- filtering fields (category, difficulty, topics, aspects, entities, key_themes, source_type) in Gemini `custom_metadata`, Tier 4 long-form content (summary, argument structure) prepended to file text during upload.

2. **Entity Dependency (Consensus):** Strict gate -- files cannot upload until BOTH Phase 6 AI metadata AND Phase 6.1 entity extraction are complete. No re-uploads, no bypass. Pipeline filter: `WHERE ai_metadata_status IN ('extracted','approved') AND entity_extraction_status = 'entities_done' AND upload_status = 'pending'`.

3. **Concurrency Configuration (Consensus):** Conservative start with `Semaphore(2)`, 1-second launch delay, exponential backoff with jitter on 429 (2s -> 4s -> 8s -> 16s -> 32s), max 3 retries. Empirical tuning: 50-file test batch, increase to Semaphore(3) if zero 429s.

4. **State Management (Consensus):** Extend files table with upload tracking columns (upload_status, upload_error, upload_attempt_count, last_upload_hash). State machine: pending -> uploading -> uploaded/failed. Checkpoint every 50 uploads.

5. **Testing Strategy (Consensus):** Three-stage validation before full deployment:
   - Stage 1: 20 docs, metadata schema validation (<$0.01)
   - Stage 2: 100 docs, semantic search quality (<$0.05)
   - Stage 3: 250 docs, pipeline validation ($0.10-$0.20)

### Claude's Discretion

- How to structure the content injection (temp file vs in-memory)
- Whether to create a new CLI command or extend the existing `upload` command
- Specific `key_themes` extraction approach from Tier 4 data
- How to handle the 17 already-uploaded files (delete + re-upload vs skip)
- Content injection format details (delimiter style, header format)

### Deferred Ideas (OUT OF SCOPE)

- Adaptive throttling based on response headers (not for initial implementation)
- Filesize-based throttling (reduce concurrency for >10MB files)
- Deletion handling for orphaned Gemini files (Phase 5 concern)
- Multi-process distributed upload
- Web dashboard for progress monitoring

</user_constraints>

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| google-genai | 1.63.0+ | Gemini File Search API (files.upload, file_search_stores.import_file, CustomMetadata with string_list_value) | Already in project; SDK confirms string_list_value support |
| aiosqlite | 0.22.1 | Async SQLite for upload state management | Already in project; used by existing AsyncUploadStateManager |
| tenacity | 9.1.4 | Exponential backoff retry for API calls | Already in project; used by existing upload client |
| rich | 13.x | Progress bars and console output | Already in project; used by existing UploadProgressTracker |
| asyncio (stdlib) | 3.12+ | Semaphore-limited concurrency, event loop | Already in project; used by existing orchestrator |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| pydantic | 2.x | ExtractedMetadata validation, CustomMetadata type construction | Already in project; schemas.py uses it for 4-tier metadata |
| hashlib (stdlib) | 3.12+ | Content+metadata hash for idempotency (last_upload_hash) | Detect whether a file needs re-upload after metadata changes |
| tempfile (stdlib) | 3.12+ | Temporary files with prepended content | Content injection: prepend AI analysis to file text |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Temp files for content injection | In-memory bytes upload | Temp files avoid memory pressure for 3.4MB files; SDK accepts file paths natively |
| New enriched upload command | Extend existing `upload` command | New command is cleaner; existing `upload` can remain for basic uploads |
| Rebuilding orchestrator | Extending existing UploadOrchestrator | Extending is better -- the orchestrator, circuit breaker, rate limiter, and recovery are proven |

**Installation:**
No new dependencies needed. All libraries are already in the project from Phase 2.

## Architecture Patterns

### Recommended Project Structure
```
src/objlib/
    upload/
        __init__.py           # existing (update exports)
        client.py             # MODIFY: add build_enriched_metadata() method
        orchestrator.py       # existing (reuse as-is or minor extension)
        circuit_breaker.py    # existing (reuse as-is)
        rate_limiter.py       # existing (reuse as-is)
        state.py              # MODIFY: update get_pending_files() query for enriched pipeline
        progress.py           # existing (reuse as-is)
        recovery.py           # existing (reuse as-is)
        content_preparer.py   # NEW: content injection (prepend Tier 4 to file text)
        metadata_builder.py   # NEW: assemble enriched CustomMetadata from AI + entity data
    cli.py                    # MODIFY: add enriched-upload command or extend upload
    database.py               # MODIFY: schema migration v5, new query methods
```

### Pattern 1: Enriched CustomMetadata with string_list_value
**What:** Build Gemini CustomMetadata using all three value types: `string_value`, `numeric_value`, and `string_list_value`.
**When to use:** Every file uploaded in Phase 6.2.
**Verified via:** Local SDK introspection confirming `types.CustomMetadataDict` supports `string_list_value: Optional[StringListDict]`.

```python
# Source: Context7 /googleapis/python-genai CustomMetadata type definition
# Verified locally: types.CustomMetadataDict has string_list_value field

def build_enriched_metadata(
    phase1_metadata: dict,           # From files.metadata_json
    ai_metadata: dict,               # From file_metadata_ai.metadata_json
    entity_names: list[str],         # From transcript_entity JOIN person
) -> list[dict]:
    """Build enriched CustomMetadata for Gemini import_file.

    Uses 7-9 metadata fields (max 20 allowed per document):
    - category (string_value) -- from AI extraction Tier 1
    - difficulty (string_value) -- from AI extraction Tier 1
    - topics (string_list_value) -- from AI extraction Tier 2 (3-8 tags)
    - aspects (string_list_value) -- from AI extraction Tier 3 (3-10 phrases)
    - entities (string_list_value) -- from Phase 6.1 entity extraction
    - key_themes (string_list_value) -- extracted from Tier 4 key_arguments
    - source_type (string_value) -- always "objectivism_library"
    - course (string_value) -- from Phase 1 metadata (if category=course)
    - quality_score (numeric_value) -- from AI confidence score
    """
    result = []

    # Tier 1: category and difficulty (string_value)
    if ai_metadata.get("category"):
        result.append({"key": "category", "string_value": ai_metadata["category"]})
    if ai_metadata.get("difficulty"):
        result.append({"key": "difficulty", "string_value": ai_metadata["difficulty"]})

    # Tier 2: primary_topics (string_list_value)
    topics = ai_metadata.get("primary_topics", [])
    if topics:
        result.append({
            "key": "topics",
            "string_list_value": {"values": topics}
        })

    # Tier 3: topic_aspects (string_list_value)
    aspects = ai_metadata.get("topic_aspects", [])
    if aspects:
        result.append({
            "key": "aspects",
            "string_list_value": {"values": aspects[:10]}  # Max 10
        })

    # Phase 6.1: entity canonical names (string_list_value)
    if entity_names:
        result.append({
            "key": "entities",
            "string_list_value": {"values": entity_names}
        })

    # Tier 4: key themes from key_arguments (string_list_value)
    semantic = ai_metadata.get("semantic_description", {})
    key_args = semantic.get("key_arguments", [])
    if key_args:
        # Use first 5 key arguments as themes (truncated to 200 chars each)
        themes = [arg[:200] for arg in key_args[:5]]
        result.append({
            "key": "key_themes",
            "string_list_value": {"values": themes}
        })

    # Static: source_type
    result.append({"key": "source_type", "string_value": "objectivism_library"})

    # Phase 1 metadata: course name (if applicable)
    course = phase1_metadata.get("course")
    if course:
        result.append({"key": "course", "string_value": str(course)})

    # AI confidence score as numeric
    confidence = ai_metadata.get("confidence_score")
    if confidence is not None:
        result.append({"key": "quality_score", "numeric_value": float(confidence)})

    return result
```

### Pattern 2: Content Injection (Prepend Tier 4 to File Text)
**What:** Create a temporary file with AI analysis prepended to the original text, upload the temp file.
**When to use:** Every enriched upload -- ensures embeddings capture both AI analysis and original content.
**Why:** Gemini File Search creates embeddings from the uploaded file content. Prepending AI summaries and argument structures means the vector index captures philosophical context in addition to the raw transcript.

```python
import tempfile
from pathlib import Path

def prepare_enriched_content(
    original_file_path: str,
    ai_metadata: dict,
) -> str:
    """Create a temporary file with AI analysis prepended to original text.

    Returns the path to the temporary file (caller must clean up).

    Format:
    [AI Analysis]
    Category: {category} | Difficulty: {difficulty}

    Summary: {tier4_summary}

    Key Arguments:
    - {argument_1}
    - {argument_2}

    Philosophical Positions:
    - {position_1}

    [Original Content]
    {file_text}
    """
    semantic = ai_metadata.get("semantic_description", {})
    summary = semantic.get("summary", "")
    key_arguments = semantic.get("key_arguments", [])
    positions = semantic.get("philosophical_positions", [])

    # Build header
    header_parts = ["[AI Analysis]"]
    header_parts.append(
        f"Category: {ai_metadata.get('category', 'unknown')} | "
        f"Difficulty: {ai_metadata.get('difficulty', 'unknown')}"
    )
    header_parts.append("")

    if summary:
        header_parts.append(f"Summary: {summary}")
        header_parts.append("")

    if key_arguments:
        header_parts.append("Key Arguments:")
        for arg in key_arguments:
            header_parts.append(f"- {arg}")
        header_parts.append("")

    if positions:
        header_parts.append("Philosophical Positions:")
        for pos in positions:
            header_parts.append(f"- {pos}")
        header_parts.append("")

    header_parts.append("[Original Content]")
    header = "\n".join(header_parts) + "\n"

    # Read original file and create temp file with prepended content
    original_text = Path(original_file_path).read_text(encoding="utf-8")

    tmp = tempfile.NamedTemporaryFile(
        mode="w", suffix=".txt", delete=False, encoding="utf-8"
    )
    tmp.write(header)
    tmp.write(original_text)
    tmp.close()

    return tmp.name
```

### Pattern 3: Enriched Pipeline Query (Join AI Metadata + Entities)
**What:** Query files that have BOTH Phase 6 AI metadata AND Phase 6.1 entity extraction complete.
**When to use:** Replacing the existing `get_pending_files()` for the enriched upload pipeline.
**Critical:** This enforces the strict dependency gate -- no file uploads without complete metadata.

```python
# Modified get_pending_files for enriched upload
async def get_enriched_pending_files(self, limit: int = 200) -> list[dict]:
    """Return files ready for enriched upload.

    Requires:
    - .txt file
    - AI metadata extracted (file_metadata_ai.is_current=1)
    - Entity extraction complete (entity_extraction_status='entities_done')
    - Not already uploaded (status='pending')
    """
    db = self._ensure_connected()
    cursor = await db.execute(
        """SELECT f.file_path, f.content_hash, f.filename, f.file_size,
                  f.metadata_json,
                  m.metadata_json as ai_metadata_json
           FROM files f
           JOIN file_metadata_ai m
               ON f.file_path = m.file_path AND m.is_current = 1
           WHERE f.filename LIKE '%.txt'
             AND f.entity_extraction_status = 'entities_done'
             AND f.status = 'pending'
           ORDER BY f.file_path
           LIMIT ?""",
        (limit,),
    )
    rows = await cursor.fetchall()

    # For each file, also fetch entity names
    results = []
    for row in rows:
        file_dict = dict(row)
        entity_cursor = await db.execute(
            """SELECT p.canonical_name
               FROM transcript_entity te
               JOIN person p ON te.person_id = p.person_id
               WHERE te.transcript_id = ?
               ORDER BY te.mention_count DESC""",
            (row["file_path"],),
        )
        entity_rows = await entity_cursor.fetchall()
        file_dict["entity_names"] = [r["canonical_name"] for r in entity_rows]
        results.append(file_dict)

    return results
```

### Pattern 4: AIP-160 Filter Syntax for List Fields
**What:** Use the `:` (has) operator to filter on `string_list_value` metadata fields.
**When to use:** Phase 3 search queries with enriched metadata.
**Source:** AIP-160 specification (google.aip.dev/160): "r:42 means True if r contains 42".

```python
# Filtering on string_list_value fields uses the ":" (has) operator
# This is the AIP-160 "has" operator for repeated/list fields

# Filter: files about epistemology
metadata_filter = 'topics:"epistemology"'

# Filter: files mentioning Leonard Peikoff
metadata_filter = 'entities:"Leonard Peikoff"'

# Combined: epistemology courses by Peikoff
metadata_filter = 'topics:"epistemology" AND entities:"Leonard Peikoff" AND category="course_transcript"'

# Difficulty filter (string_value uses = operator)
metadata_filter = 'difficulty="introductory" AND topics:"ethics"'

# NOTE: The ":" operator checks if a list CONTAINS the value
# The "=" operator checks exact equality on string_value fields
```

### Anti-Patterns to Avoid
- **Comma-separated strings instead of string_list_value:** Do NOT store topics as `"string_value": "epistemology,metaphysics"`. Use proper `string_list_value` with `{"values": ["epistemology", "metaphysics"]}`. The `:` operator only works with proper list fields.
- **Storing Tier 4 content in metadata:** Long-form text (summaries, argument structures) does NOT belong in `custom_metadata`. It bloats metadata without improving filtering. Prepend to file content instead for embedding capture.
- **Rebuilding the upload orchestrator:** The existing `UploadOrchestrator` handles batching, semaphore concurrency, signal handling, circuit breaker integration, and crash recovery. Extend it; do not rewrite it.
- **Uploading without entity gate:** Uploading a file that has AI metadata but no entity extraction defeats the purpose of Phase 6.2 and may require expensive re-upload later.
- **Modifying uploaded files in place:** Gemini File Search does not support metadata patching. To update metadata, you must delete the file from the store and re-upload. This costs ~$0.0024/file for indexing. Plan metadata carefully before upload.
- **Ignoring the 20-metadata-key limit:** Gemini allows max 20 CustomMetadata entries per document. The planned 7-9 fields are well within this limit, but do not expand significantly.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Concurrent upload with backoff | New async upload engine | Existing `UploadOrchestrator` + `GeminiFileSearchClient` | Proven code with circuit breaker, signal handling, crash recovery already built |
| Rate limiting | New throttling logic | Existing `AdaptiveRateLimiter` + `RollingWindowCircuitBreaker` | Already handles tier-based throttling and 429 backoff |
| Progress tracking | New progress bars | Existing `UploadProgressTracker` | Three-tier Rich progress already built |
| State management | New database layer | Existing `AsyncUploadStateManager` | Write-ahead intent logging, batch tracking, locks already built |
| Crash recovery | New recovery protocol | Existing `RecoveryManager` | Handles interrupted uploads, pending operations, expiration deadlines |
| Retry logic | Manual retry loops | tenacity `AsyncRetrying` | Already used throughout upload client |

**Key insight:** Phase 6.2 is primarily a **data transformation** phase, not an **infrastructure** phase. The upload infrastructure is already built (Phase 2). The new work is: (1) assembling enriched metadata from multiple database sources, (2) preparing content with injected AI analysis, and (3) feeding this enriched data through the existing pipeline. Do not rebuild the pipeline.

## Common Pitfalls

### Pitfall 1: string_list_value Format Mismatch
**What goes wrong:** Passing `string_list_value` as a plain list `["a", "b"]` instead of the required `{"values": ["a", "b"]}` format, causing API errors.
**Why it happens:** The API documentation shows `CustomMetadata` with `string_list_value` as type `StringList`, which has a `values` array wrapper. The Python SDK expects `StringListDict` format: `{"values": [...]}`.
**How to avoid:** Always wrap lists in `{"values": [...]}`. Verified locally:
```python
# CORRECT
{"key": "topics", "string_list_value": {"values": ["epistemology", "ethics"]}}

# WRONG -- will fail
{"key": "topics", "string_list_value": ["epistemology", "ethics"]}
```
**Warning signs:** `INVALID_ARGUMENT` errors from `import_file()` when using list metadata.

### Pitfall 2: Entity Extraction Not Run Yet
**What goes wrong:** Phase 6.2 query finds zero files ready for upload because entity extraction hasn't been run on any files.
**Why it happens:** Current database state shows `entity_extraction_status = 'pending'` for all 1,884 files. Phase 6.1 code is built but has not been executed against the corpus.
**How to avoid:** Run Phase 6.1 entity extraction before Phase 6.2 upload. The pipeline query joins on `entity_extraction_status = 'entities_done'`, so zero entities = zero uploads. The execution order is: Phase 6.1 extract -> Phase 6.2 upload.
**Warning signs:** `get_enriched_pending_files()` returns empty list despite having 814 files with AI metadata.

### Pitfall 3: Temp File Cleanup on Failure
**What goes wrong:** Content injection creates temporary files (with prepended AI analysis) that are never cleaned up if the upload fails mid-batch.
**Why it happens:** If an exception occurs between creating the temp file and completing the upload, the temp file remains on disk. Over thousands of files, this accumulates.
**How to avoid:** Use `try/finally` with `os.unlink()` to clean up temp files regardless of upload success. Alternatively, use `tempfile.TemporaryDirectory()` per batch and let the context manager clean up.
**Warning signs:** `/tmp` fills with `.txt` files containing `[AI Analysis]` headers.

### Pitfall 4: Schema Migration Column Collision
**What goes wrong:** Adding `upload_status` column to `files` table fails because the table already has a `status` column that serves a similar purpose.
**Why it happens:** CLARIFICATIONS-ANSWERED.md proposes new `upload_status`, `gemini_file_id`, `gemini_uri`, `upload_timestamp` columns. But the existing schema ALREADY has `status` (with values 'pending'/'uploading'/'uploaded'/'failed'), `gemini_file_id`, `gemini_file_uri`, and `upload_timestamp`. The existing columns were created in Phase 1/2.
**How to avoid:** Do NOT add duplicate columns. The existing `status` column already tracks upload state. The additional columns needed are: `upload_error` (text, for error messages -- note: `error_message` already exists), `upload_attempt_count` (integer), and `last_upload_hash` (text, for idempotency). Audit the existing schema before applying any migration.
**Warning signs:** `sqlite3.OperationalError: duplicate column name` during migration.

### Pitfall 5: Content Size Inflation
**What goes wrong:** Prepending AI analysis to file content increases file size significantly, potentially affecting upload time and Gemini processing.
**Why it happens:** AI analysis adds 500-2000 chars of header text. For the average 63.6KB file this is negligible (~2%), but for the smallest files (2.6KB) it could double the content size.
**How to avoid:** This is acceptable overhead. Average AI metadata JSON is 1,798 chars; the prepended content is a subset of this. Total corpus size increase: ~1.5-3 MB on 108.5 MB corpus (~2%). No mitigation needed, but track enriched file sizes for validation.
**Warning signs:** Enriched files exceeding Gemini's 100MB per-file limit (not a realistic risk for this corpus).

### Pitfall 6: Metadata Filter Syntax for Existing Search Command
**What goes wrong:** After uploading with new metadata fields (topics, entities, aspects), the existing `search` command's `--filter` option doesn't know how to construct queries for list fields.
**Why it happens:** The existing search client passes `metadata_filter` as a string to Gemini. Users need to use the `:` operator for list fields (`topics:"epistemology"`) instead of `=`. The CLI filter command in Phase 3 uses SQLite queries, not Gemini metadata filters.
**How to avoid:** After Phase 6.2 upload, update the CLI search command's help text and documentation to show the new filter syntax. For Phase 6.2 scope: just document the filter syntax, don't modify the search command.
**Warning signs:** Users get zero results when filtering with `topics="epistemology"` (should be `topics:"epistemology"`).

### Pitfall 7: Already-Uploaded Files (17 files)
**What goes wrong:** The 17 already-uploaded files have Phase 1 metadata only (category, raw_filename, topic). They need to be re-uploaded with enriched metadata to be consistent with the rest of the corpus.
**Why it happens:** Phase 2 uploaded 17 files before Phase 6 was complete. These files have basic metadata in Gemini but not the enriched 4-tier metadata.
**How to avoid:** For the 17 uploaded files: delete from Gemini store, reset status to 'pending', then re-upload with enriched metadata. Cost: 17 * $0.0024 = ~$0.04. This ensures metadata consistency across the entire indexed corpus.
**Warning signs:** Search results showing inconsistent metadata quality (some files with topics/entities, others without).

## Code Examples

Verified patterns from official sources and existing codebase:

### CustomMetadata with string_list_value (Verified via SDK)
```python
# Source: Context7 /googleapis/python-genai CustomMetadata type
# Verified locally: python3 -c "from google.genai import types; print(types.CustomMetadataDict.__annotations__)"
# Output: {'key': Optional[str], 'numeric_value': Optional[float],
#          'string_list_value': Optional[StringListDict], 'string_value': Optional[str]}

# Using dict format (what the existing upload client passes to import_file):
metadata = [
    {"key": "category", "string_value": "course_transcript"},
    {"key": "difficulty", "string_value": "intermediate"},
    {"key": "topics", "string_list_value": {"values": ["epistemology", "concept_formation", "reason"]}},
    {"key": "aspects", "string_list_value": {"values": ["human flourishing framework", "measurement omission"]}},
    {"key": "entities", "string_list_value": {"values": ["Ayn Rand", "Leonard Peikoff"]}},
    {"key": "key_themes", "string_list_value": {"values": [
        "Fossil fuels provide unique, low-cost, reliable energy crucial for human flourishing.",
        "The anti-impact framework distorts understanding of fossil fuel benefits."
    ]}},
    {"key": "source_type", "string_value": "objectivism_library"},
    {"key": "course", "string_value": "OPAR"},
    {"key": "quality_score", "numeric_value": 0.85},
]
```

### Existing Upload Client import_to_store (No changes needed)
```python
# Source: src/objlib/upload/client.py lines 205-234
# The existing import_to_store method already accepts list[dict] metadata
# and passes it directly to the API. string_list_value dicts will work.

async def import_to_store(self, file_name: str, metadata: list[dict[str, Any]]) -> Any:
    await self._rate_limiter.wait_if_needed()
    operation = await self._safe_call(
        self._client.aio.file_search_stores.import_file,
        file_search_store_name=self.store_name,
        file_name=file_name,
        config={"custom_metadata": metadata},
    )
    return operation
```

### Existing Pipeline Flow (What to extend, not rebuild)
```python
# Source: src/objlib/upload/orchestrator.py lines 271-357
# The _upload_single_file method shows the flow:
# 1. Check shutdown + circuit breaker
# 2. Record intent in SQLite (BEFORE API call)
# 3. Parse metadata_json and build custom_metadata
# 4. Upload with semaphore-limited concurrency
# 5. Record success in SQLite (AFTER API response)

# For Phase 6.2, the modification points are:
# - Step 3: build_custom_metadata() -> build_enriched_metadata()
# - Between step 3 and 4: prepare_enriched_content() for content injection
# - The file_path passed to upload_and_import changes from original to temp file
# - After step 5: clean up temp file
```

### Database Query for Enriched Files
```python
# Join files + file_metadata_ai + transcript_entity to get enriched data
# This replaces the simple SELECT from files WHERE status='pending'

ENRICHED_PENDING_SQL = """
SELECT f.file_path, f.content_hash, f.filename, f.file_size,
       f.metadata_json as phase1_metadata_json,
       m.metadata_json as ai_metadata_json
FROM files f
JOIN file_metadata_ai m
    ON f.file_path = m.file_path AND m.is_current = 1
WHERE f.filename LIKE '%.txt'
  AND f.entity_extraction_status = 'entities_done'
  AND f.status = 'pending'
ORDER BY f.file_path
LIMIT ?
"""

# Entity names fetched separately per file:
ENTITY_NAMES_SQL = """
SELECT p.canonical_name
FROM transcript_entity te
JOIN person p ON te.person_id = p.person_id
WHERE te.transcript_id = ?
ORDER BY te.mention_count DESC
"""
```

## State of the Art

| Old Approach (Phase 2) | New Approach (Phase 6.2) | What Changed | Impact |
|------------------------|--------------------------|--------------|--------|
| `build_custom_metadata()` with 5-8 Phase 1 fields | `build_enriched_metadata()` with 7-9 fields including AI + entities | 4-tier AI metadata + entity mentions | Rich filtering on topics, difficulty, entities |
| `string_value` and `numeric_value` only | Add `string_list_value` for list fields | SDK supports it natively | Filter with `:` operator (e.g., `topics:"epistemology"`) |
| Upload raw file text | Prepend Tier 4 analysis to file text | Content injection | Embeddings capture AI summaries + arguments |
| Simple `WHERE status='pending'` query | JOIN with file_metadata_ai + check entity_extraction_status | Strict dependency gate | No uploads without complete metadata |
| Phase 1 metadata only (category, course, date) | Phase 1 + Phase 6 AI + Phase 6.1 entities | Full 4-tier enrichment | Philosophical context in both metadata and embeddings |

## Current Database State (as of 2026-02-17)

| Metric | Value | Implication |
|--------|-------|-------------|
| Total .txt files | 1,748 | Corpus size for full upload |
| Already uploaded (Phase 2) | 17 | Need delete + re-upload with enriched metadata |
| Pending (never uploaded) | 1,721 | Main upload workload |
| Failed | 10 | Reset to pending before enriched upload |
| Files with AI metadata (file_metadata_ai) | 814 | Phase 6 extraction partially complete |
| AI status: extracted | 678 | Ready for upload (with entities) |
| AI status: needs_review | 136 | Low-confidence, may include |
| AI status: pending | 1,024 | Phase 6 not yet run |
| AI status: skipped | 46 | Non-txt or too small |
| Entity extraction: done | 0 | Phase 6.1 NOT YET RUN -- blocking |
| Entity extraction: pending | 1,884 | All files need entity extraction |
| Avg file size | 63.6 KB | Small files, fast upload |
| Max file size | 3,394.5 KB (~3.3 MB) | Well under 100 MB limit |
| Total corpus size | 108.5 MB | Well under Tier 1's 10 GB limit |
| Avg AI metadata JSON | 1,798 chars | Moderate complexity |
| Max AI metadata JSON | 4,135 chars | Fits in metadata (under typical limits) |
| Avg Tier 4 summary length | 284 chars | Small prepend overhead |
| Max Tier 4 summary length | 1,293 chars | Acceptable prepend overhead |

**Critical blocker:** Entity extraction (Phase 6.1) has not been run against any files. The enriched upload pipeline query requires `entity_extraction_status = 'entities_done'`, which currently matches zero files. Phase 6.1 must be executed before Phase 6.2 can upload anything.

## Existing Schema Columns (Audit)

The `files` table ALREADY has these upload-related columns from Phase 1/2:
- `status TEXT` -- CHECK(status IN ('pending','uploading','uploaded','failed','skipped','LOCAL_DELETE'))
- `error_message TEXT`
- `gemini_file_uri TEXT`
- `gemini_file_id TEXT`
- `upload_timestamp TEXT`
- `remote_expiration_ts TEXT`
- `embedding_model_version TEXT`
- `ai_metadata_status TEXT` -- Added in migration v3
- `ai_confidence_score REAL` -- Added in migration v3
- `entity_extraction_status TEXT` -- Added in migration v4
- `entity_extraction_version TEXT` -- Added in migration v4

Columns that need to be ADDED (migration v5):
- `upload_attempt_count INTEGER DEFAULT 0` -- Track retry attempts
- `last_upload_hash TEXT` -- Hash of metadata+content for idempotency detection

Columns from CLARIFICATIONS-ANSWERED.md that ALREADY EXIST (do NOT re-add):
- `upload_status` -- USE EXISTING `status` column (same semantics)
- `gemini_file_id` -- ALREADY EXISTS
- `gemini_uri` -- ALREADY EXISTS as `gemini_file_uri`
- `upload_timestamp` -- ALREADY EXISTS
- `upload_error` -- ALREADY EXISTS as `error_message`

## Open Questions

1. **string_list_value filter syntax validation**
   - What we know: AIP-160 specifies `:` operator for list containment (e.g., `topics:"epistemology"`). The SDK supports `string_list_value` with `{"values": [...]}` format.
   - What's unclear: Whether Gemini File Search fully implements AIP-160's `:` operator for `string_list_value` fields. No explicit example in official docs showing `string_list_value` in a metadata_filter.
   - Recommendation: **Test with Stage 1 validation (20 docs)**. Upload a file with `string_list_value` metadata, then query with `topics:"epistemology"`. If it works, proceed. If not, fall back to comma-separated `string_value` with `=` operator. This is the #1 risk to validate.

2. **Optimal key_themes extraction from Tier 4**
   - What we know: Tier 4 has `semantic_description.key_arguments` (list of strings, 1-4 items) and `semantic_description.philosophical_positions` (list of strings, 0-2 items). These are full sentences averaging 50-100 chars each.
   - What's unclear: Whether full-sentence key_arguments are too long for effective metadata filtering, or whether shorter extracted phrases would be better.
   - Recommendation: Use `key_arguments` directly (truncated to 200 chars each, max 5). They are already concise claims. If Stage 2 testing shows poor precision, switch to keyword extraction.

3. **How to handle `needs_review` files (136 files)**
   - What we know: 136 files have `ai_metadata_status = 'needs_review'` with lower confidence scores (0.56-0.66). They have AI metadata but it may be less accurate.
   - What's unclear: Whether uploading these with potentially lower-quality metadata is better than skipping them.
   - Recommendation: Include `needs_review` files in the enriched upload. The confidence score is stored as `quality_score` metadata, enabling downstream filtering. Users can filter by `quality_score >= 0.7` if they want only high-confidence results. Better to have searchable files with lower confidence than unsearchable files.

4. **Content prepend vs no prepend for already-categorized files**
   - What we know: Files with Phase 1 metadata (category, course, date) that also have Phase 6 AI metadata will get content prepend.
   - What's unclear: Whether prepending AI analysis to already-well-categorized course transcripts adds value or introduces noise.
   - Recommendation: Always prepend. The Tier 4 summary provides context that may not be in the transcript itself (e.g., "This lecture discusses X in the context of Y"). Embeddings benefit from this contextual framing.

## Sources

### Primary (HIGH confidence)
- Context7 `/googleapis/python-genai` -- CustomMetadata type definition confirming `string_list_value: Optional[StringListDict]` with `values: list[str]` format
- Context7 `/websites/ai_google_dev_api` -- CustomMetadata JSON representation showing `stringListValue` with `StringList` object
- Context7 `/websites/ai_google_dev_gemini-api` -- import_file API showing custom_metadata parameter, metadata_filter usage
- Local SDK introspection -- `types.CustomMetadataDict.__annotations__` confirming all three value types
- AIP-160 specification (google.aip.dev/160) -- `:` operator for list containment, confirmed via WebFetch
- Gemini File Search documents API (ai.google.dev/api/file-search/documents) -- "A Document can have a maximum of 20 CustomMetadata" (confirmed via WebFetch)
- Existing codebase: `src/objlib/upload/` (client.py, orchestrator.py, state.py, circuit_breaker.py, rate_limiter.py, recovery.py, progress.py) -- fully audited

### Secondary (MEDIUM confidence)
- Phase 2 research (`02-RESEARCH.md`) -- two-step upload pattern, rate limit tiers, tenacity retry patterns
- CLARIFICATIONS-ANSWERED.md -- locked decisions on split strategy, strict entity dependency, conservative concurrency, three-stage testing
- Database analysis -- all counts and schema verified via direct SQLite queries

### Tertiary (LOW confidence)
- Metadata value size limits -- "validate <2048 bytes" mentioned in CLARIFICATIONS-ANSWERED.md but not confirmed in official API docs. Need empirical validation in Stage 1.
- string_list_value filter syntax with `:` operator -- confirmed in AIP-160 spec, but no explicit Gemini File Search example combining `string_list_value` metadata with `:` filter. Needs Stage 1 validation.
- Indexing cost estimate ($0.0024/file) -- from CONTEXT.md provider analysis, not verified against current Gemini pricing.

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- all libraries already in project, no new dependencies
- Architecture (enriched metadata builder): HIGH -- SDK type system confirmed locally, format verified
- Architecture (content injection): HIGH -- straightforward file I/O with existing upload pipeline
- Architecture (pipeline query): HIGH -- SQL joins on existing tables, strict dependency gate
- Pitfalls (string_list_value format): HIGH -- verified via local SDK construction
- Pitfalls (entity dependency blocker): HIGH -- confirmed zero entities extracted in database
- Pitfalls (schema collision): HIGH -- existing columns audited against CLARIFICATIONS-ANSWERED.md proposals
- Filter syntax (`:` operator for lists): MEDIUM -- AIP-160 confirms it, but Gemini-specific validation needed
- Rate limits: MEDIUM -- same as Phase 2 (empirical testing required, not documented)

**Research date:** 2026-02-17
**Valid until:** 2026-03-17 (30 days -- google-genai SDK may update, but File Search API surface is stable)
