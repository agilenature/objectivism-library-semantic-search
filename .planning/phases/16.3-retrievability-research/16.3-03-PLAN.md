---
phase: 16.3-retrievability-research
plan: 03
type: execute
wave: 3
depends_on:
  - 16.3-01
  - 16.3-02
files_modified:
  - data/retrieval-fix-manifest.json
  - src/objlib/upload/orchestrator.py
  - scripts/re_enrich_retrieval.py
  - .planning/phases/16.3-retrievability-research/16.3-REMEDIATION-COMPLETE.md
  - .planning/STATE.md
  - .planning/ROADMAP.md
autonomous: false

must_haves:
  truths:
    - "data/retrieval-fix-manifest.json exists listing all Category A + B files for re-upload"
    - "_reset_existing_files() in orchestrator.py now calls delete_store_document() before delete_file() (MEMORY.md permanent fix implemented)"
    - "All Category A + B files from the manifest have been re-uploaded with metadata headers in production store"
    - "check_stability.py --sample-count 20 exits 0 in a fresh session (Run 1)"
    - "check_stability.py --sample-count 20 exits 0 in a second fresh session at least 1 hour after Run 1 (Run 2)"
    - "store-sync --dry-run confirms 0 orphaned documents after re-upload"
    - "16.3-REMEDIATION-COMPLETE.md documents both A7=0 runs verbatim"
  artifacts:
    - path: "data/retrieval-fix-manifest.json"
      provides: "List of Category A + B files requiring re-upload with metadata headers"
      contains: "file_path, filename, category, gemini_store_doc_id"
    - path: "src/objlib/upload/orchestrator.py"
      provides: "_reset_existing_files() with store document deletion (MEMORY.md permanent fix)"
      contains: "delete_store_document"
    - path: "scripts/re_enrich_retrieval.py"
      provides: "Re-upload script for manifest files with metadata headers"
      contains: "retrieval-fix-manifest.json"
    - path: ".planning/phases/16.3-retrievability-research/16.3-REMEDIATION-COMPLETE.md"
      provides: "Two fresh-session A7=0 run outputs verbatim + remediation summary"
      contains: "VERDICT: STABLE"
  key_links:
    - from: "scripts/re_enrich_retrieval.py"
      to: "src/objlib/upload/header_builder.py"
      via: "build_metadata_header(file_path, conn) called for each manifest file"
      pattern: "header prepended to transcript before upload"
    - from: "scripts/re_enrich_retrieval.py"
      to: "src/objlib/upload/client.py"
      via: "delete_store_document(old_store_doc_id) called before upload"
      pattern: "upload-first sequence: upload new -> add to store -> verify -> delete old store doc -> delete old raw file -> update SQLite"
    - from: "check_stability.py A7"
      to: "re-uploaded Category A + B files"
      via: "targeted query hits re-uploaded content with injected class-number header"
      pattern: "A7 exits 0 (tolerance=0)"
---

<objective>
Apply the validated H1 fix (metadata header injection) at production scale. Re-upload all Category A + B files with metadata headers using the upload-first sequence. Implement the MEMORY.md permanent fix for `_reset_existing_files()`. Confirm two consecutive fresh-session A7=0 runs separated by 1 hour.

Purpose: This is the Phase 16.3 gate. A7 must exit 0 at zero tolerance in two independent fresh-session runs. This unblocks Phase 16-02 (temporal stability protocol) and ultimately Phase 17.

Output: Production store with ~454 files re-uploaded with metadata headers. Two A7=0 verification runs. MEMORY.md permanent fix implemented. Phase 16.3 gate PASSED.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16.3-retrievability-research/16.3-CONTEXT.md
@.planning/phases/16.3-retrievability-research/CLARIFICATIONS-ANSWERED.md
@.planning/phases/16.3-retrievability-research/16.3-01-SUMMARY.md
@.planning/phases/16.3-retrievability-research/16.3-02-SUMMARY.md
@src/objlib/upload/client.py
@src/objlib/upload/header_builder.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate fix manifest and implement MEMORY.md permanent fix</name>
  <files>
    data/retrieval-fix-manifest.json
    src/objlib/upload/orchestrator.py
  </files>
  <action>
    **Part A: Generate fix manifest**

    Run the following SQL to identify all Category A and Category B files:

    ```python
    import sqlite3, json
    from pathlib import Path

    conn = sqlite3.connect("data/library.db")

    # Category A: Other-stem files (topic == filename stem, not MOTM, approved)
    cat_a = conn.execute("""
        SELECT f.file_path, f.filename, f.gemini_store_doc_id, f.gemini_file_id,
               json_extract(f.metadata_json, '$.topic') AS topic
        FROM files f
        WHERE f.ai_metadata_status = 'approved'
          AND f.gemini_state = 'indexed'
          AND f.file_path NOT LIKE '%/MOTM/%'
          AND f.filename NOT LIKE 'Episode %'
          AND json_extract(f.metadata_json, '$.topic') IS NOT NULL
          AND json_extract(f.metadata_json, '$.topic') =
              REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(
                  SUBSTR(f.filename, 1, LENGTH(f.filename) - 4),
                  ' - ', '-'), '.', ''), ' ', '-'), ':', ''), '(', '')
          -- Approximate: topic looks like filename stem (simplified)
    """).fetchall()
    ```

    Note: The exact SQL for "topic == filename stem" may need adjustment because filename stems have spaces and hyphens while topics have words. Use the simpler approach: check the category from the Phase 16.1 definition — files where `metadata_json->>'topic'` is set and the filename has a class-number pattern:

    ```python
    # Better approach: Category A = class-number files (contain "Class NN-NN")
    cat_a_rows = conn.execute("""
        SELECT file_path, filename, gemini_store_doc_id, gemini_file_id
        FROM files
        WHERE gemini_state = 'indexed'
          AND ai_metadata_status = 'approved'
          AND (
            filename LIKE '% - Class %-%.txt'
            OR filename LIKE '% - Class %-% - %.txt'
            OR filename LIKE '% - Class %-% Office%.txt'
            OR filename LIKE '% Lesson %-%.txt'
          )
        ORDER BY filename
    """).fetchall()

    # Category B: MOTM files (all MOTM are candidates; use all indexed MOTM files)
    cat_b_rows = conn.execute("""
        SELECT file_path, filename, gemini_store_doc_id, gemini_file_id
        FROM files
        WHERE gemini_state = 'indexed'
          AND ai_metadata_status = 'approved'
          AND filename LIKE 'MOTM_%'
        ORDER BY filename
    """).fetchall()
    ```

    Write to `data/retrieval-fix-manifest.json`:
    ```json
    {
      "generated_at": "...",
      "description": "Files requiring re-upload with metadata headers for A7 retrieval fix",
      "category_a": {
        "count": N,
        "description": "Course class-number files (class identifier not in transcript content)",
        "sample": ["first 5 filenames..."],
        "files": [
          {"file_path": "...", "filename": "...", "gemini_store_doc_id": "...", "gemini_file_id": "..."},
          ...
        ]
      },
      "category_b": {
        "count": N,
        "description": "MOTM files (generic topic topics, semantic discrimination needed)",
        "sample": ["first 5 filenames..."],
        "files": [...]
      },
      "total": N
    }
    ```

    Print a summary: `Category A: N files, Category B: M files, Total: N+M files`

    Spot-check: Print 5 sample filenames from each category. Verify they look correct (Category A should be class-number course files; Category B should be MOTM_.* files).

    **SAFETY CHECK before proceeding:** Confirm `gemini_store_doc_id IS NOT NULL` for all manifest files. If any manifest file has NULL `gemini_store_doc_id`, it cannot be located in the store for deletion — log a warning and exclude it from the manifest (these files may have been uploaded without store doc tracking).

    **Part B: Implement MEMORY.md permanent fix in orchestrator.py**

    Read `src/objlib/upload/orchestrator.py`. Find `_reset_existing_files()` in `EnrichedUploadOrchestrator` (or equivalent class). The current implementation calls `delete_file()` but NOT `delete_store_document()`.

    The fix is surgical (~10 lines, per MEMORY.md):
    1. Before the reset loop, fetch all store documents once: `docs = await client.list_store_documents()`
    2. Build a dict: `store_doc_by_file_id = {doc_file_id_suffix: doc.name for doc in docs}`
    3. Inside the loop after `delete_file(gemini_file_id)`, also call `delete_store_document(store_doc_name)` for the matching document.

    This is the "PERMANENT FIX REQUIRED" documented in MEMORY.md. Implement it now — it prevents future orphan accumulation whenever `_reset_existing_files()` is called.

    After the edit:
    - Verify the edit is syntactically correct: `python3 -c "import src.objlib.upload.orchestrator; print('OK')"` (or appropriate import path)
    - Confirm `delete_store_document` is called inside the loop by grepping: `grep -n "delete_store_document" src/objlib/upload/orchestrator.py`
  </action>
  <verify>
    `cat data/retrieval-fix-manifest.json | python3 -c "import json,sys; m=json.load(sys.stdin); print(f'Category A: {m[\"category_a\"][\"count\"]}, Category B: {m[\"category_b\"][\"count\"]}, Total: {m[\"total\"]}')`
    Total count should be in range 400-550 (Category A: ~440, Category B: ~468, some overlap possible).
    `grep -n "delete_store_document" src/objlib/upload/orchestrator.py` shows at least one match inside `_reset_existing_files`.
    `python3 -m py_compile src/objlib/upload/orchestrator.py` exits 0 (no syntax errors).
  </verify>
  <done>data/retrieval-fix-manifest.json exists with Category A + B file lists. _reset_existing_files() in orchestrator.py now calls delete_store_document() (MEMORY.md permanent fix implemented). Both pass import/compile checks.</done>
</task>

<task type="auto">
  <name>Task 2: Re-upload manifest files with metadata headers</name>
  <files>scripts/re_enrich_retrieval.py</files>
  <action>
    Create `scripts/re_enrich_retrieval.py` — the re-upload script that processes the manifest file and applies the metadata header fix to all Category A + B files in the production store.

    **Safety invariants (hardcoded, not configurable):**
    - ONLY touches files in `data/retrieval-fix-manifest.json`
    - ONLY operates on `objectivism-library` store (production)
    - Upload-first sequence: new upload -> store import -> verify present -> delete old store doc -> delete old raw file -> update SQLite
    - Batch size: 10 files with 3-second delays between batches (per Q5 decision)
    - After every 50 files: run store-sync dry-run and abort if orphan count > 0
    - 404 on `delete_file(old_gemini_file_id)` is treated as success (48hr raw files may already be expired)

    **Script structure:**

    ```python
    #!/usr/bin/env python3
    """Phase 16.3 production remediation: re-upload Category A + B files with metadata headers.

    Upload-first sequence per CLARIFICATIONS-ANSWERED.md Q5:
    1. Build content with metadata header from SQLite
    2. Write temp file, upload to Files API -> new_gemini_file_id
    3. Import to production store -> new_store_doc_name; poll operation.done
    4. Verify: confirm new_store_doc_name present in store
    5. Delete old store document via delete_store_document(old_store_doc_id)
    6. Delete old raw file via delete_file(old_gemini_file_id) [404 = success]
    7. Update SQLite: gemini_file_id, gemini_store_doc_id columns
    8. Remove temp file

    Run store-sync --dry-run after every 50 files to verify 0 orphan accumulation.

    Usage:
        python scripts/re_enrich_retrieval.py --manifest data/retrieval-fix-manifest.json
        python scripts/re_enrich_retrieval.py --manifest data/retrieval-fix-manifest.json --dry-run
        python scripts/re_enrich_retrieval.py --manifest data/retrieval-fix-manifest.json --limit 10
    """
    ```

    The script should:
    1. Load the manifest from `data/retrieval-fix-manifest.json`
    2. Process Category A first, then Category B
    3. For each file (upload-first sequence):
       a. Read the raw transcript from `file_path` on disk
       b. Call `build_metadata_header(file_path, conn)` from `header_builder.py`
       c. Concatenate: `header + "\n" + raw_content`
       d. Write to a temp file (use `tempfile.NamedTemporaryFile`)
       e. Upload temp file to Files API with `display_name = filename`
       f. Poll until ACTIVE (`wait_for_active`)
       g. Import to production store (`import_to_store`)
       h. Poll operation until done (`poll_operation`)
       i. Verify: list store docs and confirm new doc is present (or trust operation success from Phase 11 finding — documents are visible immediately)
       j. Delete old store document: `delete_store_document(old_full_store_doc_name)` where old_full_store_doc_name = `f"{store_resource_name}/documents/{gemini_store_doc_id}"`
       k. Delete old raw file: `delete_file(old_gemini_file_id)` — catch 404 as success (48hr TTL may be expired)
       l. Update SQLite:
          ```sql
          UPDATE files
          SET gemini_file_id = ?,
              gemini_store_doc_id = ?,
              gemini_file_uri = ?,
              upload_timestamp = ?,
              gemini_state_updated_at = ?
          WHERE file_path = ?
          ```
       m. Delete temp file
    4. Every 10 files: sleep 3 seconds (per Q5 batch delay)
    5. Every 50 files: run `python scripts/check_store_orphans.py` (or inline store-sync check) to verify 0 orphans. If orphan count > 0, log warning and continue (do NOT abort — orphan accumulation is monitored, not a hard stop)
    6. At the end: print summary (total processed, succeeded, failed, skipped)

    **On completion, run a final verification:**
    ```bash
    python -m objlib store-sync --store objectivism-library --dry-run
    ```
    Confirm: 0 orphaned documents. DB count matches store document count.

    **IMPORTANT:** If a file's `file_path` does not exist on disk (the source file at `/Volumes/U32 Shadow/Objectivism Library/...` is missing or the volume is unmounted), log a warning and skip that file. Do not abort the entire run.

    Execute the script for all manifest files:
    ```bash
    python scripts/re_enrich_retrieval.py --manifest data/retrieval-fix-manifest.json
    ```

    Monitor output for errors. If more than 20% of files fail in a batch, pause and investigate before continuing.
  </action>
  <verify>
    Script runs without Python syntax errors: `python3 -m py_compile scripts/re_enrich_retrieval.py`
    Dry-run mode works: `python scripts/re_enrich_retrieval.py --manifest data/retrieval-fix-manifest.json --dry-run`
    After full execution: `python -m objlib store-sync --store objectivism-library` shows 0 orphaned documents.
    DB count of indexed files unchanged or increased (never decreased below 1749).
    At least 90% of manifest files successfully processed (logged in script output).
  </verify>
  <done>re_enrich_retrieval.py implemented and executed successfully. All (or >90% of) Category A + B manifest files re-uploaded with metadata headers. store-sync confirms 0 orphans. DB gemini_store_doc_id updated for processed files.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Production re-upload complete:
    - All Category A + B files re-uploaded with metadata headers (upload-first sequence)
    - _reset_existing_files() patched with MEMORY.md permanent fix
    - store-sync confirms 0 orphaned documents

    Now need two fresh-session check_stability.py runs separated by 1 hour.

    CRITICAL: Each run MUST be in a fresh Claude Code session (/clear before starting).
    This is not optional ceremony — it is a core validity requirement. Fresh sessions
    cannot be biased by prior session memory of what was found.
  </what-built>
  <how-to-verify>
    **Run 1 (now, fresh session):**
    1. Type `/clear` (or start a new Claude Code session)
    2. In the fresh session, run:
       ```bash
       python scripts/check_stability.py --store objectivism-library --sample-count 20 --verbose
       ```
    3. Record the full output verbatim — especially the A7 verbose lines showing which files were queried and their results
    4. Exit code must be 0 (STABLE)
    5. Save output to `.planning/phases/16.3-retrievability-research/16.3-REMEDIATION-RUN1.md`

    **Wait at least 1 hour (per SC4 / CLARIFICATIONS-ANSWERED.md Q7)**

    **Run 2 (fresh session, 1+ hour after Run 1):**
    1. Type `/clear` again (or start another new Claude Code session)
    2. Run the same command
    3. Exit code must be 0 (STABLE)
    4. Save output to `.planning/phases/16.3-retrievability-research/16.3-REMEDIATION-RUN2.md`

    Both runs must exit 0. If either fails, do NOT proceed — surface the failure for diagnosis.
  </how-to-verify>
  <resume-signal>
    Type "run1-pass" after Run 1 exits 0 (to resume for the wait period).
    Type "both-pass" after Run 2 also exits 0 (to proceed to the final documentation task).
    Type "run1-fail [details]" or "run2-fail [details]" if either run fails — plan stops and diagnosis is needed.
  </resume-signal>
</task>

<task type="auto">
  <name>Task 4: Write remediation-complete documentation and update project state</name>
  <files>
    .planning/phases/16.3-retrievability-research/16.3-REMEDIATION-COMPLETE.md
    .planning/STATE.md
    .planning/ROADMAP.md
  </files>
  <action>
    **This task runs after the checkpoint is approved (both fresh-session runs exit 0).**

    **1. Write 16.3-REMEDIATION-COMPLETE.md:**

    ```markdown
    # Phase 16.3 Remediation Complete

    **Date:** [date of Run 2]
    **Gate:** PASSED

    ## Success Criteria

    | SC | Criterion | Status |
    |----|-----------|--------|
    | SC1 | Root cause identified with affirmative evidence | PASS |
    | SC2 | Fix tested on failing files — appear in top-10 | PASS |
    | SC3 | Fix applied to all Category A + B files at production scale | PASS |
    | SC4 | check_stability exits 0 in two consecutive fresh-session runs 1+ hour apart | PASS |
    | SC5 | No --reset-existing on pre-existing files; store-sync confirms 0 orphans | PASS |

    ## Fresh Session Verification Runs

    ### Run 1
    **Time:** [timestamp]
    **Exit code:** 0
    **Verdict:** STABLE
    [paste full output]

    ### Run 2
    **Time:** [timestamp, must be 1+ hour after Run 1]
    **Exit code:** 0
    **Verdict:** STABLE
    [paste full output]

    ## Phase 16 Temporal Stability Protocol Status

    This T=0 baseline replaces the UNSTABLE baseline from 16.1-T0-BASELINE.md.

    Phase 16-02 (temporal stability: T+4h, T+24h, T+36h) is now UNBLOCKED.

    ## Files Modified

    - `src/objlib/upload/orchestrator.py` — MEMORY.md permanent fix for _reset_existing_files()
    - `src/objlib/upload/header_builder.py` — new metadata header builder
    - `scripts/re_enrich_retrieval.py` — production re-upload script
    - `data/retrieval-fix-manifest.json` — manifest of re-uploaded files

    ## Store State

    - Production store: objectivism-library
    - Documents: [count from Run 2 A1]
    - Orphans: 0
    ```

    **2. Update STATE.md:**
    - Change Phase 16.3 status to COMPLETE
    - Change Phase 16-02 status from BLOCKED to UNBLOCKED
    - Add to temporal stability log:
      ```
      Temporal stability log (Phase 16.3 — A7 remediation):
      - Run 1 ([timestamp]): STABLE — A7 PASS (0/20 misses, tolerance=0)
      - Run 2 ([timestamp], 1h+ after Run 1): STABLE — A7 PASS (0/20 misses, tolerance=0)
      ```
    - Add to Decisions section:
      ```
      - [16.3]: H1 (content injection) confirmed as root cause. H3 confirmed (class numbers absent from transcripts). H2 + H4 falsified.
      - [16.3]: Metadata header format: structured (--- DOCUMENT METADATA ---) with Title/Course/Class/Topic/Tags fields, <200 tokens
      - [16.3]: MEMORY.md permanent fix implemented in _reset_existing_files() — delete_store_document() now called before delete_file()
      - [16.3]: Header scope: Category A (class-number course files, ~N files) + Category B (MOTM files, ~M files) only
      ```

    **3. Update ROADMAP.md:**
    - Mark Phase 16.3 plans as complete:
      ```
      Plans:
      - [x] 16.3-01-PLAN.md -- Diagnosis spike: H1/H2/H3/H4 falsification
      - [x] 16.3-02-PLAN.md -- Intervention test: fix validated on failing files in test store
      - [x] 16.3-03-PLAN.md -- Production remediation: two fresh-session A7=0 confirmations
      ```
    - Update Phase 16.3 status to COMPLETE
    - Update Phase 16 plans to show 16-02 as unblocked (remove BLOCKED note)

    **4. Run final store-sync to confirm clean state:**
    ```bash
    python -m objlib store-sync --store objectivism-library
    ```
    Confirm: 0 orphaned documents.
  </action>
  <verify>
    `cat .planning/phases/16.3-retrievability-research/16.3-REMEDIATION-COMPLETE.md` shows PASSED for all 5 success criteria.
    Both Run 1 and Run 2 outputs appear verbatim in the document (full check_stability output).
    `grep "Phase 16.3" .planning/ROADMAP.md` shows "COMPLETE" or all plans marked [x].
    `grep "16.3" .planning/STATE.md` shows COMPLETE status.
    `python -m objlib store-sync --store objectivism-library` exits 0 with 0 orphans.
  </verify>
  <done>Phase 16.3 documented as complete. Two fresh-session A7=0 runs recorded verbatim. STATE.md and ROADMAP.md updated. Phase 16-02 temporal stability protocol is unblocked. Phase 16.3 gate PASSED.</done>
</task>

</tasks>

<verification>
Gate: Two consecutive fresh-session A7=0 runs + 0 orphans + Phase 16 temporal stability protocol unblocked.

Final checklist:
- [ ] data/retrieval-fix-manifest.json: exists, lists Category A + B files with gemini_store_doc_id
- [ ] src/objlib/upload/orchestrator.py: _reset_existing_files() calls delete_store_document() (MEMORY.md fix)
- [ ] scripts/re_enrich_retrieval.py: exists, executed successfully for all manifest files
- [ ] store-sync --dry-run: 0 orphaned documents
- [ ] Run 1: check_stability exits 0 (fresh session), output in 16.3-REMEDIATION-RUN1.md
- [ ] Run 2: check_stability exits 0 (fresh session, 1+ hour after Run 1), output in 16.3-REMEDIATION-RUN2.md
- [ ] 16.3-REMEDIATION-COMPLETE.md: all 5 SC rows show PASS
- [ ] STATE.md: Phase 16.3 COMPLETE, Phase 16-02 UNBLOCKED
- [ ] ROADMAP.md: Phase 16.3 plans all [x]

Phase 16.3 gate PASSES when:
- checkpoint:human-verify returns "both-pass"
- All documentation tasks complete
- 0 orphans confirmed
</verification>

<success_criteria>
Phase 16.3 complete when:
1. data/retrieval-fix-manifest.json exists listing all re-uploaded files
2. MEMORY.md permanent fix implemented (_reset_existing_files() now deletes store document)
3. All manifest files re-uploaded with metadata headers (>90% success rate)
4. 0 orphaned store documents (store-sync confirms)
5. check_stability --sample-count 20 exits 0 in BOTH:
   - Fresh session Run 1 (immediately after re-upload)
   - Fresh session Run 2 (1+ hour after Run 1)
6. 16.3-REMEDIATION-COMPLETE.md documents all evidence verbatim
7. STATE.md + ROADMAP.md updated; Phase 16-02 unblocked
</success_criteria>

<output>
After completion, create `.planning/phases/16.3-retrievability-research/16.3-03-SUMMARY.md` following the summary template.

Key fields to populate:
- Total files re-uploaded (Category A count, Category B count)
- Both Run 1 and Run 2 timestamps and exit codes
- MEMORY.md permanent fix confirmation
- Phase 16-02 unblocked confirmation
- Any files that could not be processed (missing from disk, NULL store_doc_id, etc.)
</output>
