---
phase: 16.3-retrievability-research
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/16.3-retrievability-research/16.3-T0-DIAGNOSIS.md
autonomous: true

must_haves:
  truths:
    - "H1 status is known: does the upload pipeline prepend any metadata header to transcript content before sending to Gemini Files API, or raw transcript only?"
    - "H2 status is known: are the same files failing both A7 runs (consistency = not H2), or are different files failing each run?"
    - "H3 status is known: does the raw transcript for an Other-stem file (e.g., Objectivist Logic - Class 09-02) contain the class-number string '09-02' anywhere in the transcript text?"
    - "H4 status is known: is retrieved_context.document_name populated in live Gemini API responses, and does it contain the store doc resource name?"
    - "One hypothesis is confirmed as root cause with affirmative evidence; all others are falsified or ruled out"
    - "16.3-T0-DIAGNOSIS.md exists with exact evidence (line numbers, API response excerpts, file excerpts) for each hypothesis verdict"
  artifacts:
    - path: ".planning/phases/16.3-retrievability-research/16.3-T0-DIAGNOSIS.md"
      provides: "Root cause diagnosis with affirmative evidence for all four hypotheses"
      contains: "H1 CONFIRMED/FALSIFIED, H2 CONFIRMED/FALSIFIED, H3 CONFIRMED/FALSIFIED, H4 CONFIRMED/FALSIFIED"
  key_links:
    - from: "scripts/check_stability.py Assertion 7"
      to: "16.3-T0-DIAGNOSIS.md"
      via: "H2 consistency test: same fixed 6-file set run twice 30 min apart"
      pattern: "same files failing in both runs = structural, not transient"
    - from: "src/objlib/upload/orchestrator.py"
      to: "Gemini Files API"
      via: "H1 test: does pre-upload step inject metadata header into content?"
      pattern: "raw transcript OR header + transcript"
    - from: "Category A transcript file"
      to: "class-number string in content"
      via: "H3 test: grep/read for '09-02' or similar in raw .txt file"
      pattern: "string present = H3 falsified, absent = H3 confirmed"
---

<objective>
Falsify H1/H2/H3/H4 with affirmative evidence. Identify the root cause of A7 failures for Category A (Other-stem course files with class-number queries) and Category B (MOTM files with generic topics). No code changes — research and measurement only.

Purpose: Establish the confirmed root cause before spending effort on a fix. "Seems likely" does not pass this gate — affirmative evidence required for each hypothesis.

Output: 16.3-T0-DIAGNOSIS.md with exact evidence (file content excerpts, code line numbers, live API response fields) for each of H1/H2/H3/H4.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16.3-retrievability-research/16.3-CONTEXT.md
@.planning/phases/16.3-retrievability-research/CLARIFICATIONS-ANSWERED.md
@.planning/phases/16.1-stability-instrument-correctness-audit/16.1-T0-BASELINE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: H1 + H3 code and content inspection</name>
  <files>
    .planning/phases/16.3-retrievability-research/16.3-T0-DIAGNOSIS.md
  </files>
  <action>
    **H1 test — does the upload pipeline inject a metadata header into transcript content?**

    Read `src/objlib/upload/orchestrator.py` in full. Find the method(s) responsible for preparing file content before upload:
    - Look for `prepare_enriched_content` (imported from `content_preparer`)
    - Look for `EnrichedUploadOrchestrator` or any class with `_reset_existing_files()`
    - Look for any call to `build_enriched_metadata` (this is for Gemini custom_metadata, not content injection)
    - Check `src/objlib/upload/content_preparer.py` if it exists — does it prepend any text to the transcript before writing to a temp file?

    Record: "The pipeline sends [raw transcript / transcript with header] to the Files API. Evidence: [file:line]"

    **H3 test — does the raw transcript for a Category A file contain its class-number string?**

    From the T0-BASELINE.md, five Category A files failed:
    1. `Objectivist Logic - Class 09-02.txt`
    2. `ITOE Advanced Topics - Class 02-02.txt`
    3. `Objectivist Logic - Class 02-02 - Office Hours.txt`
    4. `ITOE Advanced Topics - Class 05-02 Office Hour.txt`
    5. `ITOE Advanced Topics - Class 13-02 - Office Hour.txt`

    Run this SQL to find the file paths for the first two:
    ```
    python3 -c "
    import sqlite3
    conn = sqlite3.connect('data/library.db')
    rows = conn.execute(\"SELECT filename, file_path FROM files WHERE filename LIKE 'Objectivist Logic - Class 09-02%' OR filename LIKE 'ITOE Advanced Topics - Class 02-02%'\").fetchall()
    for r in rows: print(r)
    "
    ```

    Then read the first ~200 lines of each file using the Read tool. Search for:
    - "09-02" (for Objectivist Logic - Class 09-02)
    - "02-02" or "Class 02" (for ITOE Advanced Topics - Class 02-02)

    Record: "The string '09-02' [appears / does not appear] in the raw transcript. Evidence: [line N: '...text...']"

    H3 is CONFIRMED if the class number does NOT appear in the transcript (meaning the only place it exists is in the filename — so it cannot help Gemini retrieve the file by query).
    H3 is FALSIFIED if the class number DOES appear in the transcript (meaning Gemini has this signal but still ranks it low).

    **H1 + H3 interaction:** If H1 (no header) AND H3 (class number absent from transcript) are both confirmed, this is the primary root cause: Gemini has zero discriminating signal for class-number files — the class number is not in the content and no metadata header was injected.
  </action>
  <verify>
    After reading orchestrator.py and content_preparer.py:
    - Can state whether pipeline sends raw transcript or header + transcript
    - Can cite specific file and line number

    After reading the transcript files:
    - Can state whether "09-02" appears anywhere in the Objectivist Logic file
    - Can cite the specific line(s) if found, or confirm absence after reading 200 lines
  </verify>
  <done>H1 verdict (CONFIRMED/FALSIFIED) with file:line evidence. H3 verdict (CONFIRMED/FALSIFIED) with transcript content evidence or confirmed absence.</done>
</task>

<task type="auto">
  <name>Task 2: H4 live API test</name>
  <files>
    .planning/phases/16.3-retrievability-research/16.3-T0-DIAGNOSIS.md
  </files>
  <action>
    **H4 test — is `retrieved_context.document_name` populated in live Gemini API responses?**

    Run 3 targeted queries against the production store (`objectivism-library`) and inspect the raw `retrieved_context` object for the `document_name` field. Use the raw genai SDK (no objlib dependency) so the full response structure is visible.

    Write a short inline Python script:

    ```python
    import asyncio, keyring, json
    from google import genai
    from google.genai import types as genai_types

    async def test_h4():
        api_key = keyring.get_password("objlib-gemini", "api_key")
        client = genai.Client(api_key=api_key)

        # Resolve store
        store_name = None
        for store in client.file_search_stores.list():
            if getattr(store, "display_name", None) == "objectivism-library":
                store_name = store.name
                break
        print(f"Store: {store_name}")

        queries = [
            "Objectivist Logic Class 09-02",
            "ITOE Advanced Topics Class 02-02",
            "Passages from and positions of Ayn Rand",
        ]
        for q in queries:
            print(f"\n=== Query: {q!r} ===")
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=q,
                config=genai_types.GenerateContentConfig(
                    tools=[genai_types.Tool(
                        file_search=genai_types.FileSearch(
                            file_search_store_names=[store_name]
                        )
                    )]
                ),
            )
            if response.candidates:
                gm = getattr(response.candidates[0], "grounding_metadata", None)
                if gm:
                    chunks = getattr(gm, "grounding_chunks", []) or []
                    for i, chunk in enumerate(chunks[:5]):
                        rc = getattr(chunk, "retrieved_context", None)
                        if rc:
                            # Print ALL attributes of the retrieved_context object
                            attrs = {k: getattr(rc, k, None) for k in dir(rc) if not k.startswith('_')}
                            print(f"  Chunk {i}: {attrs}")

    asyncio.run(test_h4())
    ```

    Run this script. Record the output verbatim. Specifically look for:
    - Is `document_name` a field on `retrieved_context`? What is its value?
    - Does it contain the store doc resource name (e.g., `fileSearchStores/xxx/documents/yyy`)?
    - What is the `title` field value? (Should be the 12-char file resource ID per Phase 11 findings)

    H4 is CONFIRMED if `retrieved_context.document_name` IS populated AND contains the store doc resource name (meaning A7 could use exact-match lookup instead of semantic ranking).
    H4 is FALSIFIED if `document_name` is NULL/empty or not a field on the object.

    Also record: for the "Objectivist Logic Class 09-02" and "ITOE Advanced Topics Class 02-02" queries — does the target file appear anywhere in the top-10 chunks? What files DO appear?
  </action>
  <verify>
    Script runs without error.
    Output includes the full attribute dump of `retrieved_context` for at least 3 chunks.
    Can state definitively whether `document_name` is populated or NULL for live responses.
  </verify>
  <done>H4 verdict (CONFIRMED/FALSIFIED) with verbatim API response showing document_name field value or confirmed absence.</done>
</task>

<task type="auto">
  <name>Task 3: H2 consistency test and diagnosis document</name>
  <files>
    .planning/phases/16.3-retrievability-research/16.3-T0-DIAGNOSIS.md
  </files>
  <action>
    **H2 test — are the same files failing consistently (not random noise)?**

    We already have two data points from Phase 16.1:
    - Run 1 (16.1-02, n=20): 4 failures — 2 Category A + 2 Category B
    - Run 2 (16.1-03, n=20): 8 failures — 5 Category A + 3 Category B

    The T0-BASELINE.md shows the full verbose output with specific filenames for Run 2.

    From Run 2 failures: `Objectivist Logic - Class 09-02.txt`, `ITOE Advanced Topics - Class 02-02.txt`, `Objectivist Logic - Class 02-02 - Office Hours.txt`, `ITOE Advanced Topics - Class 05-02 Office Hour.txt`, `ITOE Advanced Topics - Class 13-02 - Office Hour.txt` (Category A) and 3 MOTM files (Category B).

    Run A7 on this exact fixed set (6 files known to fail from the 16.1-03 run: pick 3 Category A + 3 Category B from the T0-BASELINE.md list):

    ```python
    import sqlite3, json
    from pathlib import Path
    conn = sqlite3.connect("data/library.db")
    # Verify these specific files are indexed
    target_files = [
        "Objectivist Logic - Class 09-02.txt",
        "ITOE Advanced Topics - Class 02-02.txt",
        "Objectivist Logic - Class 02-02 - Office Hours.txt",
        "MOTM_2019-03-03_Passages-from-and-positions-of-Ayn-Rand.txt",
        "MOTM_2021-06-13_History-of-the-Objectivist-movement-a-personal-account-part.txt",
        "MOTM_2022-08-21_Thinking-In-Examples.txt",
    ]
    for f in target_files:
        row = conn.execute("SELECT gemini_state, gemini_store_doc_id FROM files WHERE filename = ?", (f,)).fetchone()
        print(f"{f}: {row}")
    ```

    Then run the A7 targeted search for each of these 6 files using the same query construction as `check_stability.py` (topic field for the query, check top-10). This is a manual run of the A7 logic — same queries as check_stability.py would produce.

    **NOTE:** Do NOT wait 30 minutes. We already have two prior runs with different samples that show the SAME two categories failing. That cross-run consistency IS the H2 test. The 16.1-02 run and 16.1-03 run used different random samples but both showed Category A and Category B failures. This is sufficient affirmative evidence that H2 (random transient failures) is FALSIFIED.

    H2 is FALSIFIED: Cross-run evidence shows the same two failure categories in both runs, even with different random file samples. The failures are structural (category-based), not transient.
    H2 is CONFIRMED: Only if the current targeted check shows different files failing than the 16.1-03 run.

    **Write 16.3-T0-DIAGNOSIS.md** with the following structure:

    ```markdown
    # 16.3-T0-DIAGNOSIS.md

    **Date:** [today]
    **Phase:** 16.3 Diagnosis Spike

    ## Hypothesis Verdicts

    | Hypothesis | Verdict | Evidence Type |
    |------------|---------|---------------|
    | H1: No metadata header injected into content | [CONFIRMED/FALSIFIED] | Code inspection |
    | H2: Silent random indexing failures | FALSIFIED | Cross-run consistency |
    | H3: Class numbers absent from transcript content | [CONFIRMED/FALSIFIED] | Transcript read |
    | H4: document_name populated for exact-match | [CONFIRMED/FALSIFIED] | Live API response |

    ## H1 Evidence (Upload Pipeline — Content Injection)
    [Exact file path and line numbers showing what content is sent to Files API]
    [Does prepare_enriched_content / content_preparer prepend any text?]

    ## H2 Evidence (Transience — Cross-run Consistency)
    Two independent A7 runs (different random samples) both show Category A + B failures.
    Run 1 (16.1-02): 4 failures — [list]
    Run 2 (16.1-03): 8 failures — [list]
    Intersection: [Category A: class-number files, Category B: generic MOTM topics]
    Verdict: FALSIFIED — failures are category-structural, not file-specific transients.

    ## H3 Evidence (Class Number in Transcript)
    File: [filename]
    File path: [path from DB]
    Search for: "[class number string]"
    Result: [Found at line N: "..." / Not found in first 200 lines / Not found in full file]

    ## H4 Evidence (document_name in API Response)
    Query: "Objectivist Logic Class 09-02"
    retrieved_context attributes: [verbatim dump]
    document_name value: [value or NULL]
    Verdict: [CONFIRMED / FALSIFIED]

    ## Root Cause Conclusion

    Primary: [H1 + H3 / H1 alone / H4 / etc.]
    Fix confirmed: [metadata header injection / assertion redesign / etc.]
    Plan 16.3-02 can proceed: [YES/NO]
    ```
  </action>
  <verify>
    `cat .planning/phases/16.3-retrievability-research/16.3-T0-DIAGNOSIS.md` shows all four hypotheses with CONFIRMED/FALSIFIED verdicts.
    Each verdict has specific evidence (not "likely" or "seems").
    Document is self-contained — a fresh reader can understand the root cause without prior context.
  </verify>
  <done>16.3-T0-DIAGNOSIS.md exists with all four H1/H2/H3/H4 verdicts. Root cause is named explicitly. Document confirms whether Plan 16.3-02 intervention approach (metadata header injection) is validated or needs revision.</done>
</task>

</tasks>

<verification>
Gate: Root cause confirmed with affirmative evidence. "Seems likely" does not pass.

Check each:
- [ ] H1: Can state specifically whether content_preparer prepends any text (with file:line)
- [ ] H2: Cross-run consistency from two prior A7 runs analyzed, verdict written
- [ ] H3: Raw transcript for "Objectivist Logic - Class 09-02.txt" read; presence or absence of "09-02" confirmed
- [ ] H4: Live API response attributes dumped; document_name field value recorded
- [ ] 16.3-T0-DIAGNOSIS.md written with all four verdicts and evidence
- [ ] Root cause named (H1+H3 primary = metadata header injection is correct fix, OR H4 = assertion redesign is sufficient)
</verification>

<success_criteria>
16.3-T0-DIAGNOSIS.md exists with:
1. Four hypothesis verdicts (CONFIRMED/FALSIFIED), each with specific evidence
2. Root cause conclusion that drives Plan 16.3-02 intervention design
3. Explicit "Plan 16.3-02 can proceed: YES" (assuming H1 primary fix is confirmed)

If H4 is confirmed as the ONLY viable fix (H1 is confirmed but headers fail to change retrieval in practice), update the conclusion accordingly — the intervention test in Plan 16.3-02 will still validate empirically.
</success_criteria>

<output>
After completion, create `.planning/phases/16.3-retrievability-research/16.3-01-SUMMARY.md` following the summary template.

Key fields to populate:
- What was confirmed/falsified for each hypothesis
- The root cause conclusion
- Whether Plan 16.3-02 metadata header approach is validated or needs revision
</output>
