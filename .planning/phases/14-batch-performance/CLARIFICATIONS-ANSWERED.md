---
phase: 14-batch-performance
generated: 2026-02-22
mode: yolo
strategy: balanced (pick consensus/recommended option)
---

# CLARIFICATIONS-ANSWERED.md

## Phase 14: Batch Performance Benchmark — Auto-Generated Decisions

**Generated:** 2026-02-22
**Mode:** YOLO (balanced strategy — pick ✅ consensus option, then ⚠️ recommended, then simplest)
**Source:** Auto-generated by discuss-phase-ai --yolo

---

## Decision Summary

**Total questions:** 9
**Tier 1 (Blocking):** 5 answered
**Tier 2 (Important):** 2 answered
**Tier 3 (Polish):** 2 answered

---

## Tier 1: Blocking Decisions

### Q1: How do we define and label the "transition latency" metric?

**YOLO DECISION:** **Option C — Three labeled timings per file**

```
mock_api_ms   = cumulative time in asyncio.sleep() mock calls
db_total_ms   = time in aiosqlite transactions (begin → commit, includes lock wait)
fsm_net_ms    = total_wall_ms − mock_api_ms  (FSM+DB overhead, used for bottleneck id)
```

**Rationale:**
- Confidence: ✅ Consensus
- End-to-end throughput is needed for VLID-06 Criterion 3 (realistic hours estimate)
- Net overhead is needed for VLID-06 Criterion 2 (bottleneck identification)
- Tracking `mock_api_ms` explicitly lets the harness compute `fsm_net_ms` without ambiguity
- Strategy: Consensus pick

**Sub-decisions:**
- Mock timing: inject `asyncio.sleep()` inside the mock adapter class, record sleep duration before and after; subtract to get `mock_api_ms` per file
- DB timing: `time.perf_counter()` before `await db.execute("BEGIN IMMEDIATE")` and after `await db.commit()` for each transition
- FSM dispatch timing: `time.perf_counter()` around `fsm.send("upload")` call excluding the DB operations within

---

### Q2: What concurrency level should the primary benchmark run use?

**YOLO DECISION:** **Option B — Three-configuration matrix**

```
concurrency=1   : sequential baseline (zero contention reference)
concurrency=10  : PRIMARY metric for VLID-06 (production-representative)
concurrency=50  : stress test (reveals contention ceiling)
```

**Rationale:**
- Confidence: ✅ Consensus (OpenAI + Perplexity both proposed multi-config; Gemini proposed N=50)
- `concurrency=10` is the VLID-06 primary number; all "pass/fail" judgments use this
- `concurrency=1` establishes per-file baseline without any contention effects
- `concurrency=50` stresses WAL serialization to confirm the bottleneck hypothesis
- Strategy: Consensus pick, synthesized to N=10 as primary (middle ground between providers)

**Sub-decisions:**
- Implementation: `asyncio.Semaphore(N)` wrapping each file's full 4-transition cycle
- Report: all three configurations reported; VLID-06 conclusion drawn from concurrency=10

---

### Q3: What mock latency profile should the benchmark use?

**YOLO DECISION:** **Option C — Two named profiles**

```
--mock-profile=zero      : asyncio.sleep(0)  — stress FSM+DB, identify bottleneck
--mock-profile=realistic : asyncio.sleep(2.0) constant, seed=42 — validate throughput
```

**Rationale:**
- Confidence: ✅ Consensus
- `zero` profile is required to see FSM+DB overhead clearly; without it, mock sleep dominates
- `realistic` profile is required to compute "818 files in X hours" for Criterion 3
- Constant 2.0s (not random) ensures before/after mitigation comparison is noise-free
- seed=42 documented in JSON output for reproducibility
- Strategy: Consensus pick

**Sub-decisions:**
- Latency: 2.0s constant per API call (one for upload, one for import — 4.0s mock per file total)
- Plan 14-01 runs both profiles for baseline; Plan 14-02 runs both for mitigation measurement

---

### Q4: What is the explicit acceptable throughput threshold?

**YOLO DECISION:** **Option B — Dual thresholds**

```
Threshold 1 (FSM overhead gate):
  Profile: zero-latency (asyncio.sleep(0))
  Concurrency: 10
  Target: 818 files × 4 transitions = 3,272 transitions complete in ≤ 5 minutes
  Derived: ≥ 10.9 transitions/second throughput

Threshold 2 (Realistic upload gate):
  Profile: realistic (2.0s per API call)
  Concurrency: 10
  Target: 818 files complete in ≤ 6 hours
  Derived: ≥ 2.3 files/minute throughput
  Note: each file has 4.0s mock sleep (2 calls × 2.0s) + FSM+DB overhead
        theoretical min ≈ 818 × 4.0s / 10 concurrent = 327s ≈ 5.5 minutes
        "≤6 hours" allows for real API variance in Phase 16; benchmark will show headroom
```

**Rationale:**
- Confidence: ✅ Consensus (all 3 providers insisted on explicit number)
- Threshold 1 (FSM overhead) is the real benchmark concern; if this fails, Phase 16 will be slow
- Threshold 2 (realistic) grounds the result in user-visible upload time
- 5 minutes is generous for FSM+DB overhead at zero latency (estimated actual: <1 minute)
- 6 hours is conservative for realistic profile (theoretical min ~5.5 min at 10 concurrency)
- Strategy: Balanced pick — conservative thresholds, easy to beat, proves point conclusively

**Sub-decisions:**
- If both thresholds are met on baseline (no mitigation needed): document measurements, declare VLID-06 passed, skip mitigation in Plan 14-02 (replace with re-measurement at different concurrency for confirmation)
- If Threshold 1 fails (unlikely): implement mitigation, re-measure

---

### Q5: How do we identify which segment is the bottleneck?

**YOLO DECISION:** **Option C — yappi + explicit timing spans**

Primary tool: **yappi** (asyncio-aware, separates wall time from CPU time)

Explicit spans per file per transition:
```python
# Inside each FSM transition cycle:
t0 = perf_counter()
# ... fsm.send("upload") (dispatch only, no DB)
t1 = perf_counter()
fsm_dispatch_ms = (t1 - t0) * 1000

t2 = perf_counter()
# ... await db.execute("BEGIN IMMEDIATE")
# ... await db.execute("UPDATE files SET ...")
t3 = perf_counter()
lock_wait_ms = t3 - t2  # time to acquire write lock
# ... await db.commit()
t4 = perf_counter()
db_total_ms = (t4 - t2) * 1000

t5 = perf_counter()
# ... await mock_api_call()  # asyncio.sleep()
t6 = perf_counter()
mock_api_ms = (t6 - t5) * 1000
```

Aggregate: print P50/P95/P99 per segment. Segment with highest P95 in `fsm_net_ms` breakdown is the declared bottleneck.

**Rationale:**
- Confidence: ✅ Consensus
- yappi correctly handles coroutine suspension/resumption; cProfile would blame `asyncio.sleep`
- Explicit spans are cheap (perf_counter overhead <1μs) and give definitive per-operation numbers
- P95 rather than mean avoids outlier distortion
- Strategy: Consensus pick

**Sub-decisions:**
- Add yappi as dev dependency: `uv add yappi --dev`
- Run yappi around the full benchmark coroutine (`yappi.start()` before, `yappi.stop()` after)
- Save full yappi function stats to `benchmarks/yappi-YYYYMMDD.txt`

---

## Tier 2: Important Decisions

### Q6: How to measure WAL serialization specifically?

**YOLO DECISION:** **Option A — BEGIN IMMEDIATE timing + WAL file size delta**

```python
# Before transaction:
lock_start = perf_counter()
await db.execute("BEGIN IMMEDIATE")
lock_wait_ms = (perf_counter() - lock_start) * 1000

# After full batch:
wal_size_start_bytes = os.path.getsize(db_path + "-wal")  # before benchmark
wal_size_end_bytes = os.path.getsize(db_path + "-wal")    # after benchmark
```

WAL contention verdict: if P95 `lock_wait_ms > 5%` of P95 `db_total_ms`, WAL serialization is a contributor.

**Rationale:**
- Confidence: ⚠️ Two providers agreed
- BEGIN IMMEDIATE directly measures write lock acquisition wait
- WAL file size monitors whether checkpointing is occurring (large WAL = no checkpointing = potential spike)
- Implementation cost: ~10 lines added to harness
- Strategy: Recommended pick

---

### Q7: Which mitigation should be tested first?

**YOLO DECISION:** **Option B — Prioritized by complexity**

```
Step 1: Remove DB-reading guards (in-memory guard checks only)
  → Verify: grep for any guard that reads aiosqlite; move to in-memory state check
  → Measure: re-run benchmark, compare db_total_ms P95 before/after

Step 2: Single persistent aiosqlite connection (connection reuse)
  → Verify: confirm single `async with aiosqlite.connect()` context wraps full batch
  → Measure: re-run benchmark, compare lock_wait_ms P95 before/after

Step 3: Batch DB writes (only if Steps 1+2 don't meet Threshold 1)
  → Commit every 10 transitions; still check OCC per-row
  → Document OCC atomicity risk in SUMMARY.md if this step is needed
```

Stop at first step that brings `fsm_net_ms` throughput to meet Threshold 1 (≤5 min for 3,272 transitions).

**Rationale:**
- Confidence: ⚠️ Two providers (Gemini explicitly warned against jumping to batch writes)
- Complexity gradient prevents unnecessary architectural risk
- Stopping at first success keeps Phase 14 scoped
- Strategy: Recommended pick, complexity-ordered

---

## Tier 3: Polish Decisions

### Q8: FSM object lifecycle?

**YOLO DECISION:** **Hydration approach** — one `FileLifecycleSM` object instantiated per file per full cycle (from DB read → 4 transitions → done). Object is not retained between files.

**Rationale:** This is what Phase 16 production will do. Measuring it accurately requires testing the actual code path.

---

### Q9: Benchmark output format?

**YOLO DECISION:** JSON to `benchmarks/results-YYYYMMDD-HHMMSS.json` (add `benchmarks/` to .gitignore) + Rich table to stdout showing:
- Concurrency level, mock profile
- Total transitions, total elapsed, transitions/sec
- P50/P95 per segment (mock_api, db_total, lock_wait, fsm_dispatch, fsm_net)
- Threshold 1 and Threshold 2: PASS/FAIL

Before/after mitigation comparison: run benchmark twice and compare printed summaries side-by-side in SUMMARY.md.

---

## Phase 14 Plan Implications

Based on these decisions, Plan 14-01 and 14-02 should:

**Plan 14-01 (Benchmark harness + baseline measurement):**
- Build harness: 818 simulated files, three concurrency levels, two mock profiles
- Instrument: yappi + explicit timing spans, WAL measurement
- Run all 6 combinations (3 concurrency × 2 profiles)
- Report: P50/P95/P99 per segment, Threshold 1 and Threshold 2 verdict
- Identify bottleneck by segment (highest P95 in fsm_net breakdown)

**Plan 14-02 (Mitigation + re-measurement):**
- If baseline meets BOTH thresholds: document, declare VLID-06 passed with zero-mitigation
- If baseline fails: implement mitigations in complexity order (Step 1 → 2 → 3 from Q7)
- Re-measure after each mitigation step; stop when thresholds met
- Final report: before/after comparison, declared bottleneck, mitigation that resolved it

---

## Next Steps

1. ✅ Clarifications answered (YOLO mode)
2. ⏭ Proceed to `/gsd:plan-phase 14`

---

*Auto-generated by discuss-phase-ai --yolo (balanced strategy)*
*Human review recommended before final implementation*
*Generated: 2026-02-22*
