---
phase: 14-batch-performance
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - benchmarks/bench_fsm.py
  - pyproject.toml
  - .gitignore
autonomous: true

must_haves:
  truths:
    - "818 simulated files complete full UNTRACKED->UPLOADING->PROCESSING->INDEXED cycle at concurrency=1, =10, =50"
    - "Per-file timings recorded with three labeled segments: mock_api_ms, db_total_ms, fsm_net_ms"
    - "P50/P95/P99 per segment reported in Rich table and JSON output"
    - "Bottleneck identified as segment with highest P95 in fsm_net breakdown"
    - "Threshold 1 and Threshold 2 verdict printed as PASS or FAIL"
    - "WAL contention measured via lock_wait_ms and WAL file size delta"
  artifacts:
    - path: "benchmarks/bench_fsm.py"
      provides: "Standalone benchmark harness"
      min_lines: 200
    - path: ".gitignore"
      provides: "benchmarks/ directory excluded from git"
      contains: "benchmarks/"
    - path: "pyproject.toml"
      provides: "yappi added as dev dependency"
      contains: "yappi"
  key_links:
    - from: "benchmarks/bench_fsm.py"
      to: "src/objlib/upload/state.py"
      via: "AsyncUploadStateManager transition_to_* methods"
      pattern: "transition_to_uploading|transition_to_processing|transition_to_indexed"
    - from: "benchmarks/bench_fsm.py"
      to: "src/objlib/upload/fsm.py"
      via: "create_fsm for FSM validation before each transition"
      pattern: "create_fsm"
---

<objective>
Build the benchmark harness, run baseline measurements across 6 configurations (3 concurrency levels x 2 mock profiles), identify the bottleneck, and report Threshold 1/2 verdicts.

Purpose: VLID-06 requires measured (not estimated) FSM transition throughput under realistic batch conditions. This plan produces the harness and all baseline numbers.
Output: benchmarks/bench_fsm.py (standalone script), benchmarks/results-*.json (gitignored), yappi stats, Rich table output, bottleneck identification.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-batch-performance/14-CONTEXT.md
@.planning/phases/14-batch-performance/CLARIFICATIONS-ANSWERED.md

@src/objlib/upload/state.py
@src/objlib/upload/fsm.py
@src/objlib/upload/exceptions.py
@src/objlib/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add yappi dev dependency and create .gitignore entry</name>
  <files>pyproject.toml, .gitignore</files>
  <action>
1. Add yappi to dev dependencies in pyproject.toml:
   In `[project.optional-dependencies]` dev list, add `"yappi>=1.6"` (per locked decision Q5: yappi as primary async-aware profiler).

2. Install the updated dev dependencies:
   ```
   uv sync --dev
   ```

3. Create a root `.gitignore` if it does not exist, or append to it. Add these entries:
   ```
   benchmarks/results-*.json
   benchmarks/yappi-*.txt
   ```
   Per locked decision Q9: benchmark results are gitignored. The bench_fsm.py script itself IS committed (it is the harness, not output).

4. Create the `benchmarks/` directory:
   ```
   mkdir -p benchmarks
   ```
  </action>
  <verify>
   `uv run python -c "import yappi; print(yappi.__version__)"` prints a version number.
   `.gitignore` exists and contains `benchmarks/results-*.json`.
   `benchmarks/` directory exists.
  </verify>
  <done>yappi is importable, .gitignore excludes benchmark output files, benchmarks/ directory ready.</done>
</task>

<task type="auto">
  <name>Task 2: Build benchmark harness and run full baseline measurement</name>
  <files>benchmarks/bench_fsm.py</files>
  <action>
Create `benchmarks/bench_fsm.py` -- a standalone async benchmark script. This is a NEW file, not a modification of production code. The script must:

**A. Database setup (in-memory or temp file):**
- Create a temporary SQLite database (use tempfile.mkdtemp) with the V11 schema from `src/objlib/database.py` SCHEMA_SQL.
- Insert 818 simulated file rows with `gemini_state='untracked'`, `version=0`, unique `file_path` values like `/bench/file_0001.txt` through `/bench/file_0818.txt`.
- Use aiosqlite with WAL mode and PRAGMA synchronous=NORMAL (matching production).

**B. Mock adapter (per locked decision Q3 and Q8):**
- Create a `MockApiAdapter` class with `async def upload_file(self, file_path, display_name)` and `async def import_to_store(self, file_name, metadata)`.
- `upload_file` returns a mock file object with `.name = f"files/mock_{file_path}"` and `.uri = f"uri://mock_{file_path}"`.
- `import_to_store` returns a mock operation with `.done = True`, `.error = None`, `.response.name = f"doc_{file_path}"`.
- Both methods inject latency via `asyncio.sleep(delay)` where delay comes from the active profile:
  - `zero` profile: `delay = 0`
  - `realistic` profile: `delay = 2.0` (constant per locked decision Q3; seed=42 documented but not used since constant)
- The mock records cumulative sleep time per file for `mock_api_ms` computation.
- Delay is inside the mock adapter class, NOT inside FSM core (per locked decision Q3).

**C. Per-file lifecycle function `async def process_file(...):`**
Per locked decision Q8 (hydration approach): one FileLifecycleSM per file per cycle.

For each file, execute this sequence with explicit `time.perf_counter()` spans:

```python
async def process_file(file_path, version, db, mock_adapter, profile):
    timings = {}
    total_start = perf_counter()

    # 1. Hydrate FSM (per locked decision Q8)
    t = perf_counter()
    fsm = create_fsm("untracked")
    timings["fsm_hydrate_ms"] = (perf_counter() - t) * 1000

    # 2. Transition: untracked -> uploading
    t = perf_counter()
    fsm.start_upload()  # FSM validation only
    timings["fsm_dispatch_1_ms"] = (perf_counter() - t) * 1000

    t = perf_counter()
    # Use BEGIN IMMEDIATE for lock_wait measurement (per locked decision Q6)
    await db.execute("BEGIN IMMEDIATE")
    timings["lock_wait_1_ms"] = (perf_counter() - t) * 1000
    t_db = perf_counter()
    now = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.%f")
    new_version = version + 1
    cursor = await db.execute(
        """UPDATE files SET gemini_state='uploading',
           gemini_state_updated_at=?, version=?
           WHERE file_path=? AND gemini_state='untracked' AND version=?""",
        (now, new_version, file_path, version)
    )
    await db.commit()
    timings["db_write_1_ms"] = (perf_counter() - t_db) * 1000
    if cursor.rowcount == 0:
        raise RuntimeError(f"OCC conflict on {file_path}")
    version = new_version

    # 3. Mock API call: upload_file
    t = perf_counter()
    file_obj = await mock_adapter.upload_file(file_path, "display")
    timings["mock_upload_ms"] = (perf_counter() - t) * 1000

    # 4. Transition: uploading -> processing
    t = perf_counter()
    fsm.complete_upload()  # FSM validation
    timings["fsm_dispatch_2_ms"] = (perf_counter() - t) * 1000

    t = perf_counter()
    await db.execute("BEGIN IMMEDIATE")
    timings["lock_wait_2_ms"] = (perf_counter() - t) * 1000
    t_db = perf_counter()
    now = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.%f")
    new_version = version + 1
    cursor = await db.execute(
        """UPDATE files SET gemini_state='processing',
           gemini_file_id=?, gemini_file_uri=?,
           upload_timestamp=?, gemini_state_updated_at=?, version=?
           WHERE file_path=? AND gemini_state='uploading' AND version=?""",
        (file_obj.name, file_obj.uri, now, now, new_version, file_path, version)
    )
    await db.commit()
    timings["db_write_2_ms"] = (perf_counter() - t_db) * 1000
    version = new_version

    # 5. Mock API call: import_to_store
    t = perf_counter()
    operation = await mock_adapter.import_to_store(file_obj.name, [])
    timings["mock_import_ms"] = (perf_counter() - t) * 1000

    # 6. Transition: processing -> indexed
    t = perf_counter()
    fsm.complete_processing()  # FSM validation
    timings["fsm_dispatch_3_ms"] = (perf_counter() - t) * 1000

    t = perf_counter()
    await db.execute("BEGIN IMMEDIATE")
    timings["lock_wait_3_ms"] = (perf_counter() - t) * 1000
    t_db = perf_counter()
    now = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.%f")
    new_version = version + 1
    cursor = await db.execute(
        """UPDATE files SET gemini_state='indexed',
           gemini_store_doc_id=?, gemini_state_updated_at=?, version=?
           WHERE file_path=? AND gemini_state='processing' AND version=?""",
        (operation.response.name, now, new_version, file_path, version)
    )
    await db.commit()
    timings["db_write_3_ms"] = (perf_counter() - t_db) * 1000

    total_ms = (perf_counter() - total_start) * 1000

    # Compute aggregate segments (per locked decision Q1)
    timings["mock_api_ms"] = timings["mock_upload_ms"] + timings["mock_import_ms"]
    timings["db_total_ms"] = (timings["db_write_1_ms"] + timings["db_write_2_ms"]
                              + timings["db_write_3_ms"])
    timings["lock_wait_ms"] = (timings["lock_wait_1_ms"] + timings["lock_wait_2_ms"]
                               + timings["lock_wait_3_ms"])
    timings["fsm_dispatch_ms"] = (timings["fsm_dispatch_1_ms"]
                                  + timings["fsm_dispatch_2_ms"]
                                  + timings["fsm_dispatch_3_ms"])
    timings["total_wall_ms"] = total_ms
    timings["fsm_net_ms"] = total_ms - timings["mock_api_ms"]

    return timings
```

IMPORTANT NOTES on the DB writes above:
- The benchmark writes raw SQL matching the production `transition_to_*()` pattern (OCC guard via WHERE version=?, gemini_state check, version increment).
- This is intentional: we measure the ACTUAL DB pattern, not a simplified version.
- The `BEGIN IMMEDIATE` before each write is how aiosqlite acquires the WAL write lock; timing it separately measures lock_wait_ms (per locked decision Q6).
- NOTE: `AsyncUploadStateManager.transition_to_*()` methods do NOT use explicit `BEGIN IMMEDIATE` -- aiosqlite wraps each `execute()+commit()` in an implicit transaction. The benchmark should match this: do NOT use explicit `BEGIN IMMEDIATE`. Instead, just time `perf_counter()` around the full `execute()` + `commit()` pair for `db_write_N_ms`, and compute `lock_wait_ms` as the time for just the `execute()` call (which is where the lock is acquired). Remove the explicit `BEGIN IMMEDIATE` from the pseudocode above and match the production pattern:
  ```python
  t = perf_counter()
  cursor = await db.execute("UPDATE files SET ...")  # lock acquired here
  t_lock = perf_counter()
  timings["lock_wait_N_ms"] = (t_lock - t) * 1000
  await db.commit()
  timings["db_write_N_ms"] = (perf_counter() - t) * 1000
  ```

**D. Concurrency runner (per locked decision Q2):**
```python
async def run_benchmark(concurrency, profile, db_path):
    sem = asyncio.Semaphore(concurrency)
    all_timings = []

    async def worker(file_path, version):
        async with sem:
            async with aiosqlite.connect(db_path) as db:
                await db.execute("PRAGMA journal_mode=WAL")
                await db.execute("PRAGMA synchronous=NORMAL")
                mock = MockApiAdapter(profile)
                return await process_file(file_path, version, db, mock, profile)

    tasks = [worker(f"/bench/file_{i:04d}.txt", 0) for i in range(818)]
    results = await asyncio.gather(*tasks)
    return results
```

Each file gets its own aiosqlite connection (matching production: AsyncUploadStateManager creates one connection per context manager entry). The Semaphore limits concurrent workers.

**E. WAL measurement (per locked decision Q6):**
- Before benchmark: `wal_start = os.path.getsize(db_path + "-wal")` (handle FileNotFoundError -> 0)
- After benchmark: `wal_end = os.path.getsize(db_path + "-wal")`
- Record delta in results JSON.
- WAL contention verdict: P95 lock_wait_ms > 5% of P95 db_total_ms means WAL is a contributor.

**F. yappi integration (per locked decision Q5):**
```python
import yappi
yappi.set_clock_type("wall")  # wall time, not CPU
yappi.start()
# ... run benchmark ...
yappi.stop()
stats = yappi.get_func_stats()
stats.save(f"benchmarks/yappi-{timestamp}.txt", type="pstat")
# Also print top 20 functions by total time
stats.sort("ttot", "desc").print_all(out=sys.stdout, columns={...})
```

**G. Statistics computation:**
For each segment (mock_api_ms, db_total_ms, lock_wait_ms, fsm_dispatch_ms, fsm_net_ms, total_wall_ms):
- Compute P50, P95, P99 using `statistics` or `numpy` (prefer `statistics` to avoid adding a dependency).
- Use sorted list + index method: `P95 = sorted_values[int(0.95 * len(sorted_values))]`.

**H. Threshold evaluation (per locked decision Q4):**
```
Threshold 1 (FSM overhead gate):
  Profile: zero, Concurrency: 10
  Target: total elapsed time for 818 files <= 300 seconds (5 minutes)
  Also compute: transitions/sec = 3272 / elapsed_seconds
  Verdict: PASS if elapsed <= 300s, FAIL otherwise

Threshold 2 (Realistic upload gate):
  Profile: realistic, Concurrency: 10
  Target: total elapsed time for 818 files <= 21600 seconds (6 hours)
  Verdict: PASS if elapsed <= 21600s, FAIL otherwise
```

**I. Output (per locked decision Q9):**
1. Rich table to stdout with columns: Concurrency, Profile, Total Time, Trans/sec, P50/P95 for each segment, Threshold verdict.
2. JSON to `benchmarks/results-YYYYMMDD-HHMMSS.json` containing:
   ```json
   {
     "timestamp": "...",
     "seed": 42,
     "file_count": 818,
     "transitions_per_file": 4,
     "total_transitions": 3272,
     "configurations": [
       {
         "concurrency": 10,
         "profile": "zero",
         "elapsed_seconds": ...,
         "transitions_per_second": ...,
         "threshold_1_verdict": "PASS",
         "segments": {
           "mock_api_ms": {"p50": ..., "p95": ..., "p99": ...},
           "db_total_ms": {"p50": ..., "p95": ..., "p99": ...},
           "lock_wait_ms": {"p50": ..., "p95": ..., "p99": ...},
           "fsm_dispatch_ms": {"p50": ..., "p95": ..., "p99": ...},
           "fsm_net_ms": {"p50": ..., "p95": ..., "p99": ...}
         },
         "wal_size_start_bytes": ...,
         "wal_size_end_bytes": ...,
         "wal_contention_verdict": "..."
       },
       // ... 5 more configurations
     ],
     "bottleneck": {
       "segment": "db_total_ms",
       "evidence": "Highest P95 in fsm_net breakdown at concurrency=10 zero profile"
     }
   }
   ```
3. Bottleneck declaration: the segment with highest P95 in the fsm_net breakdown (excluding mock_api_ms) at the concurrency=10 zero-latency configuration is declared the bottleneck.

**J. Main entry point:**
```python
if __name__ == "__main__":
    asyncio.run(main())
```

The `main()` function:
1. Creates temp DB, inserts 818 files.
2. Records WAL size.
3. Runs 6 configurations in order: (1,zero), (1,realistic), (10,zero), (10,realistic), (50,zero), (50,realistic).
4. Between each configuration: reset all 818 files to `gemini_state='untracked', version=0` (simple UPDATE).
5. Computes stats, prints Rich table, writes JSON.
6. Declares bottleneck.
7. Cleans up temp directory.

**IMPORTANT:** The benchmark imports `create_fsm` from `src/objlib/upload/fsm.py` but does NOT import or use AsyncUploadStateManager -- it writes raw SQL matching the production pattern. This avoids coupling the benchmark to any future changes in the state manager while still measuring the exact same DB operations.

Add the project src to sys.path at top of script:
```python
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "src"))
```
  </action>
  <verify>
Run the benchmark:
```
uv run python benchmarks/bench_fsm.py
```
Confirm:
1. Rich table prints to stdout with 6 rows (3 concurrency x 2 profiles).
2. JSON file created in benchmarks/ directory.
3. Threshold 1 and Threshold 2 verdicts displayed.
4. Bottleneck segment declared.
5. All 818 files in each run reached indexed state (no OCC errors, no crashes).
6. yappi stats file created in benchmarks/.
  </verify>
  <done>
Benchmark harness runs 818 simulated files through full 4-transition FSM lifecycle across 6 configurations. P50/P95/P99 per timing segment reported. Threshold 1 (zero profile, concurrency=10, <=5min) and Threshold 2 (realistic profile, concurrency=10, <=6h) verdicts printed. Bottleneck segment identified by highest P95 in fsm_net breakdown. WAL contention measured. Results saved to JSON. yappi profile saved.
  </done>
</task>

</tasks>

<verification>
1. `uv run python benchmarks/bench_fsm.py` completes without error.
2. Rich table output shows 6 configuration rows with timing segments.
3. `benchmarks/results-*.json` contains valid JSON with all required fields.
4. `benchmarks/yappi-*.txt` contains yappi profile data.
5. Threshold verdicts are clearly labeled PASS or FAIL.
6. Bottleneck identification names a specific segment with evidence.
7. `git diff pyproject.toml` shows yappi in dev dependencies.
</verification>

<success_criteria>
VLID-06 Criterion 1: FSM transition throughput measured under simulated 818-file batch with transitions/sec, total elapsed, and P95 per-transition latency recorded.
VLID-06 Criterion 2 (partial): Bottleneck identified with before measurements.
VLID-06 Criterion 3 (partial): Acceptable throughput thresholds defined and baseline verdict reported.
</success_criteria>

<output>
After completion, create `.planning/phases/14-batch-performance/14-01-SUMMARY.md`
</output>
