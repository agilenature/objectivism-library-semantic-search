---
phase: 14-batch-performance
plan: 02
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - benchmarks/bench_fsm.py
autonomous: true

must_haves:
  truths:
    - "If baseline passed both thresholds: confirmation run at all 3 concurrency levels documents VLID-06 as passed with zero mitigation needed"
    - "If baseline failed Threshold 1: at least one mitigation tested with before/after P95 comparison"
    - "VLID-06 declared PASS or FAIL with explicit evidence"
    - "Mitigation order respected: guard checks -> connection reuse -> batch writes (stop at first success)"
  artifacts:
    - path: "benchmarks/bench_fsm.py"
      provides: "Benchmark harness (possibly updated with mitigation flags)"
      min_lines: 200
  key_links:
    - from: ".planning/phases/14-batch-performance/14-02-SUMMARY.md"
      to: ".planning/phases/14-batch-performance/14-01-SUMMARY.md"
      via: "References baseline measurements for before/after comparison"
      pattern: "baseline|before.*after|threshold"
---

<objective>
Based on Plan 14-01 baseline results: either confirm VLID-06 passes with zero mitigation, or implement mitigations in complexity order until thresholds are met. Produce the final VLID-06 verdict.

Purpose: VLID-06 requires "at least one mitigation tested with before/after measurements." If the baseline already passes, this plan documents that fact and runs confirmation at other concurrency levels. If the baseline fails, this plan implements and measures mitigations.
Output: Final VLID-06 verdict (PASS/FAIL), before/after comparison if mitigations were needed, updated benchmark results.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-batch-performance/14-CONTEXT.md
@.planning/phases/14-batch-performance/CLARIFICATIONS-ANSWERED.md
@.planning/phases/14-batch-performance/14-01-SUMMARY.md

@src/objlib/upload/state.py
@src/objlib/upload/fsm.py
@benchmarks/bench_fsm.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Evaluate baseline and execute appropriate path (confirmation or mitigation)</name>
  <files>benchmarks/bench_fsm.py</files>
  <action>
Read `14-01-SUMMARY.md` to get baseline results. Then follow ONE of two paths:

**PATH A: Baseline PASSED both thresholds (expected path)**

If 14-01 shows Threshold 1 PASS and Threshold 2 PASS:

1. Document the baseline pass in detail:
   - Record the exact elapsed time and transitions/sec for concurrency=10 zero profile (Threshold 1).
   - Record the exact elapsed time for concurrency=10 realistic profile (Threshold 2).
   - Record the bottleneck segment and its P95 value.
   - Record WAL contention verdict.

2. Run confirmation measurements at concurrency=1 and concurrency=50 to verify the bottleneck characterization holds across contention levels:
   ```
   uv run python benchmarks/bench_fsm.py
   ```
   (The harness already runs all 6 configurations. If 14-01 already ran all 6, simply reference those results -- do not re-run unless there's a reason to.)

3. Analyze cross-concurrency scaling:
   - Does fsm_net_ms P95 increase linearly with concurrency? Sub-linearly? Non-linearly?
   - Does the bottleneck segment change at concurrency=50 vs concurrency=10?
   - Does lock_wait_ms increase significantly at concurrency=50?
   - Document the scaling behavior.

4. Per locked decision Q4 sub-decision: "If both thresholds are met on baseline (no mitigation needed): document and declare VLID-06 passed."
   - The "at least one mitigation tested" VLID-06 requirement is satisfied by documenting that no mitigation was needed because baseline already passed with margin.
   - If desired for completeness, run ONE quick mitigation experiment (e.g., single persistent connection instead of per-file connections) to show the delta, but this is NOT required if baseline passed.

5. Declare VLID-06 PASS with explicit evidence:
   - Threshold 1: X seconds (target: <=300s) -- PASS
   - Threshold 2: X seconds (target: <=21600s) -- PASS
   - Bottleneck: [segment] at P95=[value]ms
   - WAL contention: [verdict]
   - Mitigation needed: No

**PATH B: Baseline FAILED one or both thresholds**

If 14-01 shows Threshold 1 FAIL or Threshold 2 FAIL:

Follow the mitigation order from locked decision Q7 (stop at first success):

**Step 1: Remove DB-reading guards (in-memory only)**
- Examine the benchmark's process_file function. The FSM `create_fsm()` call reads no DB -- it's already in-memory. The `fsm.start_upload()` / `fsm.complete_upload()` / `fsm.complete_processing()` calls are pure Python (no I/O).
- Check if any guard in the production `transition_to_*()` SQL reads state before writing. The current pattern uses `WHERE gemini_state = 'X' AND version = ?` which is a write-path guard (part of the UPDATE), not a separate read query. So this mitigation may be a no-op.
- If there IS a separate SELECT before UPDATE anywhere: remove it, use in-memory state from the FSM instead.
- Re-run benchmark at concurrency=10 zero profile. Compare P95 before/after.
- If Threshold 1 now passes: stop, declare VLID-06 PASS with this mitigation.

**Step 2: Single persistent aiosqlite connection**
- Modify the benchmark's `run_benchmark()` to use ONE shared `aiosqlite.connect()` for all workers instead of one connection per file.
- The Semaphore still limits concurrency, but all workers share the connection object.
- IMPORTANT: aiosqlite connections are NOT thread-safe but ARE asyncio-safe (single event loop). A shared connection with a Semaphore is valid.
- Re-run benchmark at concurrency=10 zero profile. Compare P95 before/after.
- If Threshold 1 now passes: stop, declare VLID-06 PASS with this mitigation.
- NOTE: If this mitigation works, document that production `AsyncUploadStateManager` should use a single connection for the full batch instead of reconnecting per operation. This is a recommendation for Phase 16, not a change to production code in this phase.

**Step 3: Batch DB writes (only if Steps 1+2 failed)**
- Modify the benchmark to batch commits: instead of committing after each transition, accumulate writes and commit every 10 transitions.
- Still check OCC per-row (the WHERE clause remains).
- DOCUMENT the OCC atomicity risk: if a batch commit fails, up to 10 files may have inconsistent state.
- Re-run benchmark at concurrency=10 zero profile. Compare P95 before/after.
- If Threshold 1 now passes: declare VLID-06 PASS with this mitigation + documented risk.

**For any mitigation path:**
- Save mitigation results to a separate JSON file: `benchmarks/results-mitigation-YYYYMMDD-HHMMSS.json`.
- Print before/after Rich table comparison.
- The "before" numbers come from 14-01 baseline.

**VLID-06 FINAL VERDICT:**
If after all 3 mitigation steps, Threshold 1 still fails: declare VLID-06 FAIL and document what was tried. This blocks Phase 15.
  </action>
  <verify>
1. If PATH A: `14-01-SUMMARY.md` baseline numbers are referenced. Cross-concurrency analysis documented. VLID-06 verdict is PASS.
2. If PATH B: At least one mitigation has been tested. Before/after P95 comparison is documented. `benchmarks/results-mitigation-*.json` exists with mitigation results (or baseline re-run confirms). VLID-06 verdict is PASS or FAIL with explicit evidence.
3. In either path: the SUMMARY.md contains the VLID-06 verdict with all three criteria explicitly addressed:
   - Criterion 1: Throughput measured (transitions/sec, elapsed, P95 per transition)
   - Criterion 2: Bottleneck identified with evidence
   - Criterion 3: Threshold defined and met (or mitigation applied)
  </verify>
  <done>
VLID-06 is declared PASS or FAIL with:
- Measured throughput numbers (transitions/sec, P95 latency, total elapsed)
- Identified bottleneck segment with P95 evidence
- Explicit threshold verdicts (Threshold 1: PASS/FAIL, Threshold 2: PASS/FAIL)
- If mitigation was needed: before/after comparison with the specific mitigation that resolved it
- If no mitigation needed: documentation that baseline passed with headroom
  </done>
</task>

<task type="auto">
  <name>Task 2: Write VLID-06 gate verdict and Phase 14 completion artifacts</name>
  <files>benchmarks/bench_fsm.py</files>
  <action>
This task ensures all Phase 14 artifacts are complete for the gate review.

1. Verify the benchmark script `benchmarks/bench_fsm.py` is clean, well-commented, and runnable:
   ```
   uv run python benchmarks/bench_fsm.py
   ```
   (This is a re-run to confirm reproducibility. If it was already run in Task 1, just verify the existing results are consistent.)

2. Verify all existing tests still pass (benchmark must not break production code):
   ```
   uv run pytest tests/ -x -q
   ```

3. Prepare VLID-06 gate summary for the SUMMARY.md (the executor writes the SUMMARY):

   **VLID-06 Gate Criteria Checklist:**
   - [ ] SC1: FSM transition throughput measured under 818-file simulated batch -- transitions/sec, total elapsed, P95 per-transition latency recorded
   - [ ] SC2: Bottleneck identified (name the segment) with at least one mitigation tested (or documented as unnecessary) -- with before/after measurements (or baseline-only if sufficient)
   - [ ] SC3: Acceptable throughput threshold defined explicitly (Threshold 1: <=5min zero profile; Threshold 2: <=6h realistic profile) and current measured throughput meets it

   All three criteria must be PASS for Phase 14 gate to pass and unblock Phase 15.

4. If VLID-06 is PASS:
   - Confirm Phase 15 is unblocked.
   - Note any recommendations for Phase 16 (e.g., connection reuse, expected upload time).

5. If VLID-06 is FAIL:
   - Document what was tried and what failed.
   - Phase 15 remains BLOCKED.
   - Recommend next steps (deeper investigation, different approach).
  </action>
  <verify>
1. `uv run pytest tests/ -x -q` passes (all existing tests green).
2. `uv run python benchmarks/bench_fsm.py` runs without error.
3. SUMMARY.md contains explicit VLID-06 gate verdict with all 3 criteria addressed.
  </verify>
  <done>
Phase 14 gate artifacts complete. VLID-06 verdict documented with evidence. All existing tests pass (benchmark did not break production code). Phase 15 status (UNBLOCKED or still BLOCKED) is clear.
  </done>
</task>

</tasks>

<verification>
1. VLID-06 Criterion 1: Throughput numbers exist in SUMMARY.md with transitions/sec, elapsed, P95.
2. VLID-06 Criterion 2: Bottleneck segment named with P95 evidence. At least one mitigation addressed (tested or documented as unnecessary).
3. VLID-06 Criterion 3: Thresholds defined (<=5min zero, <=6h realistic) and verdict is PASS or FAIL.
4. `uv run pytest tests/ -x -q` passes.
5. Benchmark results in benchmarks/ directory (JSON + yappi stats).
</verification>

<success_criteria>
VLID-06 declared PASS or FAIL with explicit evidence for all 3 criteria. If PASS: Phase 15 unblocked. If FAIL: blockers documented with next steps. All existing tests still pass.
</success_criteria>

<output>
After completion, create `.planning/phases/14-batch-performance/14-02-SUMMARY.md`
</output>
