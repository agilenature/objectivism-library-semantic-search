---
phase: 05-incremental-updates-offline-mode
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - src/objlib/sync/detector.py
  - src/objlib/sync/orchestrator.py
  - src/objlib/sync/__init__.py
  - src/objlib/cli.py
autonomous: true

must_haves:
  truths:
    - "sync detects new files, uploads only new, existing untouched"
    - "sync detects content hash changes, removes old Gemini entry, uploads new version"
    - "sync detects deletions, marks them as 'missing' with timestamp (not auto-delete)"
    - "sync --force re-processes all files regardless of change detection"
    - "sync --prune-missing deletes missing files (>7 days old) from Gemini store"
    - "sync --dry-run shows what would happen without executing"
    - "sync --skip-enrichment uses simple upload pipeline instead of enriched"
    - "sync --cleanup-orphans removes orphaned Gemini entries"
    - "Orphan cleanup runs automatically on each sync startup"
    - "mtime optimization skips hash computation for unchanged files"
    - "Upload-first atomicity: new version uploaded before old deleted"
    - "Library config verified on sync startup (store name mismatch = abort)"
  artifacts:
    - path: "src/objlib/sync/detector.py"
      provides: "SyncDetector with mtime-optimized change detection"
      exports: ["SyncDetector"]
    - path: "src/objlib/sync/orchestrator.py"
      provides: "SyncOrchestrator coordinating detect-upload-mark-cleanup"
      exports: ["SyncOrchestrator"]
    - path: "src/objlib/cli.py"
      provides: "sync CLI command with all flags"
      contains: "def sync"
  key_links:
    - from: "src/objlib/sync/detector.py"
      to: "src/objlib/scanner.py"
      via: "Reuses FileScanner.compute_hash and discover_files"
      pattern: "FileScanner"
    - from: "src/objlib/sync/orchestrator.py"
      to: "src/objlib/upload/orchestrator.py"
      via: "Extracts enrichment logic into per-file _build_file_upload_data helper (EnrichedUploadOrchestrator is batch-oriented, SyncOrchestrator needs per-file control)"
      pattern: "_build_file_upload_data|build_enriched_metadata|prepare_enriched_content"
    - from: "src/objlib/sync/orchestrator.py"
      to: "src/objlib/upload/client.py"
      via: "Uses delete_store_document for orphan/prune cleanup"
      pattern: "delete_store_document|find_store_document_name"
    - from: "src/objlib/sync/orchestrator.py"
      to: "src/objlib/database.py"
      via: "mark_missing, get_orphaned_files, set_library_config, get_all_active_files_with_mtime"
      pattern: "mark_missing|get_orphaned|library_config"
    - from: "src/objlib/cli.py"
      to: "src/objlib/sync/orchestrator.py"
      via: "sync command calls SyncOrchestrator.run()"
      pattern: "SyncOrchestrator"
---

<objective>
Sync command core: change detection with mtime optimization, upload-first atomic replacement, missing file marking, orphan cleanup, and CLI integration with all flags.

Purpose: This is the primary deliverable of Phase 5 -- the sync command that enables incremental updates. It composes existing primitives (FileScanner for discovery, enrichment helpers for upload, GeminiFileSearchClient for store operations) into a new sync workflow.

Output: SyncDetector (mtime-optimized change detection), SyncOrchestrator (full sync pipeline), and `sync` CLI command with --force, --prune-missing, --dry-run, --skip-enrichment, --cleanup-orphans flags.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-incremental-updates-offline-mode/05-01-SUMMARY.md
@.planning/phases/05-incremental-updates-offline-mode/05-02-SUMMARY.md
@src/objlib/sync/disk.py
@src/objlib/database.py
@src/objlib/scanner.py
@src/objlib/upload/client.py
@src/objlib/upload/orchestrator.py
@src/objlib/upload/metadata_builder.py
@src/objlib/upload/content_preparer.py
@src/objlib/upload/state.py
@src/objlib/models.py
@src/objlib/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: SyncDetector with mtime-optimized change detection</name>
  <files>src/objlib/sync/detector.py</files>
  <action>
Create `src/objlib/sync/detector.py` with a `SyncDetector` class that wraps FileScanner for mtime-optimized change detection.

**SyncDetector class:**

```python
@dataclass
class SyncChangeSet:
    """Result of sync change detection with mtime optimization."""
    new_files: list[dict]         # [{file_path, content_hash, mtime, file_size, metadata_json, metadata_quality}]
    modified_files: list[dict]    # Same shape, plus old_gemini_file_id
    missing_files: set[str]       # file_paths in DB but not on disk
    unchanged_count: int          # Count of skipped files
    mtime_skipped_count: int      # Files skipped by mtime optimization (subset of unchanged)

    @property
    def summary(self) -> str:
        return (f"new={len(self.new_files)}, modified={len(self.modified_files)}, "
                f"missing={len(self.missing_files)}, unchanged={self.unchanged_count}, "
                f"mtime_skipped={self.mtime_skipped_count}")
```

**SyncDetector(config: ScannerConfig, db: Database, metadata_extractor: MetadataExtractor):**

`detect_changes(force: bool = False) -> SyncChangeSet:`
1. Discover eligible files using FileScanner.discover_files() (reuse the scanner's discovery logic, not duplicating it -- instantiate a FileScanner internally)
2. Load all active files from DB via `db.get_all_active_files_with_mtime()` to get {path: (hash, size, mtime)}
3. Compute sets: scan_paths, db_paths, new = scan - db, missing = db - scan, common = scan & db
4. For each file in `common`:
   a. Get current mtime via `os.stat(path).st_mtime`
   b. If NOT force AND db_mtime is not None AND current_mtime == db_mtime (within 1e-6 epsilon for float comparison per research pitfall #5): skip (mtime_skipped++)
   c. Else: compute SHA-256 hash via FileScanner.compute_hash()
   d. If hash differs from db_hash: add to modified_files with old_gemini_file_id from DB
   e. Else: unchanged (update mtime in DB if it changed -- store current mtime)
5. For each file in `new`:
   - Compute hash, extract metadata, build file info dict
6. If `force=True`: treat ALL common files as needing hash check (skip mtime optimization), and re-upload any file whose enrichment_version differs from the current app version OR whose content_hash differs

Return SyncChangeSet.

**Helper to compute current enrichment version:**
```python
def compute_current_enrichment_version() -> str:
    """Compute a short hash of the current enrichment config.

    Per locked decision #8: sha256(prompt_template + model_name + schema_version)[:8]
    """
    # Import the prompt template and model from the extraction config
    # Use a stable representation of the enrichment pipeline version
    import hashlib
    # Components that affect enrichment output:
    version_string = "enriched_upload_v1"  # Bump when enrichment logic changes
    return hashlib.sha256(version_string.encode()).hexdigest()[:8]
```

Store this as a module-level constant `CURRENT_ENRICHMENT_VERSION` computed at import time.

**IMPORTANT:** Do not re-implement file discovery -- create a FileScanner instance internally and call discover_files(). The SyncDetector adds mtime optimization ON TOP of the existing scanner primitives.

For metadata extraction of new files: reuse MetadataExtractor.extract() exactly as scanner.py does.
  </action>
  <verify>
Run: `python -c "from objlib.sync.detector import SyncDetector, SyncChangeSet, CURRENT_ENRICHMENT_VERSION; print('SyncDetector OK, enrichment_version:', CURRENT_ENRICHMENT_VERSION)"`

Run: `python -c "from objlib.sync.detector import SyncChangeSet; cs = SyncChangeSet([], [], set(), 100, 80); print(cs.summary)"` -- should print summary string.
  </verify>
  <done>
SyncDetector.detect_changes() returns SyncChangeSet with new, modified, missing, unchanged categorization. mtime optimization skips hash for unmodified files. CURRENT_ENRICHMENT_VERSION computed from enrichment config hash. FileScanner primitives reused (no duplication).
  </done>
</task>

<task type="auto">
  <name>Task 2: SyncOrchestrator and CLI sync command</name>
  <files>src/objlib/sync/orchestrator.py, src/objlib/sync/__init__.py, src/objlib/cli.py</files>
  <action>
**src/objlib/sync/orchestrator.py:**

Create `SyncOrchestrator` class that coordinates the full sync pipeline.

```python
class SyncOrchestrator:
    """Orchestrates incremental sync: detect -> upload -> mark -> cleanup.

    Composes SyncDetector (change detection) and GeminiFileSearchClient
    (store operations). Re-implements per-file enriched upload logic via
    a _build_file_upload_data helper rather than composing
    EnrichedUploadOrchestrator, because EnrichedUploadOrchestrator is
    batch-oriented and SyncOrchestrator needs per-file control for
    upload-first atomicity and individual error handling.

    Per locked decisions:
    - Upload-first atomicity (#1): Upload new version before deleting old
    - Enriched by default (#3): Uses EnrichedUploadOrchestrator
    - Mark-missing, never auto-delete (#4): Missing files marked, not deleted
    - Per-file commits (#5): Each file committed individually
    - Store-level deletion (#6): Uses delete_store_document, 404=success
    """
```

**Constructor:** Takes Database, GeminiFileSearchClient, ScannerConfig, UploadConfig, and optional Rich console.

**async run() method with these parameters:**
- `force: bool = False`
- `skip_enrichment: bool = False`
- `dry_run: bool = False`
- `prune_missing: bool = False`
- `cleanup_orphans: bool = False`
- `prune_age_days: int = 7`

**run() pipeline:**

1. **Verify library config** (locked decision #13):
   - Check `db.get_library_config("gemini_store_name")`
   - If set and doesn't match current store_name: abort with clear error
   - If not set: store current store_name via `db.set_library_config("gemini_store_name", store_name)`

2. **Auto-cleanup orphans on startup** (locked decision #1 sub-decision):
   - Call `db.get_orphaned_files()`
   - For each: `find_store_document_name(orphaned_id)` then `delete_store_document(doc_name)`
   - On success: `db.clear_orphan(file_path)`
   - Log summary: "Cleaned up N orphaned Gemini entries"

3. **Run change detection:**
   - Create SyncDetector and call `detect_changes(force=force)`
   - Display summary via Rich console

4. **Dry-run mode:**
   - If dry_run: print what would happen (N new, N modified, N missing) and return
   - Include prune preview if prune_missing: show files that would be pruned

5. **Upload new files** (locked decision #3 -- enriched by default):
   - For each new file in changeset:
     a. If NOT skip_enrichment: use enriched upload pipeline (build metadata via build_enriched_metadata, prepare content via prepare_enriched_content, upload via client.upload_and_import)
     b. If skip_enrichment: use simple upload (client.upload_and_import with basic metadata via build_custom_metadata)
     c. On success: update DB record (status='uploaded', gemini_file_id, upload_timestamp, mtime, upload_hash, enrichment_version)
     d. On failure: mark status='failed', log error, continue to next file (locked decision #5)
     e. Commit after each file (per-file commits pattern)
   - Use Semaphore(2) for conservative concurrency (consistent with enriched upload)
   - Max 3 retries with exponential backoff per file

6. **Upload-first replacement for modified files** (locked decision #1):
   - For each modified file:
     a. Upload new version (same enriched/simple logic as new files)
     b. On upload success: update DB with new gemini_file_id, store OLD gemini_file_id in orphaned_gemini_file_id column
     c. Attempt deletion of old store document: find_store_document_name(old_id) then delete_store_document()
     d. On delete success: clear orphaned_gemini_file_id
     e. On delete failure (non-404): leave orphaned_gemini_file_id for next startup cleanup -- fire-and-forget
     f. Commit after each file

7. **Mark missing files** (locked decision #4):
   - Only if disk availability is 'available' (CRITICAL safety check -- recheck before marking)
   - Call `db.mark_missing(changeset.missing_files)`
   - Log: "Marked N files as missing"

8. **Prune missing if requested:**
   - If prune_missing: get files with status='missing' and missing_since > prune_age_days days old
   - For each: find_store_document_name() then delete_store_document()
   - On success: update status to 'LOCAL_DELETE'
   - Log summary

9. **Explicit orphan cleanup if requested:**
   - If cleanup_orphans: same logic as step 2 (auto-cleanup)

10. **Report summary** via Rich table: new uploaded, modified replaced, missing marked, orphans cleaned, pruned, errors.

**IMPORTANT IMPLEMENTATION NOTE -- Per-file enrichment via helper, not batch orchestrator:**
EnrichedUploadOrchestrator is designed for batch operations (processing all pending files at once). SyncOrchestrator needs per-file control for upload-first atomicity and individual error handling/retry. Therefore, extract the enrichment logic into a `_build_file_upload_data(file_path, db)` helper method that:
- Queries file_metadata_ai and transcript_entity tables (same data EnrichedUploadOrchestrator._upload_enriched_file reads)
- Calls build_enriched_metadata() and prepare_enriched_content() from the upload submodules
- Returns (custom_metadata, enriched_file_path_or_None, upload_hash)
This re-uses the same upload utility functions (build_enriched_metadata, prepare_enriched_content) but gives SyncOrchestrator per-file control flow.

**AI metadata fallback** (locked decision #3): If AI metadata lookup fails (no extraction exists for a file), fall back to Phase 1 metadata via build_custom_metadata(). Log a warning but DO NOT skip the file.

**Other implementation notes:**
- Use aiosqlite for async DB access in the upload loop, consistent with the existing upload pipeline pattern.
- Import heavy dependencies (genai, upload modules) only inside the sync function, not at module top level, for fast CLI startup.

**src/objlib/sync/__init__.py:**
Update to export SyncOrchestrator and SyncDetector.

**src/objlib/cli.py:**
Add a `sync` command to the app:

```python
@app.command()
def sync(
    library_path: Annotated[Path | None, typer.Option("--library", "-l", help="Path to library root")] = None,
    store_name: Annotated[str, typer.Option("--store", "-s", help="Gemini File Search store name")] = "objectivism-library-test",
    db_path: Annotated[Path, typer.Option("--db", "-d", help="Path to SQLite database")] = Path("data/library.db"),
    force: Annotated[bool, typer.Option("--force", help="Re-process all files regardless of change detection")] = False,
    skip_enrichment: Annotated[bool, typer.Option("--skip-enrichment", help="Use simple upload (skip AI metadata enrichment)")] = False,
    dry_run: Annotated[bool, typer.Option("--dry-run", help="Preview changes without executing")] = False,
    prune_missing: Annotated[bool, typer.Option("--prune-missing", help="Delete files missing >7 days from Gemini")] = False,
    cleanup_orphans: Annotated[bool, typer.Option("--cleanup-orphans", help="Remove orphaned Gemini entries")] = False,
) -> None:
    """Detect changes and sync with Gemini File Search store.

    Detects new, modified, and deleted files by comparing the library
    directory against SQLite state. Uploads new/modified files using the
    enriched pipeline (with AI metadata). Marks deleted files as 'missing'
    without auto-deleting from Gemini.

    Renamed files are treated as delete+add (path-based identity).
    """
```

Implementation:
1. Check disk availability via `check_disk_availability(str(library_path or default_path))`. If unavailable: print error via `disk_error_message()` and exit(1).
2. Resolve library_path from config or CLI
3. Load API key from keyring (same pattern as enriched_upload command)
4. Initialize GeminiFileSearchClient with circuit breaker and rate limiter (same pattern as upload command)
5. Create SyncOrchestrator and call `asyncio.run(orchestrator.run(...))`
6. Display results

**CRITICAL:** The sync command must use `--store objectivism-library-test` as default (matching enriched upload), NOT `objectivism-library-v1` (which is the basic upload default). Per locked decision #13, the library_config table will catch mismatches.
  </action>
  <verify>
Run: `python -m objlib sync --help` -- should show all flags (--force, --dry-run, --prune-missing, --skip-enrichment, --cleanup-orphans, --library, --store, --db).

Run: `python -m objlib sync --dry-run --db data/library.db` -- should either show disk unavailable error (if USB not connected) or show dry-run preview of changes.

Run: `python -c "from objlib.sync.orchestrator import SyncOrchestrator; print('SyncOrchestrator imported OK')"`.

Run: `python -c "from objlib.sync import SyncOrchestrator, SyncDetector, check_disk_availability; print('All sync exports OK')"`.
  </verify>
  <done>
SyncOrchestrator.run() executes the full pipeline: verify config -> cleanup orphans -> detect changes -> upload new (enriched) -> replace modified (upload-first) -> mark missing -> prune (optional) -> report. CLI `sync` command works with all flags. Disk availability checked at command start. Library config verified on startup. Per-file commits ensure crash recovery.
  </done>
</task>

</tasks>

<verification>
- `python -m objlib sync --help` shows all flags and help text
- `python -c "from objlib.sync.detector import SyncDetector, SyncChangeSet; print('PASS')"` imports succeed
- `python -c "from objlib.sync.orchestrator import SyncOrchestrator; print('PASS')"` imports succeed
- `python -c "from objlib.sync import SyncOrchestrator, SyncDetector, check_disk_availability; print('PASS')"` all exports work
- If USB connected: `python -m objlib sync --dry-run --db data/library.db --library "/Volumes/U32 Shadow/Objectivism Library"` shows change detection summary
- If USB disconnected: `python -m objlib sync --db data/library.db` shows clear disk unavailable error
</verification>

<success_criteria>
1. sync --dry-run detects new/modified/missing files and displays summary without making changes
2. sync uploads new files using enriched pipeline by default (with --skip-enrichment fallback)
3. sync replaces modified files using upload-first atomicity (new uploaded before old deleted)
4. sync marks missing files as status='missing' with missing_since timestamp
5. sync --prune-missing deletes missing files (>7 days) from Gemini store
6. sync --force re-processes all files regardless of mtime/hash
7. sync --cleanup-orphans explicitly cleans orphaned Gemini entries
8. Orphan cleanup runs automatically on sync startup
9. Library config store name verified on startup (mismatch = abort)
10. Per-file SQLite commits for crash recovery
11. mtime optimization reduces hash computations for unchanged files
12. Disk availability checked before any filesystem operations
</success_criteria>

<output>
After completion, create `.planning/phases/05-incremental-updates-offline-mode/05-03-SUMMARY.md`
</output>
