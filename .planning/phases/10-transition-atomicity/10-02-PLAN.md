---
phase: 10-transition-atomicity
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - spike/phase10_spike/recovery_crawler.py
  - spike/phase10_spike/tests/test_recovery.py
  - spike/phase10_spike/tests/test_failed_escape.py
  - spike/phase10_spike/tests/test_sc3_simplicity.py
  - spike/phase10_spike/harness.py
autonomous: true

must_haves:
  truths:
    - "A file left in partial-intent state (any crash point) is automatically recovered to 'untracked' by the startup recovery crawler"
    - "A file in 'failed' state can be transitioned to 'untracked' via explicit retry (no manual SQL)"
    - "Recovery code is demonstrably simpler than transition code (fewer lines, no retry loops)"
    - "The combined harness produces structured JSON evidence of all Phase 10 success criteria"
  artifacts:
    - path: "spike/phase10_spike/recovery_crawler.py"
      provides: "RecoveryCrawler with startup_recovery() scan and per-file linear step resumption"
      contains: "recover_all"
    - path: "spike/phase10_spike/tests/test_recovery.py"
      provides: "Recovery tests for all 3 crash points + empty DB edge case"
      min_lines: 60
    - path: "spike/phase10_spike/tests/test_failed_escape.py"
      provides: "FAILED->UNTRACKED retry transition test"
      min_lines: 20
    - path: "spike/phase10_spike/tests/test_sc3_simplicity.py"
      provides: "SC3 line count measurement: recovery lines <= transition lines"
      min_lines: 15
    - path: "spike/phase10_spike/harness.py"
      provides: "Combined harness with 6 evidence checks and structured JSON output"
      contains: "ALL CHECKS PASSED"
  key_links:
    - from: "spike/phase10_spike/recovery_crawler.py"
      to: "spike/phase10_spike/safe_delete.py"
      via: "Uses safe_delete wrappers for idempotent re-deletion"
      pattern: "safe_delete_store_document|safe_delete_file"
    - from: "spike/phase10_spike/recovery_crawler.py"
      to: "spike/phase10_spike/db.py"
      via: "Scans intent_type IS NOT NULL, calls update_progress + finalize_reset"
      pattern: "intent_type IS NOT NULL"
    - from: "spike/phase10_spike/tests/test_recovery.py"
      to: "spike/phase10_spike/transition_reset.py"
      via: "Creates crash state via ResetTransitionManager, then recovers via RecoveryCrawler"
      pattern: "RecoveryCrawler.*recover_all"
---

<objective>
Implement the RecoveryCrawler for startup recovery, FAILED state escape path, SC3 simplicity measurement, and combined evidence harness -- completing all Phase 10 success criteria.

Purpose: Plan 10-01 proved that crash points leave deterministic partial state. This plan proves those partial states are automatically recoverable, that no file can get stuck requiring manual SQL, and that the recovery code is simpler than the transition code it compensates for.

Output: RecoveryCrawler module, FAILED->UNTRACKED retry, SC3 measurement test, combined harness with structured JSON evidence for Phase 10 gate assessment.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-transition-atomicity/10-CONTEXT.md
@.planning/phases/10-transition-atomicity/10-RESEARCH.md
@.planning/phases/10-transition-atomicity/10-01-SUMMARY.md

# Phase 10 spike code (from Plan 10-01)
@spike/phase10_spike/db.py
@spike/phase10_spike/states.py
@spike/phase10_spike/safe_delete.py
@spike/phase10_spike/transition_reset.py
@spike/phase10_spike/tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RecoveryCrawler, FAILED escape, and SC3 measurement tests</name>
  <files>
    spike/phase10_spike/recovery_crawler.py
    spike/phase10_spike/tests/test_recovery.py
    spike/phase10_spike/tests/test_failed_escape.py
    spike/phase10_spike/tests/test_sc3_simplicity.py
  </files>
  <action>
    **recovery_crawler.py** -- RecoveryCrawler class (GA-3):
    - Constructor takes: `db_path: str`, `delete_store_doc_fn: Callable`, `delete_file_fn: Callable`
    - `async def recover_all(self) -> list[str]`:
      1. Scan DB: `SELECT file_path, intent_type, intent_api_calls_completed, gemini_store_doc_id, gemini_file_id, version FROM files WHERE intent_type IS NOT NULL ORDER BY intent_started_at ASC`
      2. For each row, call `_recover_file(row)` -- linear step resumption, NO retry loops (GA-9)
      3. Return list of recovered file_path strings
      4. Log INFO per recovered file (GA-8): `logger.info("Recovered %s: intent=%s, resumed_from_step=%d", file_path, intent_type, api_calls_completed)`
    - `async def _recover_file(self, row: dict) -> None`:
      Linear step resumption based on `intent_api_calls_completed`:
      - If completed < 1: call safe_delete_store_document(self._delete_store_doc_fn, row["gemini_store_doc_id"]), then update_progress(db_path, file_path, 1)
      - If completed < 2: call safe_delete_file(self._delete_file_fn, row["gemini_file_id"]), then update_progress(db_path, file_path, 2)
      - Always: call finalize_reset(db_path, file_path, row["version"])
      - No try/except around recovery steps. If safe_delete fails with non-404 error, it propagates (file transitions to FAILED in production, but spike lets it crash for visibility).
    - Import safe_delete_store_document, safe_delete_file from spike.phase10_spike.safe_delete
    - Import update_progress, finalize_reset from spike.phase10_spike.db
    - Use standard logging: `logger = logging.getLogger(__name__)`

    **FAILED->UNTRACKED retry** -- Add a function (can be in recovery_crawler.py or a separate retry module):
    - `async def retry_failed_file(db_path: str, file_path: str) -> bool`:
      1. Open fresh aiosqlite connection with WAL + BEGIN IMMEDIATE
      2. Execute: `UPDATE files SET gemini_state='untracked', gemini_file_id=NULL, gemini_store_doc_id=NULL, intent_type=NULL, intent_started_at=NULL, intent_api_calls_completed=NULL, version=version+1, gemini_state_updated_at=? WHERE file_path=? AND gemini_state='failed'` with current ISO timestamp
      3. Return True if rowcount==1, False otherwise
    - This implements GA-4: FAILED -> UNTRACKED escape path. No auto-retry from crawler (prevents loops).

    **tests/test_recovery.py** -- 4 tests proving automatic recovery:
    1. `test_recovery_crash_point_1` -- Seed indexed file, run ResetTransitionManager with crash at API call 1 (same as 10-01 test_crash_point_1 setup), then create fresh RecoveryCrawler with AsyncMock delete fns (both return None), call recover_all(). Assert: file is gemini_state='untracked', version incremented, intent columns all NULL. Assert recovered list contains file_path.
    2. `test_recovery_crash_point_2` -- Seed indexed file, simulate crash point 2 (intent_api_calls_completed=2 in DB), run RecoveryCrawler. Assert: crawler skips both API calls (mock delete fns NOT called), only runs finalize. File is 'untracked'.
    3. `test_recovery_crash_point_3` -- Same as CP2 (identical DB state). Run RecoveryCrawler. Assert same result. (This confirms CP2 and CP3 have identical recovery paths.)
    4. `test_recovery_empty_db` -- No files with intent_type set. RecoveryCrawler.recover_all() returns empty list. No errors.

    For tests 2 and 3, instead of running the full ResetTransitionManager crash flow, directly INSERT a file into the DB with the expected partial state (gemini_state='indexed', intent_type='reset_intent', intent_api_calls_completed=2, version=5). This is simpler and tests recovery in isolation.

    **tests/test_failed_escape.py** -- 2 tests:
    1. `test_failed_to_untracked` -- Seed failed file (use seed_failed_file fixture). Call retry_failed_file(). Assert: gemini_state='untracked', version incremented, intent columns NULL.
    2. `test_retry_wrong_state_noop` -- Seed indexed file. Call retry_failed_file(). Assert: returns False, file state unchanged.

    **tests/test_sc3_simplicity.py** -- 1 test (GA-9):
    - `test_recovery_simpler_than_transition`:
      1. Read spike/phase10_spike/recovery_crawler.py, count non-blank, non-comment, non-docstring lines in RecoveryCrawler class (including _recover_file)
      2. Read spike/phase10_spike/transition_reset.py, count non-blank, non-comment, non-docstring lines in ResetTransitionManager class (including execute_reset)
      3. Assert: recovery_lines <= transition_lines
      4. Assert: "while" not in recovery source and "for attempt" not in recovery source (no retry loops in recovery code, per GA-9)
      5. Print both line counts for evidence
  </action>
  <verify>
    Run: `python -m pytest spike/phase10_spike/tests/test_recovery.py spike/phase10_spike/tests/test_failed_escape.py spike/phase10_spike/tests/test_sc3_simplicity.py -v`
    All 7 tests pass (4 recovery + 2 failed escape + 1 SC3).
    Run: `python -m pytest spike/phase10_spike/tests/ -v`
    ALL tests pass (Plan 01 + Plan 02 combined). Expected: 14+ tests.
  </verify>
  <done>
    - RecoveryCrawler.recover_all() scans and recovers all 3 crash point states to 'untracked'
    - Recovery uses linear step resumption (no retry loops)
    - retry_failed_file() transitions FAILED -> UNTRACKED with OCC
    - SC3 measurement confirms recovery_lines <= transition_lines
    - All 7 new tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Combined harness with structured JSON evidence</name>
  <files>
    spike/phase10_spike/harness.py
  </files>
  <action>
    Create spike/phase10_spike/harness.py following Phase 9 harness pattern (spike/phase9_spike/harness.py). The harness produces structured JSON output demonstrating all 6 affirmative evidence checks for Phase 10 gate assessment.

    **Harness structure:**
    - `class HarnessResult` with `add_check(name, passed, details)` and `all_passed` property (same as Phase 9)
    - `async def run_harness() -> HarnessResult`:
      1. Setup: create temp DB, init_spike_db(), seed one indexed file with Gemini IDs
      2. **Check 1: Safe delete 404 handling** -- Call safe_delete_store_document with AsyncMock that raises ClientError(code=404). Assert returns True. Call safe_delete_file with AsyncMock that raises ClientError(code=403). Assert it propagates.
      3. **Check 2: Crash point 1 recovery** -- Run ResetTransitionManager with delete_file crash. Verify partial state (api_calls_completed=1). Run RecoveryCrawler.recover_all(). Verify file is 'untracked', intent cleared.
      4. **Check 3: Crash point 2 recovery** -- Insert file with api_calls_completed=2 state directly. Run RecoveryCrawler. Verify file is 'untracked', crawler skips API calls.
      5. **Check 4: Crash point 3 recovery** -- Same as check 3 (identical state). Run RecoveryCrawler. Verify recovery.
      6. **Check 5: FAILED escape** -- Seed failed file. Call retry_failed_file(). Verify gemini_state='untracked'.
      7. **Check 6: SC3 simplicity** -- Count lines in recovery_crawler.py RecoveryCrawler vs transition_reset.py ResetTransitionManager. Assert recovery_lines <= transition_lines. Report both counts.
    - Print progress and results in same format as Phase 9 harness
    - Print full JSON results at end
    - `def main()`: `asyncio.run(run_harness())`, exit 0 if all passed, exit 1 if any failed
    - Entry point: `python -m spike.phase10_spike.harness`

    For the ClientError mock in the harness, use the same approach as the test_safe_delete.py tests from Plan 10-01.

    Cleanup: remove temp DB and WAL/SHM files at end.
  </action>
  <verify>
    Run: `python -m spike.phase10_spike.harness`
    Output shows "ALL CHECKS PASSED" and structured JSON with all 6 checks passed.
    Exit code 0.
  </verify>
  <done>
    - Harness runs all 6 evidence checks for Phase 10 gate assessment
    - Structured JSON output shows: safe_delete 404 handling, 3 crash point recoveries, FAILED escape, SC3 line counts
    - "ALL CHECKS PASSED" printed on success
    - Exit code 0 = gate ready, exit code 1 = failures detected
  </done>
</task>

</tasks>

<verification>
Run full test suite for Phase 10:
```bash
python -m pytest spike/phase10_spike/tests/ -v --tb=short
```
Expected: 14+ tests pass (4 safe_delete + 3 crash_points + 4 recovery + 2 failed_escape + 1 SC3).

Run combined harness:
```bash
python -m spike.phase10_spike.harness
```
Expected: ALL CHECKS PASSED, exit code 0.

Verify Phase 10 Success Criteria from ROADMAP.md:
1. SC1 (Write-ahead intent covers all crash points): Crash point tests 1-3 prove recoverable state at each crash point. RecoveryCrawler proves automatic recovery.
2. SC2 (No manual SQL required): FAILED->UNTRACKED via retry_failed_file(). Recovery crawler handles all intent states. No state requires manual SQL.
3. SC3 (Recovery simpler than transition): SC3 test measures and asserts recovery_lines <= transition_lines, zero retry loops in recovery.
</verification>

<success_criteria>
1. RecoveryCrawler.recover_all() automatically recovers files from all 3 crash point states to 'untracked'
2. retry_failed_file() transitions FAILED -> UNTRACKED without manual SQL
3. SC3 measurement: recovery code line count <= transition code line count, zero retry loops
4. Combined harness shows ALL CHECKS PASSED with structured JSON evidence
5. All tests pass: `python -m pytest spike/phase10_spike/tests/ -v` shows 14+ passing, 0 failing
6. Harness exit code 0: `python -m spike.phase10_spike.harness`
</success_criteria>

<output>
After completion, create `.planning/phases/10-transition-atomicity/10-02-SUMMARY.md`
</output>
