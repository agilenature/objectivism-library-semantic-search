---
phase: 02-upload-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - src/objlib/upload/recovery.py
  - src/objlib/upload/orchestrator.py
  - src/objlib/upload/__init__.py
  - tests/test_upload.py
autonomous: false

must_haves:
  truths:
    - "Restarting after a crash automatically recovers: files marked 'uploading' are retried, pending operations are polled to completion"
    - "Recovery prioritizes files with upload_timestamp approaching 48-hour expiration deadline"
    - "After full upload completes, every file in the database shows status 'uploaded' with a valid gemini_file_id"
    - "A small test batch (5-10 files) uploads successfully to real Gemini API and appears in the File Search store"
    - "Upload tests validate the circuit breaker, state lifecycle, and metadata builder without hitting real API"
  artifacts:
    - path: "src/objlib/upload/recovery.py"
      provides: "Crash recovery protocol: identify incomplete ops, poll Gemini, reconcile state, prioritize deadline-critical"
      exports: ["RecoveryManager"]
    - path: "tests/test_upload.py"
      provides: "Unit tests for circuit breaker, state manager, metadata builder, and orchestrator logic"
      min_lines: 100
  key_links:
    - from: "src/objlib/upload/recovery.py"
      to: "src/objlib/upload/state.py"
      via: "Recovery reads uploading files and pending operations from state"
      pattern: "state\\.get_uploading_files|state\\.get_pending_operations"
    - from: "src/objlib/upload/recovery.py"
      to: "src/objlib/upload/client.py"
      via: "Recovery polls operations and checks file status via client"
      pattern: "client\\.poll_operation|client\\.aio\\.files\\.get"
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/recovery.py"
      via: "Orchestrator runs recovery on startup before processing new files"
      pattern: "recovery\\.run|RecoveryManager"
---

<objective>
Complete the upload pipeline with crash recovery, state reconciliation, unit tests, and a real-API verification checkpoint. After this plan, the pipeline is production-ready: it recovers from any interruption, has test coverage for core logic, and has been validated against the live Gemini API.

Purpose: Recovery and testing are what transform a prototype into a reliable tool. The user needs confidence that interrupting and restarting (which WILL happen with 1,884 files) works correctly. The test upload proves the two-step pattern, metadata attachment, and operation polling all work against the real API.

Output: `src/objlib/upload/recovery.py`, `tests/test_upload.py`, updated orchestrator with recovery integration, verified upload against real Gemini API.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-upload-pipeline/02-RESEARCH.md
@.planning/phases/02-upload-pipeline/02-CONTEXT.md
@.planning/phases/02-upload-pipeline/02-01-SUMMARY.md
@.planning/phases/02-upload-pipeline/02-02-SUMMARY.md

# Source files to understand
@src/objlib/upload/client.py
@src/objlib/upload/state.py
@src/objlib/upload/orchestrator.py
@src/objlib/upload/circuit_breaker.py
@src/objlib/database.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement crash recovery, integrate into orchestrator, and create unit tests</name>
  <files>
    src/objlib/upload/recovery.py
    src/objlib/upload/orchestrator.py
    src/objlib/upload/__init__.py
    tests/test_upload.py
  </files>
  <action>
**src/objlib/upload/recovery.py -- RecoveryManager:**

Implements crash recovery per locked decisions #1 (48-hour TTL) and #9 (Crash Recovery):

Constructor:
- `client: GeminiFileSearchClient`
- `state: AsyncUploadStateManager`
- `config: UploadConfig`

Methods:

- `async run() -> RecoveryResult`:
  Main recovery entry point. Runs with a 4-hour timeout (`asyncio.wait_for(self._recover(), timeout=config.recovery_timeout_seconds)`). On timeout, log CRITICAL and raise `RecoveryTimeoutError`. Returns `RecoveryResult` dataclass with counts.

- `async _recover() -> RecoveryResult`:
  1. **Phase 1: Check interrupted uploads** -- Query `state.get_uploading_files()`. These are files where intent was recorded but no API response captured. For each:
     - Check if file has a gemini_file_id already (upload succeeded but import didn't happen)
     - If no gemini_file_id: reset status to 'pending' for re-upload
     - If has gemini_file_id: check if it's still valid (not expired)

  2. **Phase 2: Check pending operations** -- Query `state.get_pending_operations()`. For each:
     - Try `client.poll_operation(operation)` with a short timeout (60 seconds)
     - If operation.done and succeeded: `state.record_import_success()`
     - If operation.done and failed: `state.update_operation_state('failed')`
     - If not done: leave as pending for the orchestrator to poll later
     - On API error (file expired, not found): mark as failed, reset file to pending for re-upload

  3. **Phase 3: Check expiration deadlines** -- Query files with status='uploading' or files with gemini_file_id where `remote_expiration_ts` is set. For files where the 48-hour deadline is within 8 hours (danger zone):
     - Log WARNING about approaching deadline
     - If expired (past remote_expiration_ts): reset gemini_file_id, gemini_file_uri to NULL, set status to 'pending' for re-upload
     - Track these as `deadline_critical` count

  4. **Return RecoveryResult** with counts: recovered_operations, reset_to_pending, expired_files, deadline_critical.

- `RecoveryResult` dataclass: `recovered_operations: int`, `reset_to_pending: int`, `expired_files: int`, `deadline_critical: int`, `errors: list[str]`.

- `RecoveryTimeoutError(Exception)` custom exception.

**src/objlib/upload/orchestrator.py -- Integrate recovery:**

Modify `UploadOrchestrator.run()` to call recovery FIRST before processing new files:

```python
async def run(self, store_display_name: str):
    # Step 0: Run crash recovery
    recovery = RecoveryManager(self.client, self.state, self.config)
    recovery_result = await recovery.run()
    if recovery_result.recovered_operations > 0 or recovery_result.reset_to_pending > 0:
        logger.info(f"Recovery: {recovery_result.recovered_operations} ops recovered, "
                     f"{recovery_result.reset_to_pending} files reset to pending")

    # Step 1: Get or create store (existing)
    # Step 2: Acquire lock (existing)
    # ... rest of existing run() logic
```

**src/objlib/upload/__init__.py:** Add export:
```python
from objlib.upload.recovery import RecoveryManager, RecoveryResult, RecoveryTimeoutError
```

**tests/test_upload.py -- Unit tests for upload infrastructure:**

Create a comprehensive test file covering the components that can be tested WITHOUT hitting the real Gemini API. Use pytest and pytest-asyncio.

Test groups:

**Circuit Breaker Tests (5-6 tests):**
- `test_circuit_starts_closed`: New breaker is in CLOSED state
- `test_circuit_trips_on_error_rate`: After filling window with >5% 429s, state is OPEN
- `test_circuit_trips_on_consecutive_429s`: 3 consecutive 429s trips regardless of overall rate
- `test_circuit_recovers_after_cooldown`: After cooldown_seconds, state transitions to HALF_OPEN
- `test_circuit_closes_on_half_open_success`: Success in HALF_OPEN transitions to CLOSED
- `test_recommended_concurrency_reduces_when_open`: get_recommended_concurrency returns reduced value when OPEN

**Rate Limiter Tests (3 tests):**
- `test_tier1_config`: Tier 1 has rpm=20, interval=3.0s
- `test_free_tier_config`: Free tier has rpm=5, interval=12.0s
- `test_unknown_tier_raises`: Invalid tier name raises KeyError or ValueError

**Metadata Builder Tests (4-5 tests):**
- `test_build_metadata_full`: All fields present produces correct format
- `test_build_metadata_partial`: Missing optional fields are excluded
- `test_build_metadata_numeric_year`: Year value becomes numeric_value
- `test_build_metadata_quality_mapping`: MetadataQuality grades map to numeric scores
- `test_build_metadata_empty`: Empty dict produces empty list

**Async State Manager Tests (4-5 tests, use pytest-asyncio):**
- `test_pending_files_query`: Files with pending status are returned
- `test_record_upload_intent_changes_status`: After intent, file status is 'uploading'
- `test_record_upload_success`: After success, gemini_file_uri and operation are recorded
- `test_record_failure`: After failure, error_message is stored
- `test_lock_acquire_and_release`: Lock can be acquired, released, and re-acquired

All async tests should use `@pytest.mark.asyncio` and create temp databases in fixtures.

Add a `conftest.py`-style fixture or use the existing one. For the state manager tests, create a temp database with the full schema (use the sync Database class to initialize, then test with AsyncUploadStateManager).
  </action>
  <verify>
Run: `python -c "
from objlib.upload.recovery import RecoveryManager, RecoveryResult, RecoveryTimeoutError
print('Recovery imports OK')
"` -- must succeed.

Run: `pytest tests/test_upload.py -v` -- all upload tests must pass.

Run: `pytest tests/ -x -q` -- ALL tests (original 35 + new upload tests) must pass.
  </verify>
  <done>
RecoveryManager handles three recovery phases: interrupted uploads, pending operations, and expiration deadlines. Orchestrator calls recovery on startup before processing new files. Test suite covers circuit breaker (6 tests), rate limiter (3 tests), metadata builder (5 tests), and async state manager (5 tests). All tests pass. Total test count is 35 (original) + ~19 (new) = ~54 tests.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify upload pipeline against real Gemini API with a small test batch</name>
  <what-built>
Complete upload pipeline including:
- Two-step upload pattern (files.upload -> wait ACTIVE -> import_file with custom_metadata)
- Circuit breaker with rolling-window 429 tracking
- Async state management with write-ahead intent logging
- Rich progress bars with pipeline/batch hierarchy
- Crash recovery with 48-hour TTL awareness
- CLI command: `objlib upload`
  </what-built>
  <how-to-verify>
**Prerequisites:**
1. Ensure `GEMINI_API_KEY` is set in your environment
2. Ensure the library database has been scanned (run `objlib scan` if not already done)

**Test steps:**

1. Check pending file count:
   ```bash
   objlib upload --dry-run --db data/library.db
   ```
   Expected: Shows count of pending files (should be ~1,884)

2. Run a small test upload (5-10 files):
   ```bash
   objlib upload --store "objectivism-library-test" --db data/library.db --batch-size 5 --concurrency 3
   ```
   Then Ctrl+C after the first batch completes (or let it finish the 5 files).

3. Verify files were uploaded by checking database status:
   ```bash
   objlib status --db data/library.db
   ```
   Expected: Some files show status "uploaded" with gemini_file_id values

4. Verify crash recovery by running upload again:
   ```bash
   objlib upload --store "objectivism-library-test" --db data/library.db --batch-size 5 --concurrency 3
   ```
   Expected: Previously uploaded files are SKIPPED (not re-uploaded). New pending files start processing.

5. Verify Rich progress bars display correctly:
   - Pipeline-level bar showing overall progress
   - Batch-level bar showing current batch progress
   - ETA and percentage visible

6. Check that the Gemini File Search store was created and contains the uploaded files (optional -- can check via Google AI Studio dashboard).

**What to look for:**
- No crash or unhandled exception during upload
- Progress bars render cleanly in terminal
- Status command shows uploaded files with gemini_file_id
- Resume skips already-uploaded files
- Metadata fields (course, difficulty, year etc.) attached to files in store
  </how-to-verify>
  <resume-signal>Type "approved" to confirm the upload pipeline works, or describe any issues you see.</resume-signal>
</task>

</tasks>

<verification>
1. RecoveryManager handles interrupted uploads, pending operations, and TTL expiration
2. Orchestrator runs recovery on startup before processing new files
3. All unit tests pass (circuit breaker, rate limiter, metadata, state manager)
4. Real Gemini API test upload succeeds with metadata attached
5. Resume after interruption skips already-uploaded files
6. Total test suite (original + upload) passes cleanly
</verification>

<success_criteria>
- Recovery protocol handles all three crash scenarios (interrupted upload, pending operation, expired file)
- 4-hour recovery timeout is enforced
- Unit test suite covers circuit breaker, rate limiter, metadata builder, and state manager
- Real API test proves: file upload, ACTIVE wait, import with metadata, operation polling
- Resume capability verified: Ctrl+C then restart skips completed files
- All tests pass (original 35 + new ~19)
</success_criteria>

<output>
After completion, create `.planning/phases/02-upload-pipeline/02-03-SUMMARY.md`
</output>
