---
phase: 02-upload-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/objlib/models.py
  - src/objlib/config.py
  - src/objlib/database.py
  - src/objlib/upload/__init__.py
  - src/objlib/upload/client.py
  - src/objlib/upload/circuit_breaker.py
  - src/objlib/upload/rate_limiter.py
autonomous: true
user_setup:
  - service: gemini
    why: "File Search API for document indexing"
    env_vars:
      - name: GEMINI_API_KEY
        source: "Google AI Studio -> Get API Key (https://aistudio.google.com/apikey)"
    dashboard_config: []

must_haves:
  truths:
    - "google-genai, tenacity, and aiosqlite are installable and importable"
    - "Database creates upload_operations, upload_batches, and upload_locks tables on init"
    - "Gemini client wrapper can upload a file and import it into a store with custom_metadata using two-step pattern"
    - "Circuit breaker tracks 429 error rate over a rolling window of 100 requests and trips at 5%"
    - "Rate limiter provides tier-based configuration with default Tier 1 (20 RPM) settings"
  artifacts:
    - path: "src/objlib/upload/__init__.py"
      provides: "Upload subpackage public API exports"
    - path: "src/objlib/upload/client.py"
      provides: "GeminiFileSearchClient with two-step upload (files.upload -> wait ACTIVE -> import_file)"
      exports: ["GeminiFileSearchClient"]
    - path: "src/objlib/upload/circuit_breaker.py"
      provides: "Rolling-window circuit breaker with CLOSED/OPEN/HALF_OPEN states"
      exports: ["CircuitState", "RollingWindowCircuitBreaker"]
    - path: "src/objlib/upload/rate_limiter.py"
      provides: "Rate limit tier config and adaptive throttling"
      exports: ["RateLimitTier", "RateLimiterConfig"]
    - path: "src/objlib/database.py"
      provides: "Extended schema with upload_operations, upload_batches, upload_locks tables"
      contains: "upload_operations"
  key_links:
    - from: "src/objlib/upload/client.py"
      to: "google.genai"
      via: "genai.Client with aio accessor"
      pattern: "client\\.aio\\.files\\.upload|client\\.aio\\.file_search_stores\\.import_file"
    - from: "src/objlib/upload/circuit_breaker.py"
      to: "collections.deque"
      via: "Rolling window of last 100 requests"
      pattern: "deque.*maxlen"
    - from: "src/objlib/upload/client.py"
      to: "src/objlib/upload/circuit_breaker.py"
      via: "Client records success/429 on circuit breaker after each API call"
      pattern: "circuit_breaker\\.record"
---

<objective>
Build the foundation layer for the upload pipeline: add dependencies, extend the database schema for upload tracking, and create the three core building blocks (Gemini API client wrapper, rolling-window circuit breaker, rate limiter config).

Purpose: These are the primitives that the orchestrator (Plan 02) composes. Without them, no upload can happen. The two-step upload pattern (files.upload -> wait ACTIVE -> import_file with custom_metadata) is the single most critical pattern -- it is the ONLY way to attach searchable metadata to indexed files.

Output: `src/objlib/upload/` package with client.py, circuit_breaker.py, rate_limiter.py; extended database.py schema; updated pyproject.toml with new dependencies.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-upload-pipeline/02-RESEARCH.md
@.planning/phases/02-upload-pipeline/02-CONTEXT.md
@.planning/phases/01-foundation/01-01-SUMMARY.md

# Key source files to understand before modifying
@src/objlib/database.py
@src/objlib/models.py
@src/objlib/config.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies, extend models/config, and migrate database schema for upload tracking</name>
  <files>
    pyproject.toml
    src/objlib/models.py
    src/objlib/config.py
    src/objlib/database.py
  </files>
  <action>
**pyproject.toml:** Add upload dependencies to the `[project]` dependencies list:
- `"google-genai>=1.63.0"` (Gemini File Search API -- NOT the old google-generativeai package)
- `"tenacity>=9.1"` (retry/backoff)
- `"aiosqlite>=0.22"` (async SQLite wrapper)

Also add `"pytest-asyncio>=0.24"` to the `[project.optional-dependencies] dev` list for async test support.

**src/objlib/models.py:** Add an `OperationState` enum (str, Enum) with values: `PENDING = "pending"`, `IN_PROGRESS = "in_progress"`, `SUCCEEDED = "succeeded"`, `FAILED = "failed"`, `TIMEOUT = "timeout"`. Export it from `__all__` in `__init__.py`.

Add an `UploadConfig` dataclass with these fields:
- `store_name: str` (Gemini File Search store name, e.g., "objectivism-library-v1")
- `api_key: str | None = None` (falls back to GEMINI_API_KEY env var)
- `max_concurrent_uploads: int = 7` (Semaphore starting value, adjustable 3-10 per circuit breaker)
- `max_concurrent_polls: int = 20`
- `batch_size: int = 150` (logical batch, 100-200 range)
- `poll_timeout_seconds: int = 3600` (1 hour per operation)
- `poll_min_wait: int = 5` (5 seconds initial poll)
- `poll_max_wait: int = 60` (60 seconds max poll)
- `rate_limit_tier: str = "tier1"` (manual tier config, default Tier 1)
- `recovery_timeout_seconds: int = 14400` (4 hours)
- `db_path: str = "data/library.db"`

**src/objlib/config.py:** Add a `load_upload_config(config_path: Path | None = None) -> UploadConfig` function that loads from `config/upload_config.json` if it exists, falling back to UploadConfig defaults. If `api_key` is None, read from `os.getenv("GEMINI_API_KEY")`.

**src/objlib/database.py:** Add the upload tracking schema. Append to `SCHEMA_SQL` (after the existing _skipped_files table):

```sql
-- Upload operations tracking (Phase 2)
CREATE TABLE IF NOT EXISTS upload_operations (
    operation_name TEXT PRIMARY KEY,
    file_path TEXT NOT NULL,
    gemini_file_name TEXT,
    operation_state TEXT NOT NULL DEFAULT 'pending'
        CHECK(operation_state IN ('pending', 'in_progress', 'succeeded', 'failed', 'timeout')),
    created_at TEXT DEFAULT (strftime('%Y-%m-%dT%H:%M:%f', 'now')),
    last_polled_at TEXT,
    completed_at TEXT,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    FOREIGN KEY (file_path) REFERENCES files(file_path)
);

CREATE INDEX IF NOT EXISTS idx_upload_ops_state ON upload_operations(operation_state);
CREATE INDEX IF NOT EXISTS idx_upload_ops_file ON upload_operations(file_path);

-- Logical batch tracking
CREATE TABLE IF NOT EXISTS upload_batches (
    batch_id INTEGER PRIMARY KEY AUTOINCREMENT,
    batch_number INTEGER NOT NULL,
    file_count INTEGER NOT NULL,
    succeeded_count INTEGER DEFAULT 0,
    failed_count INTEGER DEFAULT 0,
    status TEXT DEFAULT 'pending'
        CHECK(status IN ('pending', 'in_progress', 'completed', 'failed')),
    started_at TEXT,
    completed_at TEXT
);

-- Single-writer lock (max one row enforced by CHECK)
CREATE TABLE IF NOT EXISTS upload_locks (
    lock_id INTEGER PRIMARY KEY CHECK(lock_id = 1),
    instance_id TEXT NOT NULL,
    acquired_at TEXT DEFAULT (strftime('%Y-%m-%dT%H:%M:%f', 'now')),
    last_heartbeat TEXT DEFAULT (strftime('%Y-%m-%dT%H:%M:%f', 'now'))
);
```

Bump `PRAGMA user_version` from 1 to 2. The `CREATE TABLE IF NOT EXISTS` pattern ensures backward compatibility -- existing Phase 1 databases won't break.

Add a `get_pending_files(self, limit: int = 200) -> list[sqlite3.Row]` method to Database that returns files with `status = 'pending'` ordered by `file_path`, limited to `limit` rows. Each row should include: `file_path, content_hash, filename, file_size, metadata_json`.

Add a `update_file_status(self, file_path: str, status: FileStatus, **kwargs)` method that updates the status column and any additional columns passed as kwargs (e.g., `gemini_file_uri=..., gemini_file_id=..., upload_timestamp=..., error_message=...`).
  </action>
  <verify>
Run: `cd /Users/david/projects/objectivism-library-semantic-search && pip install -e ".[dev]"` -- must succeed with all new dependencies installed.

Run: `python -c "from google import genai; import tenacity; import aiosqlite; print('imports OK')"` -- must print "imports OK".

Run: `python -c "from objlib.models import OperationState, UploadConfig; print(OperationState.PENDING, UploadConfig(store_name='test'))"` -- must print enum and config values.

Run: `python -c "
from objlib.database import Database
import tempfile, os
db_path = os.path.join(tempfile.mkdtemp(), 'test.db')
with Database(db_path) as db:
    # Check new tables exist
    tables = [r[0] for r in db.conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()]
    assert 'upload_operations' in tables, f'Missing upload_operations, got {tables}'
    assert 'upload_batches' in tables, f'Missing upload_batches, got {tables}'
    assert 'upload_locks' in tables, f'Missing upload_locks, got {tables}'
    # Check user_version
    v = db.conn.execute('PRAGMA user_version').fetchone()[0]
    assert v == 2, f'Expected user_version 2, got {v}'
    print('schema OK')
"` -- must print "schema OK".

Run: `pytest tests/ -x -q` -- existing 35 tests must still pass (backward compatibility).
  </verify>
  <done>
All new dependencies install. OperationState enum and UploadConfig dataclass are importable. Database creates three new upload-tracking tables. Schema version is 2. All 35 existing tests pass unchanged.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Gemini client wrapper, rolling-window circuit breaker, and rate limiter</name>
  <files>
    src/objlib/upload/__init__.py
    src/objlib/upload/client.py
    src/objlib/upload/circuit_breaker.py
    src/objlib/upload/rate_limiter.py
  </files>
  <action>
Create the `src/objlib/upload/` package directory with `__init__.py`.

**src/objlib/upload/circuit_breaker.py:**

Implement `RollingWindowCircuitBreaker` per locked decision #5 (Circuit Breaker):
- `CircuitState(str, Enum)` with CLOSED, OPEN, HALF_OPEN values
- Constructor params: `window_size: int = 100`, `error_threshold: float = 0.05` (5%), `consecutive_threshold: int = 3`, `cooldown_seconds: float = 300` (5 minutes), `recovery_increment: int = 20` (successes before +1 concurrency)
- Internal state: `_window: collections.deque(maxlen=window_size)` storing booleans (True=success, False=429), `_state: CircuitState`, `_opened_at: float | None`, `_consecutive_429s: int`, `_success_since_recovery: int`
- Methods:
  - `record_success()`: Append True, reset consecutive counter. If HALF_OPEN, transition to CLOSED and reset window.
  - `record_429()`: Append False, increment consecutive counter. Call `_should_trip()` and transition if needed.
  - `record_error()`: Record non-429 errors (don't trip circuit, just log).
  - Property `state -> CircuitState`: Checks if OPEN has exceeded cooldown, transitions to HALF_OPEN.
  - Property `error_rate -> float`: 429 count / window size.
  - `_should_trip() -> bool`: True if error_rate > threshold OR consecutive_429s >= consecutive_threshold.
  - `_trip()`: Set state to OPEN, record opened_at, reset success counter.
  - `get_recommended_concurrency(max_concurrency: int = 10) -> int`: Returns max when CLOSED, max//2 (min 3) when OPEN, gradually increasing based on _success_since_recovery when HALF_OPEN or after recovery.

Do NOT use pybreaker -- hand-roll this per research recommendation (pybreaker doesn't fit rolling-window 429 tracking).

**src/objlib/upload/rate_limiter.py:**

Implement per locked decision #7 (Rate Limit Tier Detection):
- `RATE_LIMIT_TIERS` dict mapping tier names to `{"rpm": int, "rpd": int | None}`:
  - `"free"`: rpm=5, rpd=100
  - `"tier1"`: rpm=20, rpd=None
  - `"tier2"`: rpm=200, rpd=None
  - `"tier3"`: rpm=2000, rpd=None
- `RateLimiterConfig` dataclass with `tier: str = "tier1"`, computed `rpm` and `rpd` from RATE_LIMIT_TIERS, and `min_request_interval` property (= 60/rpm seconds).
- `AdaptiveRateLimiter` class:
  - Constructor: `config: RateLimiterConfig`, `circuit_breaker: RollingWindowCircuitBreaker`
  - `async wait_if_needed()`: Sleep for `min_request_interval` adjusted by circuit breaker state. When OPEN, multiply interval by 3. When HALF_OPEN, multiply by 1.5.
  - Tracks observed rate limit headers (store `x-ratelimit-remaining` if present in responses) and logs them. This is observation-only, not enforcement.

**src/objlib/upload/client.py:**

Implement `GeminiFileSearchClient` -- the core upload wrapper implementing the two-step upload pattern per the CRITICAL RESEARCH FINDING:

```
Step 1: client.aio.files.upload() -> File (temporary, 48hr TTL)
Step 2: Poll File.state until ACTIVE
Step 3: client.aio.file_search_stores.import_file() with custom_metadata -> Operation
```

Constructor:
- `api_key: str` (required)
- `circuit_breaker: RollingWindowCircuitBreaker`
- `rate_limiter: AdaptiveRateLimiter`
- Creates `genai.Client(api_key=api_key)` internally.
- `store_name: str | None = None` (set later via `set_store()`)

Methods:

- `async create_store(display_name: str) -> str`: Creates a File Search store, returns store name. Calls `client.aio.file_search_stores.create(config={"display_name": display_name})`.

- `async get_or_create_store(display_name: str) -> str`: Lists existing stores, returns one matching display_name if found, otherwise creates new. Saves to `self.store_name`.

- `async upload_file(file_path: str, display_name: str) -> object`:
  Step 1 of two-step pattern. Calls `client.aio.files.upload(file=file_path, config={"display_name": display_name[:512]})`. Records success/429 on circuit_breaker. Calls `rate_limiter.wait_if_needed()` before the call. Returns the File object.

- `async wait_for_active(file_obj: object, timeout: int = 300) -> object`:
  Polls `client.aio.files.get(name=file_obj.name)` until `state.name == "ACTIVE"`. Use tenacity `AsyncRetrying` with `wait_exponential(min=2, max=30)` and `stop_after_delay(timeout)`. Raises `RuntimeError` if state becomes `"FAILED"`. Returns the active File object.

- `async import_to_store(file_name: str, metadata: list[dict]) -> object`:
  Step 3 of two-step pattern. Calls `client.aio.file_search_stores.import_file(file_search_store_name=self.store_name, file_name=file_name, config={"custom_metadata": metadata})`. Records success/429 on circuit_breaker. Returns the Operation object.

- `async upload_and_import(file_path: str, display_name: str, metadata: list[dict]) -> tuple`:
  Combines all three steps: upload_file -> wait_for_active -> import_to_store. Returns `(file_obj, operation)` tuple.

- `async poll_operation(operation: object, timeout: int = 3600) -> object`:
  Polls operation until done using tenacity `AsyncRetrying` with `wait_exponential(multiplier=1, min=5, max=60)`, `stop_after_delay(timeout)`, `retry_if_result(lambda op: not op.done)`. Returns completed operation.

- `build_custom_metadata(metadata_dict: dict) -> list[dict]`:
  Static method. Converts flat metadata dict to Gemini custom_metadata format. Maps to Tier 1 searchable fields only (per locked decision #2):
  - `category` -> string_value
  - `course` -> string_value
  - `difficulty` -> string_value
  - `year` -> numeric_value (if present and numeric)
  - `quarter` -> string_value
  - `quality_score` -> numeric_value (map MetadataQuality to number: complete=100, partial=75, minimal=50, none=25, unknown=0)
  - `date` -> string_value (from metadata if present)
  - `week` -> numeric_value (if present and numeric)

  Only include keys that have non-None values. Return list of dicts like `[{"key": "course", "string_value": "OPAR"}, {"key": "year", "numeric_value": 2019}]`.

- `async close()`: Closes the client if it has a close method.

Error handling: Catch `google.genai.errors.APIError`. On `e.code == 429`, call `circuit_breaker.record_429()` and re-raise as a custom `RateLimitError(Exception)`. On other codes, call `circuit_breaker.record_error()` and re-raise. On success, call `circuit_breaker.record_success()`.

Define custom exceptions at module level: `RateLimitError(Exception)`, `TransientError(Exception)`, `PermanentError(Exception)`.

**src/objlib/upload/__init__.py:**

Export the public API:
```python
from objlib.upload.client import GeminiFileSearchClient, RateLimitError, TransientError, PermanentError
from objlib.upload.circuit_breaker import CircuitState, RollingWindowCircuitBreaker
from objlib.upload.rate_limiter import RateLimiterConfig, AdaptiveRateLimiter
```
  </action>
  <verify>
Run: `python -c "
from objlib.upload import (
    GeminiFileSearchClient, RateLimitError, TransientError, PermanentError,
    CircuitState, RollingWindowCircuitBreaker,
    RateLimiterConfig, AdaptiveRateLimiter,
)
print('All upload imports OK')

# Test circuit breaker logic
cb = RollingWindowCircuitBreaker(window_size=100, error_threshold=0.05)
assert cb.state == CircuitState.CLOSED
for _ in range(95):
    cb.record_success()
for _ in range(6):
    cb.record_429()
# 6/101 > 5% -- should trip (but deque maxlen=100 means oldest drops)
# With 100 window: 95 success + 6 = 101, deque drops first, leaving 94s + 6f = 6/100 = 6% > 5%
assert cb.state == CircuitState.OPEN, f'Expected OPEN, got {cb.state}'
print('Circuit breaker trips at 6% 429 rate: PASS')

# Test recommended concurrency
assert cb.get_recommended_concurrency(10) <= 5, 'Should reduce concurrency when OPEN'
print('Concurrency reduction when OPEN: PASS')

# Test rate limiter config
rlc = RateLimiterConfig(tier='tier1')
assert rlc.rpm == 20
assert rlc.min_request_interval == 3.0  # 60/20
print('Rate limiter config: PASS')

# Test metadata builder
meta = GeminiFileSearchClient.build_custom_metadata({
    'category': 'course',
    'course': 'OPAR',
    'difficulty': 'introductory',
    'year': 2019,
    'quarter': 'Q1',
    'quality_score': 'complete',
})
assert any(m['key'] == 'course' and m.get('string_value') == 'OPAR' for m in meta)
assert any(m['key'] == 'year' and m.get('numeric_value') == 2019 for m in meta)
print('Metadata builder: PASS')

print('All verification passed')
"` -- must print all PASS lines.

Run: `pytest tests/ -x -q` -- existing tests still pass.
  </verify>
  <done>
Upload subpackage exists at `src/objlib/upload/` with three modules. GeminiFileSearchClient implements the two-step upload pattern (files.upload -> wait ACTIVE -> import_file with custom_metadata). Circuit breaker uses rolling window of 100 requests, trips at 5% 429 rate, reduces concurrency by 50%. Rate limiter provides tier-based config with Tier 1 (20 RPM) default. All imports resolve. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `pip install -e ".[dev]"` succeeds with google-genai, tenacity, aiosqlite installed
2. `python -c "from objlib.upload import GeminiFileSearchClient, RollingWindowCircuitBreaker, RateLimiterConfig"` succeeds
3. Database creates upload_operations, upload_batches, upload_locks tables automatically
4. Circuit breaker state machine works: CLOSED -> OPEN (at 5% 429) -> HALF_OPEN (after 5min) -> CLOSED (on success)
5. `build_custom_metadata()` produces correct Gemini format with string_value/numeric_value fields
6. All 35 existing tests pass (backward compatibility)
</verification>

<success_criteria>
- google-genai >= 1.63.0, tenacity >= 9.1, aiosqlite >= 0.22 are installed and importable
- OperationState enum and UploadConfig dataclass exist in models.py
- Database schema version is 2 with three new tables
- GeminiFileSearchClient has upload_file, wait_for_active, import_to_store, upload_and_import, poll_operation methods
- Circuit breaker trips at 5% 429 rate over 100-request rolling window
- Rate limiter defaults to Tier 1 (20 RPM, 3s interval)
- Metadata builder produces correct custom_metadata list format
- Zero test regressions
</success_criteria>

<output>
After completion, create `.planning/phases/02-upload-pipeline/02-01-SUMMARY.md`
</output>
