---
phase: 02-upload-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/objlib/upload/state.py
  - src/objlib/upload/orchestrator.py
  - src/objlib/upload/progress.py
  - src/objlib/upload/__init__.py
  - src/objlib/cli.py
autonomous: true

must_haves:
  truths:
    - "Running `objlib upload` processes pending files in batches with Rich progress bars showing per-file and per-batch status"
    - "Upload respects Semaphore(7) concurrency limit, adjustable by circuit breaker state"
    - "Each file goes through the full two-step pipeline: upload -> wait_active -> import_to_store -> poll_operation"
    - "SQLite state is written BEFORE API call (intent) and AFTER API response (result), enabling crash recovery"
    - "Interrupting with Ctrl+C gracefully shuts down and saves state (no corrupted records)"
  artifacts:
    - path: "src/objlib/upload/state.py"
      provides: "Async state manager wrapping aiosqlite for upload intent/result tracking"
      exports: ["AsyncUploadStateManager"]
    - path: "src/objlib/upload/orchestrator.py"
      provides: "Batch upload orchestrator with semaphore concurrency, polling, and batch coordination"
      exports: ["UploadOrchestrator"]
    - path: "src/objlib/upload/progress.py"
      provides: "Rich hierarchical progress tracking (file/batch/pipeline levels)"
      exports: ["UploadProgressTracker"]
    - path: "src/objlib/cli.py"
      provides: "Extended CLI with upload command"
      contains: "def upload"
  key_links:
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/client.py"
      via: "Orchestrator calls client.upload_and_import() for each file"
      pattern: "client\\.upload_and_import|client\\.poll_operation"
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/state.py"
      via: "Orchestrator writes intent before upload, records result after"
      pattern: "state\\.record_upload_intent|state\\.record_upload_success"
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/circuit_breaker.py"
      via: "Orchestrator reads circuit breaker state to adjust semaphore"
      pattern: "circuit_breaker\\.get_recommended_concurrency|circuit_breaker\\.state"
    - from: "src/objlib/upload/orchestrator.py"
      to: "src/objlib/upload/progress.py"
      via: "Orchestrator updates progress tracker on each file completion"
      pattern: "progress\\.update|progress\\.advance"
    - from: "src/objlib/cli.py"
      to: "src/objlib/upload/orchestrator.py"
      via: "CLI upload command creates and runs UploadOrchestrator"
      pattern: "UploadOrchestrator|orchestrator\\.run"
---

<objective>
Build the upload orchestration layer that ties the Plan 01 primitives together into a working pipeline: async state management (aiosqlite), batch upload orchestrator with semaphore-limited concurrency, Rich progress tracking, and the `objlib upload` CLI command.

Purpose: This is the core execution engine. It takes pending files from SQLite, uploads them to Gemini via the two-step pattern, polls operations to completion, tracks progress with Rich bars, and writes state at every step for crash recoverability. After this plan, the user can run `objlib upload` and watch their library get indexed.

Output: `src/objlib/upload/state.py`, `orchestrator.py`, `progress.py` modules; updated `cli.py` with upload command.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-upload-pipeline/02-RESEARCH.md
@.planning/phases/02-upload-pipeline/02-CONTEXT.md
@.planning/phases/02-upload-pipeline/02-01-SUMMARY.md

# Source files from Plan 01 (must read before extending)
@src/objlib/upload/client.py
@src/objlib/upload/circuit_breaker.py
@src/objlib/upload/rate_limiter.py
@src/objlib/database.py
@src/objlib/models.py
@src/objlib/cli.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create async state manager and upload orchestrator with batch processing</name>
  <files>
    src/objlib/upload/state.py
    src/objlib/upload/orchestrator.py
  </files>
  <action>
**src/objlib/upload/state.py -- AsyncUploadStateManager:**

Create an async SQLite state manager using aiosqlite. This wraps the existing database for async upload operations per locked decision #6 (SQLite-as-Source-of-Truth).

Constructor: `db_path: str`. Internal: `_db: aiosqlite.Connection | None`.

Methods:

- `async connect()`: Open aiosqlite connection. Set `row_factory = aiosqlite.Row`. Execute pragmas: `PRAGMA journal_mode=WAL`, `PRAGMA synchronous=NORMAL`, `PRAGMA foreign_keys=ON`.

- `async close()`: Close connection if open.

- `async get_pending_files(limit: int = 200) -> list[dict]`: Query files with `status = 'pending'` ordered by `file_path`, limited to `limit`. Return list of dicts with keys: `file_path, content_hash, filename, file_size, metadata_json`.

- `async get_uploading_files() -> list[dict]`: Query files with `status = 'uploading'`. These are files where intent was recorded but upload didn't complete (crash recovery candidates).

- `async record_upload_intent(file_path: str)`: Write intent BEFORE API call per locked decision #6. Update files: `SET status = 'uploading', updated_at = strftime(...)`. Commit immediately. This is the crash recovery anchor -- if we crash after this but before API call, resume will find this file in 'uploading' status and retry.

- `async record_upload_success(file_path: str, gemini_file_uri: str, gemini_file_id: str, operation_name: str)`: Update files with gemini_file_uri, gemini_file_id, upload_timestamp (now), remote_expiration_ts (now + 48 hours). Insert into upload_operations with file_path, gemini_file_name=gemini_file_id, operation_state='pending'. Commit.

- `async record_import_success(file_path: str, operation_name: str)`: Update upload_operations: operation_state='succeeded', completed_at=now. Update files: status='uploaded'. Commit.

- `async record_upload_failure(file_path: str, error_message: str, retry_count: int = 0)`: Update files: status='failed', error_message. If there's an operation, update upload_operations: operation_state='failed', error_message. Commit.

- `async update_operation_state(operation_name: str, state: str, error_message: str | None = None)`: Update upload_operations: operation_state, last_polled_at=now, error_message if provided. If state is 'succeeded' or 'failed', set completed_at=now. Commit.

- `async get_pending_operations() -> list[dict]`: Query upload_operations with `operation_state IN ('pending', 'in_progress')`. Return list of dicts.

- `async create_batch(batch_number: int, file_count: int) -> int`: Insert into upload_batches, return batch_id.

- `async update_batch(batch_id: int, succeeded: int, failed: int, status: str)`: Update upload_batches counters and status. Set completed_at if status in ('completed', 'failed').

- `async acquire_lock(instance_id: str) -> bool`: Try to INSERT OR REPLACE into upload_locks with instance_id and timestamps. Return True. This implements single-writer per locked decision #10.

- `async release_lock()`: DELETE FROM upload_locks. Commit.

- `async update_heartbeat(instance_id: str)`: UPDATE upload_locks SET last_heartbeat=now WHERE instance_id=?.

- Async context manager: `__aenter__` calls connect, `__aexit__` calls close.

IMPORTANT: Each write method must commit immediately after the write. Do NOT hold transactions across await boundaries (per Pitfall 5 from research -- aiosqlite connection sharing).

**src/objlib/upload/orchestrator.py -- UploadOrchestrator:**

The main upload engine. Composes client + state + circuit_breaker + rate_limiter + progress.

Constructor params:
- `client: GeminiFileSearchClient`
- `state: AsyncUploadStateManager`
- `circuit_breaker: RollingWindowCircuitBreaker`
- `config: UploadConfig`
- `progress: UploadProgressTracker | None = None` (optional, for headless mode)

Internal state:
- `_upload_semaphore: asyncio.Semaphore` (initialized from config.max_concurrent_uploads)
- `_poll_semaphore: asyncio.Semaphore` (initialized from config.max_concurrent_polls)
- `_shutdown_event: asyncio.Event` (for graceful Ctrl+C)

Methods:

- `async run(store_display_name: str)`: Main entry point.
  1. Call `client.get_or_create_store(store_display_name)` to ensure store exists
  2. Acquire single-writer lock via state
  3. Get all pending files
  4. Split into logical batches of `config.batch_size` per locked decision #3
  5. For each batch: call `_process_batch(batch_files, batch_number)`
  6. After all batches: release lock, print summary
  7. Handle `asyncio.CancelledError` for graceful shutdown

- `async _process_batch(files: list[dict], batch_number: int)`:
  1. Create batch record via `state.create_batch()`
  2. If progress tracker, add batch task
  3. Create upload tasks: `[self._upload_single_file(f) for f in files]`
  4. Run `asyncio.gather(*tasks, return_exceptions=True)` -- MUST use return_exceptions=True per research anti-patterns
  5. Collect operations from successful uploads
  6. Poll all operations: `[self._poll_single_operation(op) for op in operations]`
  7. Run `asyncio.gather(*poll_tasks, return_exceptions=True)`
  8. Update batch record with succeeded/failed counts

- `async _upload_single_file(file_info: dict) -> tuple | None`:
  1. Check `_shutdown_event` -- if set, return None
  2. Check circuit breaker state -- if OPEN, wait for cooldown or return None
  3. Acquire `_upload_semaphore`
  4. Call `state.record_upload_intent(file_path)` -- BEFORE API call
  5. Parse metadata from file_info['metadata_json'] (json.loads)
  6. Build custom_metadata via `client.build_custom_metadata(metadata)`
  7. Build display_name from filename (truncated to 512 chars)
  8. Call `client.upload_and_import(file_path, display_name, custom_metadata)`
  9. Call `state.record_upload_success(file_path, file_obj.uri, file_obj.name, operation.name)`
  10. Update progress tracker
  11. Adjust semaphore based on `circuit_breaker.get_recommended_concurrency()` (recreate semaphore if capacity changed)
  12. Return (file_path, operation)
  13. On RateLimitError: log, state.record_upload_failure, update progress
  14. On Exception: log, state.record_upload_failure, update progress

  IMPORTANT on semaphore: Use `async with self._upload_semaphore:` for the API call section only. The state writes happen outside the semaphore to avoid holding it during DB writes.

- `async _poll_single_operation(operation_info: tuple) -> bool`:
  1. Acquire `_poll_semaphore`
  2. Call `client.poll_operation(operation, timeout=config.poll_timeout_seconds)`
  3. On success: `state.record_import_success(file_path, operation.name)`
  4. On timeout/failure: `state.update_operation_state(operation.name, 'failed' or 'timeout')`
  5. Update progress tracker
  6. Return success boolean

- `setup_signal_handlers()`: Register SIGINT/SIGTERM handlers that set `_shutdown_event`. On first signal, log "Graceful shutdown initiated, completing current uploads...". On second signal, force exit.

- Property `summary -> dict`: Returns counts dict with keys: total, succeeded, failed, pending, skipped.
  </action>
  <verify>
Run: `python -c "
from objlib.upload.state import AsyncUploadStateManager
from objlib.upload.orchestrator import UploadOrchestrator
print('State manager and orchestrator import OK')
"` -- must succeed.

Run: `python -c "
import asyncio
import tempfile, os

from objlib.upload.state import AsyncUploadStateManager
from objlib.database import Database

async def test_state():
    db_path = os.path.join(tempfile.mkdtemp(), 'test.db')
    # Initialize schema with sync Database first
    with Database(db_path) as db:
        from objlib.models import FileRecord, FileStatus, MetadataQuality
        db.upsert_file(FileRecord(
            file_path='/test/file.txt',
            content_hash='abc123',
            filename='file.txt',
            file_size=1000,
            metadata_json='{\"topic\": \"test\"}',
            metadata_quality=MetadataQuality.COMPLETE,
            status=FileStatus.PENDING,
        ))

    async with AsyncUploadStateManager(db_path) as state:
        # Test get_pending_files
        pending = await state.get_pending_files(limit=10)
        assert len(pending) == 1, f'Expected 1 pending, got {len(pending)}'
        assert pending[0]['file_path'] == '/test/file.txt'

        # Test record_upload_intent
        await state.record_upload_intent('/test/file.txt')
        pending = await state.get_pending_files(limit=10)
        assert len(pending) == 0, 'File should no longer be pending after intent'

        uploading = await state.get_uploading_files()
        assert len(uploading) == 1, f'Expected 1 uploading, got {len(uploading)}'

        # Test lock
        locked = await state.acquire_lock('test-instance')
        assert locked, 'Should acquire lock'

        await state.release_lock()
        print('All state manager tests passed')

asyncio.run(test_state())
"` -- must print "All state manager tests passed".

Run: `pytest tests/ -x -q` -- existing tests still pass.
  </verify>
  <done>
AsyncUploadStateManager provides full async CRUD for upload state with write-ahead intent logging. UploadOrchestrator coordinates the full pipeline: semaphore-limited concurrent uploads, operation polling, batch processing, circuit breaker integration, graceful shutdown handling. State is written before every API call and updated after every response.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Rich progress tracking and add upload CLI command</name>
  <files>
    src/objlib/upload/progress.py
    src/objlib/upload/__init__.py
    src/objlib/cli.py
  </files>
  <action>
**src/objlib/upload/progress.py -- UploadProgressTracker:**

Implement three-tier hierarchical progress per locked decision #8 (Progress Tracking):

Class `UploadProgressTracker`:

Constructor: `total_files: int`, `total_batches: int`.

Internal state:
- `_progress: rich.progress.Progress` with columns:
  - `SpinnerColumn()`
  - `TextColumn("[progress.description]{task.description}")`
  - `BarColumn()`
  - `MofNCompleteColumn()`
  - `TextColumn("[progress.percentage]{task.percentage:>3.0f}%")`
  - `TimeRemainingColumn()`
  - `TextColumn("{task.fields[status]}", style="dim")` for status text
- `_pipeline_task: TaskID` (overall pipeline progress)
- `_current_batch_task: TaskID | None`
- `_stats: dict` tracking succeeded, failed, rate_limited counts

Methods:

- `start()`: Enter the Rich progress context (`self._progress.start()`). Create pipeline task: `progress.add_task("[green]Pipeline", total=total_files, status="starting...")`.

- `stop()`: Exit progress context (`self._progress.stop()`).

- `start_batch(batch_number: int, batch_size: int)`: Create a new batch task: `progress.add_task(f"[blue]Batch {batch_number}", total=batch_size, status="uploading")`. Hide previous batch task if it exists.

- `complete_batch(batch_number: int)`: Mark current batch task as completed. Update status text.

- `file_uploaded(file_path: str)`: Advance pipeline task by 1. Advance current batch task by 1. Increment `_stats['succeeded']`. Update status to filename (truncated).

- `file_failed(file_path: str, error: str)`: Advance pipeline task by 1. Advance current batch task by 1. Increment `_stats['failed']`. Update status to show error briefly.

- `file_rate_limited(file_path: str)`: Increment `_stats['rate_limited']` counter. Update pipeline status to show "Rate limited...".

- `update_circuit_state(state: str, concurrency: int)`: Update pipeline status field to show circuit breaker state and current concurrency.

- Property `stats -> dict`: Return copy of _stats dict.

- Context manager support: `__enter__` calls start(), `__exit__` calls stop().

**src/objlib/upload/__init__.py:** Update to also export:
```python
from objlib.upload.state import AsyncUploadStateManager
from objlib.upload.orchestrator import UploadOrchestrator
from objlib.upload.progress import UploadProgressTracker
```

**src/objlib/cli.py:** Add an `upload` command to the existing Typer app:

```python
@app.command()
def upload(
    store_name: Annotated[
        str,
        typer.Option("--store", "-s", help="Gemini File Search store display name"),
    ] = "objectivism-library-v1",
    db_path: Annotated[
        Path,
        typer.Option("--db", "-d", help="Path to SQLite database file"),
    ] = Path("data/library.db"),
    batch_size: Annotated[
        int,
        typer.Option("--batch-size", "-b", help="Files per logical batch (100-200)"),
    ] = 150,
    max_concurrent: Annotated[
        int,
        typer.Option("--concurrency", "-n", help="Max concurrent uploads (3-10)"),
    ] = 7,
    dry_run: Annotated[
        bool,
        typer.Option("--dry-run", help="Show what would be uploaded without uploading"),
    ] = False,
    api_key: Annotated[
        str | None,
        typer.Option("--api-key", help="Gemini API key (default: GEMINI_API_KEY env var)", envvar="GEMINI_API_KEY"),
    ] = None,
) -> None:
```

Command implementation:
1. Validate db_path exists (or error with guidance to run `objlib scan` first)
2. Validate api_key is set (from CLI arg or env var, error if missing)
3. Create UploadConfig from CLI args
4. If dry_run: Open sync Database, call get_pending_files, display count + first 20 files in Rich table, exit
5. Otherwise:
   - Import asyncio and the upload modules
   - Create RollingWindowCircuitBreaker, RateLimiterConfig, AdaptiveRateLimiter, GeminiFileSearchClient, AsyncUploadStateManager, UploadProgressTracker
   - Create UploadOrchestrator with all components
   - Run `asyncio.run(orchestrator.run(store_name))`
   - Print final summary using Rich Panel

Add import for the upload modules at the top of the function (not top of file) to avoid import errors when upload deps aren't installed (e.g., running `objlib scan` shouldn't require google-genai). Use a try/except ImportError with a helpful message: "Upload dependencies not installed. Run: pip install objlib[upload]" -- OR just add them as main dependencies per pyproject.toml (they were added in Plan 01). Since they're main deps now, top-level import is fine. Place the upload-specific imports at module top alongside existing imports:

```python
# Upload dependencies (added in Phase 2)
from objlib.models import UploadConfig
```

But delay the heavy upload imports (google-genai etc.) to inside the command function to keep CLI startup fast for scan/status commands.
  </action>
  <verify>
Run: `python -c "
from objlib.upload import (
    AsyncUploadStateManager, UploadOrchestrator, UploadProgressTracker,
    GeminiFileSearchClient, RollingWindowCircuitBreaker,
    RateLimiterConfig, AdaptiveRateLimiter,
)
print('All upload package imports OK')
"` -- must succeed.

Run: `objlib upload --help` -- must display help text showing all options (--store, --db, --batch-size, --concurrency, --dry-run, --api-key).

Run: `python -c "
import asyncio
import tempfile, os

from objlib.database import Database
from objlib.models import FileRecord, FileStatus, MetadataQuality

# Create a test DB with pending files
db_path = os.path.join(tempfile.mkdtemp(), 'test.db')
with Database(db_path) as db:
    for i in range(5):
        db.upsert_file(FileRecord(
            file_path=f'/test/file_{i}.txt',
            content_hash=f'hash_{i}',
            filename=f'file_{i}.txt',
            file_size=1000,
            metadata_json='{\"topic\": \"test\"}',
            metadata_quality=MetadataQuality.COMPLETE,
            status=FileStatus.PENDING,
        ))

# Test progress tracker in isolation
from objlib.upload.progress import UploadProgressTracker
tracker = UploadProgressTracker(total_files=5, total_batches=1)
# Just verify it can be instantiated without crashing
print('Progress tracker created OK')
print('CLI and progress verification passed')
"` -- must succeed.

Run: `objlib upload --dry-run --db /tmp/nonexistent.db 2>&1 || true` -- should show error about database not found.

Run: `pytest tests/ -x -q` -- existing tests still pass.
  </verify>
  <done>
UploadProgressTracker provides three-tier Rich progress bars (pipeline/batch/file). CLI `upload` command is functional with --store, --db, --batch-size, --concurrency, --dry-run, --api-key options. Dry-run mode shows pending files without uploading. All upload modules are exported from the package. Existing CLI commands (scan, status, purge) still work.
  </done>
</task>

</tasks>

<verification>
1. `objlib upload --help` shows all upload options
2. `objlib upload --dry-run --db data/library.db` shows pending file count from real database
3. AsyncUploadStateManager correctly tracks intent -> success/failure lifecycle
4. UploadOrchestrator composes all primitives (client, state, circuit breaker, rate limiter, progress)
5. Rich progress bars show pipeline and batch levels during upload
6. Ctrl+C handling sets shutdown event for graceful termination
7. All 35 existing tests pass
</verification>

<success_criteria>
- `objlib upload` command exists and shows help
- `--dry-run` mode works with real database
- AsyncUploadStateManager passes intent/success/failure lifecycle test
- UploadOrchestrator has run(), _process_batch(), _upload_single_file(), _poll_single_operation() methods
- Progress tracker shows pipeline and batch level bars
- Signal handlers registered for graceful shutdown
- All upload modules correctly exported from package __init__.py
</success_criteria>

<output>
After completion, create `.planning/phases/02-upload-pipeline/02-02-SUMMARY.md`
</output>
