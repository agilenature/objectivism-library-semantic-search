# CONTEXT.md — Phase 6.3: Test Foundation & Canon Governance

**Generated:** 2026-02-18
**Phase Goal:** Before the TUI is built as a client of the system, prove the system is stable (retroactive test suite), understand the workflow landscape (GSD/Ralph/BMAD research), implement the Canon governance skill as a reusable global skill, and apply it to this project — so Phase 7 starts from verified, well-governed ground.
**Synthesis Source:** Multi-provider AI analysis (Gemini Pro, Perplexity Sonar Deep Research)
**Note:** OpenAI output truncated; Gemini and Perplexity provided full coverage of all gray areas.

---

## Prior Context

Before reading this synthesis, consult:
- `CANON-CONTEXT.md` in this directory — the authoritative pre-session Canon design context
- It already establishes: Canon.json schema (Layer 1 = contract, Layer 2 = versions), skill directory structure, GSD detection signals, and the two-layer governance model

This synthesis focuses on the **unresolved implementation ambiguities** not covered by CANON-CONTEXT.md.

---

## Overview

Phase 6.3 has two parallel tracks followed by two dependent waves:
- **Wave 1 (parallel):** Retroactive test suite (plans 01-04)
- **Wave 2 (parallel):** Workflow research (plan 05)
- **Wave 3 (depends on Wave 2):** Canon skill implementation (plans 06-07)
- **Wave 4 (depends on Wave 1 + Wave 3):** Apply & verify (plan 08)

Eight gray areas were identified. Five are consensus across providers. Two are important recommendations from single providers. One is already resolved by CANON-CONTEXT.md.

**Confidence markers:**
- ✅ **Consensus** — Both providers independently identified this as critical
- ⚠️ **Recommended** — One provider identified as important with strong rationale
- ✅ **Resolved** — Prior session decision (CANON-CONTEXT.md) already covers this

---

## Gray Areas Identified

### ✅ 1. Filesystem Simulation for Scanner Tests (Consensus)

**What needs to be decided:**
Plans 06.3-02 and 06.3-04 test the file scanner, SyncDetector, and disk utility using only in-memory resources. These modules call `os.stat`, `pathlib.Path`, and mtime comparison — real OS operations that touch the disk.

**Why it's ambiguous:**
The constraint "no disk I/O" conflicts with the scanner's implementation using standard `pathlib`. There are two fundamentally different approaches: (a) introduce `pyfakefs` to intercept all OS calls and return in-memory results, or (b) mock individual functions at the boundary. The choice affects test depth: pyfakefs allows testing the full path including error handling, while mocking may miss OS interaction bugs.

**Provider synthesis:**
- **Gemini:** Recommended `pyfakefs` as the least invasive approach for retroactive tests — doesn't require refactoring the scanner code, intercepts at OS level
- **Perplexity:** Agreed on `pyfakefs` + noted that `SyncDetector`'s mtime comparison requires specific attention (use epsilon 1e-6 per prior decision in STATE.md `[05-04]`)

**Proposed implementation decision:**
Use `pyfakefs` (add as test dependency) for all scanner, SyncDetector, and disk utility tests. For the `is_disk_mounted()` utility which checks `Path.exists()` on a mount point, `pyfakefs` can simulate both mounted and unmounted states cleanly.

**Open questions:**
- Does the SyncDetector use any `inotify`/`kqueue` low-level calls, or only `os.stat`/`mtime`? (Check `src/objlib/sync/` before choosing approach)
- Should scanner tests be in `tests/unit/` or `tests/integration/` given they require filesystem simulation?

**Confidence:** ✅ Both providers agreed this is blocking

---

### ✅ 2. API Mocking Strategy for Tests (Consensus)

**What needs to be decided:**
Plans 06.3-01 through 06.3-04 cannot make API calls to Gemini or Mistral. But search, citation, reranking, and synthesis modules all call these APIs. What level of mocking creates useful tests without just asserting "if mock returns X, function returns X"?

**Why it's ambiguous:**
Pure mocking (unittest.mock patches) produces tests that pass even when the real API contract changes. But contract testing requires a running mock server which is heavyweight for a test suite. The right balance depends on what each plan is actually testing.

**Provider synthesis:**
- **Gemini:** Key insight — distinguish *data plumbing* tests from *intelligence* tests. Plumbing tests (prompt construction, response parsing, error handling, citation mapping) are valuable. Intelligence tests (does Gemini return good results) are not testable without the API.
- **Perplexity:** Proposed a two-tier strategy: `unittest.mock` for unit tests (business logic given a response), lightweight Flask mock server for integration tests. Also emphasized testing prompt construction and response format parsing separately.

**Proposed implementation decision:**
Two-tier mocking strategy:
1. **Unit tests** — `unittest.mock.patch` at the API client boundary. Tests verify: prompt template filled correctly, response JSON parsed correctly, malformed JSON handled gracefully, citation IDs mapped to file records correctly. Do NOT test that the AI returns good results.
2. **Integration path** — For the reranker and synthesizer, use deterministic mock responses with known content to verify the scoring/MMR/diversity logic is correct independent of AI quality.

The Gemini search client (`src/objlib/search/`) has a clear API boundary where mocking is appropriate.

**Open questions:**
- Is the citation enrichment logic (`enrich_citations()`) tightly coupled to the raw Gemini response format, or does it operate on an intermediate data model? (Tight coupling means mocking must precisely replicate Gemini's actual response structure)
- Should `MockGeminiSearchClient` be a shared fixture or inline per test?

**Confidence:** ✅ Both providers agreed this is blocking

---

### ✅ 3. Schema Migration Test Scope (Consensus)

**What needs to be decided:**
Plan 06.3-01 specifies "database schema migrations V1→V7." This requires knowing what V1 through V6 schemas look like. The database module likely has only the current schema, not historical migration scripts.

**Why it's ambiguous:**
If the codebase only contains the current V7 schema (via `CREATE TABLE IF NOT EXISTS` + `ALTER TABLE` attempts), there are no discrete V1→V2, V2→V3 migration artifacts to test sequentially. We'd be fabricating a test history that doesn't correspond to real migrations.

**Provider synthesis:**
- **Gemini:** If migration scripts don't exist as discrete artifacts, change scope to "Schema Initialization" (V7 from scratch) and "Schema Verification" (all tables, columns, and constraints match expectations). Don't fabricate fake history.
- **Perplexity:** Agreed — use a schema factory pattern to create databases at specific versions on-demand. Note that the database module uses `try/except` for `ALTER TABLE` (per STATE.md `[06-01]`) which is the migration mechanism.

**Proposed implementation decision:**
Audit the database module first (plan 06.3-01 should start by reading `src/objlib/database.py`). The migration strategy is:
- If each version is initialized by cumulative `IF NOT EXISTS` + `ALTER TABLE` calls in one function: test the final V7 schema initialization and verify all tables/columns/triggers exist correctly
- If version-specific SQL strings exist: test applying them sequentially
- Specifically test: all 7 schema versions' key tables are present, foreign keys work, triggers fire on relevant events, CRUD methods work on the final schema

The goal is "prove the schema is correct" not "prove each migration was applied."

**Open questions:**
- Does `Database.__init__` apply all migrations on every open, or is it idempotent per version?
- Are there schema-level triggers that need testing (e.g., cascade deletes, update triggers)?

**Confidence:** ✅ Both providers agreed this is blocking

---

### ✅ 4. Canon.json Template Design for canon-init (Consensus)

**What needs to be decided:**
Plan 06.3-06 implements the `canon-init` skill with a `Canon.json.template`. The template must be flexible enough to work for GSD/Ralph/BMAD/Generic projects, but concrete enough to produce a valid Canon.json for the detected workflow type.

**Why it's ambiguous:**
The CANON-CONTEXT.md defines the Canon.json schema (using canon.so's format with `folders`, `excludeFolders`, `excludeFiles`, `rules`, `previousVersions`, `branchVersions`). But the template must be workflow-parameterized: a GSD project has different public surface hints than a Ralph project. The template variables and parameterization strategy are not yet defined.

**Provider synthesis:**
- **Gemini:** Proposed a single template with `"workflow"` field added; the skill fills in the workflow-specific values. Noted that `Canon.json` lives at project root (canon.so convention).
- **Perplexity:** Proposed separate per-workflow Layer 2 templates with a shared Layer 1 baseline. The baseline stays in `~/.claude/skills/canon-init/templates/` and is combined with workflow-specific additions.

**Proposed implementation decision:**
Use a single `Canon.json.template` with placeholder variables that `canon-init` fills in:
```
PROJECT_TITLE, PROJECT_DESCRIPTION, BRANCH,
PUBLIC_FOLDERS, EXCLUDE_FOLDERS, EXCLUDE_FILES, RULES
```
The `RULES` array is the most workflow-sensitive part. Have workflow-specific `rules-{workflow}.md` reference files that contain the standard rule set for that workflow type. The skill reads the appropriate rules file and populates the template.

Also: the `src/objlib/services/` folder listed in the current `Canon.json` does not yet exist (it's a Phase 7 artifact). The template should include a comment/note that public service layer folders should be added when the service layer is built.

**Open questions:**
- Should the template include a `_workflow` metadata field (not part of canon.so schema but useful for `canon-update` to know which workflow flavor generated this Canon.json)?
- Should `client-interface.md` be generated by `canon-init` or by a separate step in the skill?

**Confidence:** ✅ Both providers agreed this requires decision before implementation

---

### ✅ 5. Workflow Detection Before Research Wave (Consensus)

**What needs to be decided:**
Plan 06.3-06 (`canon-init`) must detect GSD/Ralph/BMAD from project files. But plan 06.3-05 is the research wave that defines what Ralph and BMAD control documents look like. Plans 06-07 depend on 05. The detection signals for Ralph and BMAD are currently "TBD from research wave" (CANON-CONTEXT.md §5).

**Why it's ambiguous:**
The dependency is clear: detection logic cannot be hardcoded until the research is done. But the skill needs to be designed with a detection interface that can accommodate whatever signals the research discovers.

**Provider synthesis:**
- **Gemini:** Priority-based detection with fallback: (1) explicit override in existing Canon.json, (2) BMAD config files, (3) GSD artifacts, (4) Ralph artifacts, (5) Generic fallback
- **Perplexity:** Staged algorithm with confidence scoring. Multiple framework signals present → report ambiguity or ask user. GSD already has known signals: `.planning/STATE.md` + `.planning/ROADMAP.md`.

**Proposed implementation decision:**
Design the skill with a **pluggable detector interface**. Plan 06.3-05 (research) produces `canon-init/workflows/gsd.md`, `ralph.md`, `bmad.md`, `generic.md` reference files that document detection signals. Plan 06.3-06 (implementation) reads these files and implements the detection logic. The detection interface is:

```
detect_workflow(project_root) → {workflow: "gsd"|"ralph"|"bmad"|"generic", confidence: float, signals: []}
```

Known GSD signals (from CANON-CONTEXT.md): `.planning/STATE.md` + `.planning/ROADMAP.md` present. Ralph and BMAD signals defined by plan 05 research.

**Open questions:**
- Should the detection algorithm be encoded in the SKILL.md itself, or in the workflow/*.md reference files? (SKILL.md is cleaner; reference files are data, not logic)
- If neither GSD/Ralph/BMAD is detected, should `canon-init` default to Generic and proceed, or pause and ask?

**Confidence:** ✅ Both providers agreed on the dependency ordering

---

### ⚠️ 6. Test Coverage Targets (Recommended)

**What needs to be decided:**
The phase plan specifies "run full test suite, verify no regressions" but gives no coverage targets. Without explicit targets, "adequate" coverage is undefined.

**Why it's ambiguous:**
Coverage targets set expectations for plan scope. Higher targets require more test cases. No target means the planner doesn't know when a plan is done.

**Provider synthesis (Perplexity):** Proposed two-dimensional framework:
- Structural coverage: 85% line, 80% branch minimum
- Functional coverage: specific test categories as checklists

**Proposed implementation decision:**
Set practical targets for this retroactive phase:
- **Line coverage target: 80%** across all modules covered by plans 01-04 (not project-wide)
- **Branch coverage:** measured but not gated (retroactive tests can't easily hit all error paths)
- **Functional completeness:** each plan defines a checklist of behaviors that must have at least one test

These are measured with `pytest --cov` and `coverage html`. The goal is "meaningful coverage" not "coverage score."

**Open questions:**
- Should coverage be measured per-module or as an aggregate?
- Include or exclude `cli.py` from coverage measurement?

**Confidence:** ⚠️ Single provider but clear rationale

---

### ⚠️ 7. Test Fixture Organization (Recommended)

**What needs to be decided:**
Four plans (01-04) each need in-memory SQLite fixtures. If each plan creates its own database setup, there will be fixture duplication and inconsistency across test files.

**Why it's ambiguous:**
A shared `conftest.py` avoids duplication but creates coupling between test files. Separate per-plan fixtures are simpler but create drift.

**Provider synthesis (Perplexity):** Proposed hierarchical conftest.py:
```
tests/conftest.py      ← shared fixtures (db connection, in-memory db)
tests/unit/conftest.py ← fast fixtures (fresh db per test)
tests/integration/conftest.py ← transaction rollback fixtures
```

**Proposed implementation decision:**
Create a root `tests/conftest.py` with:
- `in_memory_db()` fixture that returns an initialized in-memory SQLite connection (full V7 schema)
- `populated_db()` that inserts a small set of canonical test fixtures (5-10 files with known metadata)
- `mock_gemini_client()` that returns a `MagicMock` with deterministic search results

Each plan's tests import from this shared set. Individual test modules can define additional fixtures as needed. Use `scope="function"` (fresh DB per test) by default for isolation.

**Open questions:**
- Should the schema initialization in `conftest.py` call the real `Database.initialize()` method (to test it works) or apply a hardcoded schema string?

**Confidence:** ⚠️ Single provider but clearly correct for test maintainability

---

### ✅ 8. Skill Architecture: Skills Not MCP Servers (Resolved)

**What needs to be decided:**
Already decided in CANON-CONTEXT.md §5.

**Decision:** Claude Code Skills (`~/.claude/skills/` directory-based, SKILL.md entry point) — NOT MCP servers. Gemini suggested MCP servers, but this conflicts with the prior design decision. Claude Code skills support templates and supporting files which is what canon-init and canon-update need.

**Confidence:** ✅ Resolved by CANON-CONTEXT.md

---

## Summary: Decision Checklist

Before planning, confirm:

**Tier 1 (Blocking — must decide for plans 01-04):**
- [ ] Filesystem simulation: pyfakefs vs. patching individual functions
- [ ] API mock level: what the mock should return, how to structure MockGeminiClient
- [ ] Schema migration scope: test cumulative migrations or final schema only

**Tier 2 (Important — must decide for plans 06-07):**
- [ ] Canon.json template structure: single template with variables vs. per-workflow templates
- [ ] Detection algorithm placement: SKILL.md logic vs. workflow/*.md data files
- [ ] Fallback behavior: auto-default Generic or ask user on ambiguity

**Tier 3 (Polish — can refine during implementation):**
- [ ] Coverage targets: 80% line minimum per module
- [ ] Fixture organization: shared conftest.py hierarchy

---

## Next Steps (YOLO Mode)

1. ✅ CONTEXT.md created
2. ✅ CLARIFICATIONS-NEEDED.md generated
3. ✅ CLARIFICATIONS-ANSWERED.md auto-generated (YOLO)
4. ⏭ Proceed to `/gsd:plan-phase 6.3`

---

*Multi-provider synthesis by: Gemini Pro, Perplexity Sonar Deep Research*
*Generated: 2026-02-18 (YOLO mode)*
