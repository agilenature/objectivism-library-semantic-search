# CLARIFICATIONS-ANSWERED.md

## Phase 3: Search & CLI ‚Äî Stakeholder Decisions

**Generated:** 2026-02-16
**Mode:** YOLO (balanced strategy)
**Source:** Auto-generated by discuss-phase-ai

---

## Decision Summary

**Total questions:** 11 gray areas
**Tier 1 (Blocking):** 5 answered
**Tier 2 (Important):** 4 answered
**Tier 3 (Polish):** 2 answered

---

## Tier 1: Blocking Decisions

### Q1: Hybrid Search Strategy ‚Äî Semantic-Only or Dual-System?

**Question:** Should Phase 3 implement semantic-only search (Gemini File Search alone) or hybrid search combining semantic (Gemini) with keyword/full-text search (SQLite FTS5)?

**YOLO DECISION:** **Option A - Semantic-only search (Gemini File Search)**

**Rationale:**
- Confidence level: ‚úÖ Consensus (Gemini, Perplexity)
- Philosophy library use case is primarily conceptual research, not keyword-based queries
- Reduces Phase 3 scope and architectural complexity
- Faster to implement (no SQLite FTS5 integration, no RRF algorithm needed)
- Defer hybrid search to Phase 4 if user feedback indicates keyword queries fail
- Strategy: Balanced (pick recommended MVP option)

**Sub-decisions:**
- **Is keyword search a hard requirement?** **No, deferred to Phase 4** ‚Äî Philosophy researchers ask conceptual questions ("What is Rand's view of rationality?"), not keyword searches. If Phase 3 user testing reveals keyword search gaps, we'll implement hybrid in Phase 4.
- **Acceptable latency?** **10-30 seconds for complex queries** ‚Äî Philosophy researchers tolerate delays for comprehensive results. We'll implement 30-second global timeout with retry feedback.
- **If hybrid needed later, weighting strategy?** **Semantic should dominate** ‚Äî Use Reciprocal Rank Fusion (RRF) in Phase 4, but weight semantic results higher than keyword (e.g., 70/30 split).

---

### Q2: Metadata Filtering Architecture ‚Äî Where to Apply Filters?

**Question:** Should metadata filters (course, year, difficulty, topic) be applied via Gemini's server-side `metadata_filter` parameter, via client-side SQLite queries, or both in combination?

**YOLO DECISION:** **Option C - Hybrid two-stage filtering (Gemini + SQLite)**

**Rationale:**
- Confidence level: ‚úÖ Consensus (OpenAI, Perplexity)
- Simple single-field filters (year, course) ‚Üí Gemini `metadata_filter` parameter for pre-filtering (reduces vector space)
- Complex multi-field filters (range queries, pattern matching) ‚Üí SQLite client-side post-filter for flexibility
- Best balance: Gemini handles what it's good at (simple equality filters), SQLite handles what it's good at (complex SQL queries)
- Validate filter behavior empirically through A/B testing during implementation
- Strategy: Balanced (recommended by 2 providers)

**Sub-decisions:**
- **What if Gemini and SQLite metadata diverge?** **Tolerate temporary divergence, log warnings** ‚Äî SQLite is source of truth. If counts diverge >5%, log warning but don't block. Provide `library sync-metadata` command to reconcile if needed.
- **Support compound boolean filters in Gemini?** **Yes, simple AND only** ‚Äî Support `year=2023 AND course="OPAR"` in Gemini filter syntax. Complex OR/NOT logic handled client-side via SQLite.
- **Is metadata consistency a hard requirement?** **No, eventual consistency acceptable** ‚Äî Philosophy library doesn't require real-time consistency. Metadata updated during scan (Phase 1), synced during upload (Phase 2).

---

### Q3: Passage-Level Citation Format ‚Äî How to Display Citations in Terminal?

**Question:** How should passage-level citations be formatted and displayed in the CLI? Inline citations, footnotes, or multi-tier display? What metadata should be included?

**YOLO DECISION:** **Option C - Three-tier progressive disclosure**

**Rationale:**
- Confidence level: ‚úÖ Consensus (all 3 providers)
- Tier 1: Inline citations `[1][2]` in response text for immediate context
- Tier 2: Citation details panel below response (passage excerpts 100-150 chars, course/year/quarter)
- Tier 3: Full source listing from SQLite (complete metadata: course, year, quarter, difficulty, instructor, topic)
- Use Rich `Panel` for citation details, `Table` for source listing
- Best balance of scannability (compact inline) and comprehensiveness (full metadata on-demand)
- Strategy: Balanced (unanimous recommendation)

**Sub-decisions:**
- **Excerpt length?** **100-150 characters** ‚Äî Fits in 80-100 char terminal width with wrapping. Truncate at word boundary, add "..." indicator. Users can request full text via `library view <doc_id>`.
- **Validate Gemini citations?** **Yes, check cited files exist** ‚Äî After parsing `groundingChunks`, verify each file URI exists in SQLite metadata. Log warning if mismatch, display "Source unavailable" to user.
- **Auto-discover cross-references?** **No, on-demand only (see Q9)** ‚Äî Cross-references shown only when user runs `library view <result_id> --show-related`.

---

### Q4: CLI Command Structure ‚Äî Separate Commands or Unified Interface?

**Question:** Should `search`, `filter`, and `browse` be implemented as separate commands, subcommands of unified `query`, or flags on single `find` command? Should filters be stateful (persistent) or stateless (explicit flags)?

**YOLO DECISION:** **Option A - Three separate commands (search, filter, browse), stateless filters**

**Rationale:**
- Confidence level: ‚úÖ Consensus (Perplexity, Gemini)
- `library search "query" [--filter field:value]` ‚Äî Primary interaction, semantic search with optional metadata filters
- `library filter field:value` ‚Äî Metadata-only query, lists documents matching filters without semantic ranking
- `library browse [--course X]` ‚Äî Structural navigation, interactive drill-down (courses ‚Üí years ‚Üí quarters ‚Üí documents)
- All filters are explicit flags (stateless), not persistent state across commands
- Clear separation of concerns, easy to discover, scriptable
- Strategy: Balanced (recommended pattern)

**Sub-decisions:**
- **Should filters persist across searches?** **No, stateless only** ‚Äî All filters are explicit CLI flags. Simpler, more predictable, better for scripting. No hidden state between commands.
- **CLI primarily interactive or scripted?** **Both, prioritize scriptable** ‚Äî Design for automation first (explicit flags, JSON output option), but provide Rich formatting for interactive use.
- **Should browse show previews or just names/counts?** **Names and counts only** ‚Äî Compact list: "Intro to Epistemology (2022): 15 files, 3 quarters". Users run `library browse --course "Intro to Epistemology" --year 2022` to drill down, then `library view <doc_id>` for previews.

---

### Q5: Error Handling and Retry Strategy ‚Äî How to Handle API Failures?

**Question:** When Gemini API returns errors (429 rate limits, timeouts, 5xx), should the CLI retry automatically, show errors immediately, or fall back to alternative methods (SQLite search)?

**YOLO DECISION:** **Option B - Automatic retry with exponential backoff + user feedback**

**Rationale:**
- Confidence level: ‚úÖ Consensus (all 3 providers)
- Retry up to 3 attempts with exponential backoff: 0.5s, 1s, 2s (doubles each time)
- Add random jitter (¬±50%) to prevent thundering herd when multiple clients retry simultaneously
- Display retry status via Rich: "[yellow]‚ü≥ Retrying search (1/3) in 0.8s...[/yellow]"
- Respect `Retry-After` header on 429 responses (use header value instead of exponential if provided)
- Global timeout: 30 seconds for all API calls (fail fast after 30s)
- Optional circuit breaker deferred to Phase 4 (after 5 consecutive failures, disable Gemini for 60s cooldown)
- Strategy: Balanced (industry standard pattern)

**Sub-decisions:**
- **Should system silently degrade to SQLite?** **No, always inform user** ‚Äî If Gemini unavailable after retries, display error message with actionable guidance: "Gemini API unavailable. Try again later or contact support." Do NOT silently fall back to different search mode.
- **Acceptable timeout?** **30 seconds** ‚Äî Philosophy researchers tolerate delays for comprehensive results. Display progress: "[cyan]‚è≥ Searching library (15s)...[/cyan]" at 5s, 10s, 15s intervals.
- **Retry indefinitely or give up?** **Give up after 3 attempts** ‚Äî After 3 retries (total ~8s of waiting), display error and exit with code 1. User can manually retry if transient issue.

---

## Tier 2: Important Decisions

### Q6: Gemini Authentication Strategy

**Question:** How should the CLI obtain and store Gemini API credentials? System keychain (like Phase 2), environment variables, config file, or combination?

**YOLO DECISION:** **Option A - System keychain + environment variable fallback**

**Rationale:**
- Confidence level: ‚ö†Ô∏è Recommended (2 providers, consistent with Phase 2 decision 02-03)
- Primary: Read from system keychain using keyring library (`keyring.get_password("objlib-gemini", "api_key")`)
- Fallback: Try `GEMINI_API_KEY` environment variable if keychain fails
- Most secure (credentials not in files or version control)
- Consistent with Phase 2 upload pipeline authentication
- Display actionable error if no key found: "Run: library config set-api-key"
- Strategy: Balanced (continuity with existing system)

**Sub-decisions:**
- **Support `.env` files via python-dotenv?** **No** ‚Äî Keep it simple. Users can set env vars in shell profile. Adding `.env` support creates security risk (accidentally committing .env files).
- **Support Google ADC (service accounts)?** **No, defer to Phase 5** ‚Äî API key authentication sufficient for personal use (v1 scope). ADC support deferred to future multi-user/deployment scenarios.
- **Expected to run in CI/CD?** **No, personal tool only** ‚Äî v1 is single-user CLI on local machine. CI/CD integration out of scope.

---

### Q7: Result Ranking and Score Display

**Question:** Should the CLI display raw Gemini relevance scores, normalize them to 0-100%, or hide scores entirely? Should secondary ranking criteria (difficulty, recency) be applied?

**YOLO DECISION:** **Option A - Gemini-native ranking only, normalize scores to 0-100% for display**

**Rationale:**
- Confidence level: ‚ö†Ô∏è Recommended (Gemini as Phase 3 MVP, defer multi-factor to Phase 4)
- Use Gemini's native semantic ranking (preserve vector similarity order)
- Normalize cosine similarity (0-1) to percentage: `int(cosine_similarity * 100)`
- Display visual bar graph using Rich: `‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚óã‚óã 87%`
- Stable tie-breaker: higher semantic score ‚Üí recency (upload_timestamp) ‚Üí alphabetical (file_path)
- Defer multi-factor ranking (semantic + difficulty + recency) to Phase 4 if users report difficulty-ordering issues
- Strategy: Balanced (start simple, iterate based on feedback)

**Sub-decisions:**
- **Allow users to customize ranking weights?** **No, not in Phase 3** ‚Äî Fixed algorithm for MVP. If Phase 4 implements multi-factor ranking, consider exposing weights as config file options (not CLI flags).
- **How to break ties?** **Higher score ‚Üí recency ‚Üí alphabetical** ‚Äî If two results have identical semantic score (rare), prefer more recent upload. If same timestamp, sort alphabetically by file_path.
- **Is 0-100% scale misleading?** **No, with visual bar graph** ‚Äî Percentage alone is abstract, but combined with bar graph (`‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚óã‚óã`) provides intuitive visual feedback. Document in `--help` that 60-70% is "good" for philosophy domain.

---

### Q8: Chunk Size and Context Extraction

**Question:** Should Phase 3 configure Gemini's chunk size via `chunking_config`, or accept defaults? How should excerpts be truncated for terminal display?

**YOLO DECISION:** **Option A - Accept Gemini defaults (no custom chunking)**

**Rationale:**
- Confidence level: ‚ö†Ô∏è Recommended (Perplexity as Phase 3 approach)
- Use Gemini's automatic chunking without configuration
- Avoid premature optimization (optimal chunk size for philosophy texts unknown)
- Gather user feedback during Phase 3 testing on whether excerpts are too fragmented or too verbose
- If needed in Phase 4, conduct empirical testing (NVIDIA methodology) to find optimal chunk size (likely 800-1200 tokens)
- Truncate excerpts at 100-150 chars for display, word boundary, add "..." indicator
- Strategy: Balanced (defer optimization until proven necessary)

**Sub-decisions:**
- **Acceptable preview length?** **100-150 characters, adapt to terminal width** ‚Äî Query terminal width via `shutil.get_terminal_size()`, truncate excerpts to `width - 20` (leave margin). Minimum 80 chars even on narrow terminals.
- **Respect conceptual boundaries?** **Not in Phase 3** ‚Äî Gemini's auto-chunking may not respect paragraph boundaries. If users complain about mid-sentence chunks, implement custom chunking in Phase 4 with paragraph detection.
- **Allow full document view?** **Yes, via `library view <doc_id> --full`** ‚Äî Command opens document in pager (less/more style) with Rich syntax highlighting for markdown.

---

### Q9: Cross-Reference Discovery Scope

**Question:** How should SRCH-08 "cross-reference discovery" be implemented? Automatically for all results, or on-demand when user selects a result?

**YOLO DECISION:** **Option A - On-demand "More like this"**

**Rationale:**
- Confidence level: ‚ö†Ô∏è Recommended (OpenAI, Gemini)
- Do NOT show cross-references automatically in main search results (avoids N+1 query problem)
- Provide detail command: `library view <result_id> --show-related`
- When user requests, query Gemini for similar documents using selected document's text as query
- Return top 5 related documents with similarity scores
- Phase 4 enhancement: Cache related results in SQLite if query frequency justifies pre-computation
- Strategy: Balanced (on-demand reduces cost and latency)

**Sub-decisions:**
- **Is SRCH-08 required for Phase 3 acceptance?** **Yes, but minimal implementation acceptable** ‚Äî On-demand cross-references satisfy "find related discussions automatically" requirement. "Automatically" means "without manual keyword search," not "without user trigger."
- **Cross-references within same course or across library?** **Across whole library** ‚Äî Show related documents regardless of course. Rich context: if "Intro to Epistemology (2022)" cites similar concept from "Advanced Metaphysics (2023)," user should see the connection.
- **Explain why related (shared terms)?** **Not in Phase 3** ‚Äî Just show similarity score (0-100%). If users request explanations in Phase 4, implement term highlighting or topic overlap display.

---

## Tier 3: Polish Decisions

### Q10: Typer CLI State Management Pattern

**Question:** How should the Gemini client and SQLite connection be stored and passed across Typer commands? Typed dataclass, context object, or singleton pattern?

**YOLO DECISION:** **Option A - Typed AppState dataclass in callback**

**Rationale:**
- Confidence level: üîç Needs clarification (1 provider, but best practice)
- Create `@dataclass AppState(gemini_client: Client, sqlite_db: Connection, store_name: str, config: dict)`
- Initialize in `@app.callback()`, attach to `ctx.obj`
- Type-hinted helper: `def get_app_state(ctx: typer.Context) -> AppState` for IDE autocompletion
- Best type safety (no untyped `ctx.obj`), easy to test (pass mock AppState)
- Consistent with Python best practices for Typer applications
- Strategy: Balanced (simplest + safest option)

**Sub-decisions:**
- **Configuration from file or environment variables?** **Both, with precedence** ‚Äî Read config from `~/.objlib/config.json` (file locations, store name), environment variables override file (e.g., `OBJLIB_DB_PATH` overrides config.json `db_path`).
- **API credentials in config file or env vars?** **System keychain, never config file** ‚Äî Credentials always from keychain (primary) or env var (fallback). Never store in config.json (security risk).
- **Support multiple File Search stores?** **No, single store only** ‚Äî v1 is single-library tool. Multi-store support deferred to v2 if users request library switching.

---

### Q11: Result Display Layout Hierarchy

**Question:** How should search results balance information density with terminal readability? Show all metadata in compact list, or progressive disclosure (compact ‚Üí detailed ‚Üí full)?

**YOLO DECISION:** **Option C - Three-tier progressive disclosure**

**Rationale:**
- Confidence level: üîç Needs clarification (1 provider, but aligns with Q3 citation pattern)
- Tier 1: Compact list for quick scanning (rank, title, score bar, course/year)
  ```
  1. "Introduction to Objectivism"  [87%] ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë  ‚îÇ Epistemology, 2022
  ```
- Tier 2: Detailed view via `library view <result_id>` (all metadata, excerpt, citations)
  - Rich Panel with course, year, quarter, difficulty, instructor, topic
  - Excerpt (100-150 chars) with syntax highlighting
  - Citation panel (see Q3)
- Tier 3: Full document via `library view <doc_id> --full` (entire file in pager)
- Strategy: Balanced (progressive disclosure standard UX pattern)

**Sub-decisions:**
- **Compact list interactive or just printed?** **Just printed table** ‚Äî Non-interactive for simplicity and scriptability. Users copy result ID and run `library view 3`. Defer interactive selection (arrow keys) to Phase 4 if requested.
- **Excerpt length?** **100-150 characters** ‚Äî See Q8 answer. Truncate at word boundary, terminal width adaptive.
- **Show difficulty/instructor in compact view?** **No, detailed view only** ‚Äî Compact list shows minimal info (rank, title, score, course/year). Difficulty, instructor, topic shown in Tier 2 detailed view. Keeps compact list scannable.

---

## Next Steps

1. ‚úÖ **Clarifications answered** (YOLO mode)
2. ‚è≠ **Proceed to planning:** `/gsd:plan-phase 3`
3. üìã **Review YOLO decisions before implementation** if needed

---

## Summary of Key Decisions

**Architecture:**
- Semantic-only search (no hybrid in Phase 3)
- Hybrid two-stage metadata filtering (Gemini + SQLite)
- Three separate CLI commands (search, filter, browse)
- System keychain authentication with env var fallback

**UX:**
- Three-tier progressive disclosure (compact ‚Üí detailed ‚Üí full)
- Three-tier citation display (inline ‚Üí panel ‚Üí full sources)
- Automatic retry with exponential backoff + user feedback
- On-demand cross-references (not automatic)

**Technical:**
- Accept Gemini default chunking (no custom config)
- Gemini-native ranking with 0-100% normalized scores
- Typed AppState dataclass for Typer state management
- Stateless filters (explicit flags, no persistence)

---

*Auto-generated by discuss-phase-ai --yolo (balanced strategy)*
*Human review recommended before final implementation*
*Based on multi-provider synthesis: OpenAI gpt-5.2 + Gemini Pro + Perplexity Sonar Deep Research*
