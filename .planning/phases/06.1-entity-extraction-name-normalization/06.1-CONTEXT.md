# CONTEXT.md ‚Äî Phase 6.1: Entity Extraction & Name Normalization

**Generated:** 2026-02-16
**Phase Goal:** User can automatically extract and normalize person names mentioned in transcripts against a canonical list of Objectivist philosophers and ARI instructors -- transforming raw text mentions into structured, searchable entity metadata
**Synthesis Source:** Multi-provider AI analysis (OpenAI gpt-5.2, Gemini Pro)

---

## Overview

Phase 6.1 adds entity extraction to identify and normalize mentions of 15 canonical Objectivist philosophers and ARI instructors in transcript files. This transforms raw text mentions ("Rand", "Dr. Peikoff", "Onkar") into structured metadata for search filtering and discovery.

**Canonical List (15 people):**
1. Ayn Rand (philosopher)
2. Leonard Peikoff (ARI instructor)
3. Onkar Ghate (ARI instructor)
4. Robert Mayhew (ARI instructor)
5. Tara Smith (ARI instructor)
6. Ben Bayer (ARI instructor)
7. Mike Mazza (ARI instructor)
8. Aaron Smith (ARI instructor)
9. Tristan de Li√®ge (ARI instructor)
10. Gregory Salmieri (ARI instructor)
11. Harry Binswanger (ARI instructor)
12. Jean Moroney (ARI instructor - frequent in Meeting of Minds)
13. Yaron Brook (ARI instructor)
14. Don Watkins (ARI instructor)
15. Keith Lockitch (ARI instructor)

**Key challenges identified:**
- Disambiguation of shared surnames (Tara Smith vs Aaron Smith)
- Matching algorithm choice (LLM vs deterministic fuzzy matching)
- Data model design (SQLite vs Gemini metadata structure)
- Integration with existing Phase 6 metadata extraction pipeline
- Backfill strategy for 1,614+ already-processed transcripts

**Confidence markers:**
- ‚úÖ **Consensus** ‚Äî Both providers identified this as critical
- ‚ö†Ô∏è **Recommended** ‚Äî One provider identified, strong rationale
- üîç **Needs Clarification** ‚Äî Unique insight, potentially important

---

## Gray Areas Identified

### ‚úÖ 1. Disambiguation Rules: The Smith Problem (Consensus)

**What needs to be decided:**
How to resolve extracted entities when the transcript contains a surname shared by multiple people on the canonical list (specifically **Tara Smith** vs. **Aaron Smith**, and handling common surnames like "Smith" that may refer to non-canonical individuals).

**Why it's ambiguous:**
A raw text mention of "Smith" or "Dr. Smith" is structurally insufficient to distinguish between the two instructors. Incorrectly attributing a "Tara Smith" lecture reference to "Aaron Smith" degrades search reliability and user trust. The canonical list constraint implies we must map to a specific full name, but requirements don't specify handling rules for ambiguous partial mentions.

**Provider synthesis:**
- **OpenAI:** Recommends ambiguity safeguards with single-token surname blocks unless preceded by title+initial or in speaker labels. Proposes `blocked_alias` list for overly generic aliases.
- **Gemini:** Proposes "Strict Mode for Collisions" requiring full name appearance or instructor metadata match. Suggests unresolved fallback (log as generic or drop) to avoid data poisoning.

**Proposed implementation decision:**
Implement **conservative disambiguation** with multi-stage validation:

1. **Single-token surnames blocked by default:** "Smith" alone is NOT accepted as a valid mention
2. **Disambiguation triggers:**
   - Full name appears in transcript ("Tara Smith", "Aaron Smith") ‚Üí map to that person
   - Instructor metadata field (from Phase 6) matches one canonical person ‚Üí map to that person
   - Title + initial pattern ("Dr. T. Smith", "Prof. Aaron Smith") + context score high ‚Üí map
   - Speaker label exact match ("Tara Smith:", "Aaron Smith:") ‚Üí map
3. **Unresolved handling:** If "Smith" appears but cannot be disambiguated, do NOT normalize it; log as "ambiguous_mention" for later review
4. **Blocked alias list:** Maintain explicit list of too-generic aliases: ["Smith", "Aaron", "Tara", "Ben", "Mike", "Harry", "Greg"]

**Open questions:**
- Are there known non-canonical "Smith" references in the corpus (e.g., Adam Smith in philosophy comparisons)?
- Should we allow "Tara" alone to map to Tara Smith if no other Tara exists in transcripts?
- Do stakeholders prefer missing some true mentions over any false positives?
- Does "Tristan de Li√®ge" appear in transcripts as "Tristan" or "de Li√®ge"? (Need sample file check)

**Confidence:** ‚úÖ Both providers agreed this is blocking

---

### ‚úÖ 2. Extraction Engine & Matching Policy (Consensus)

**What needs to be decided:**
The algorithmic approach for extracting and matching raw mentions to canonical persons: whether to use LLM-based extraction (Mistral API) or deterministic fuzzy matching (Python libraries), and what thresholds define "match" vs "unknown".

**Why it's ambiguous:**
Requirements say "fuzzy matches" and "spelling variations, nicknames, partial names" but don't specify acceptable false positives/negatives, cost constraints, or whether LLM calls are allowed. Running 1,600+ full transcripts through Mistral consumes massive context windows and hits rate limits, but deterministic matching may miss context-dependent mentions.

**Provider synthesis:**
- **OpenAI:** Proposes two-stage deterministic-first pipeline: exact match ‚Üí alias match ‚Üí fuzzy match (RapidFuzz ‚â•92 threshold) ‚Üí LLM fallback for 80-91 range with "none" option. Controls cost while handling tricky mentions.
- **Gemini:** Strongly recommends **Hybrid/Deterministic-First** using RapidFuzz and RegEx. Notes canonical list is small (15 names) and highly unique (Peikoff, Ghate, Binswanger), suggesting deterministic is 100x faster and sufficiently accurate. Proposes "uniqueness" weight scoring.

**Proposed implementation decision:**
Use **deterministic-first with controlled LLM fallback**:

**Stage A (Deterministic - Fast & Free):**
1. Text normalization: casefold, strip punctuation, Unicode normalize
2. Exact match to `person.canonical_name` (e.g., "Leonard Peikoff")
3. Exact match to `person_alias.alias_text` (e.g., "Peikoff", "Dr. Peikoff", "LP")
4. Fuzzy match using RapidFuzz `token_set_ratio` against canonical+aliases:
   - Threshold ‚â•92: Accept as high-confidence match
   - Threshold 80-91: Flag for Stage B review
   - Threshold <80: Reject (not a mention)

**Stage B (LLM Fallback - Expensive & Contextual):**
- Only invoke for fuzzy scores in 80-91 range AND context window suggests person reference
- Send ¬±200 chars context + canonical list (15 names) to Mistral
- Require structured JSON response: `{"person_id": "ayn-rand" | "none"}`
- Temperature: 0.1 (deterministic)
- Max retries: 2 on transient failures

**Uniqueness weighting:**
- High-uniqueness surnames (Peikoff, Ghate, Binswanger, Mayhew, Salmieri, Mazza, Li√®ge): Match on surname alone
- Common first names (Ben, Mike, Harry, Greg, Tara, Aaron): Require surname or full context validation

**Open questions:**
- Is it acceptable to ever "guess" a person from partial mention (e.g., "Peikoff")? Or must we require full name?
- What's the tolerance for false positives in search UX (worse than false negatives)?
- Are LLM calls permitted for this step given cost/privacy constraints?
- Should we use temperature=0.0 or 0.1 for LLM fallback?

**Confidence:** ‚úÖ Both providers agreed deterministic-first is optimal

---

### ‚úÖ 3. Output Data Model: SQLite vs Gemini Metadata Structure (Consensus)

**What needs to be decided:**
The schema of extracted entities: just normalized names, or also mention counts, offsets, contexts, confidence? How to store in SQLite vs what to send to Gemini File Search (which has metadata structure constraints)?

**Why it's ambiguous:**
"Structured, searchable entity metadata" could mean many levels of detail. Gemini's `custom_metadata` prefers flat key-value pairs or lists of strings for filtering. Without a clear model, downstream indexing and UI features can't be implemented. Storing complex objects (e.g., `{"Ayn Rand": 15_mentions}`) might make filtering difficult.

**Provider synthesis:**
- **OpenAI:** Proposes two-layer storage: (1) Transcript-level summary table `transcript_entity` with counts/confidence/evidence_sample, (2) Optional detailed `entity_mention` table with offsets for highlighting (feature-flagged, can add later).
- **Gemini:** Recommends sending `List[str]` (unique names) to Gemini for Boolean filtering, but storing `Dict[str, int]` (mention counts) in SQLite for analytics. Separates search filtering (existence) from density ranking (counts).

**Proposed implementation decision:**
Implement **two-tier storage with SQLite primary, Gemini simplified**:

**SQLite Schema (Primary Storage):**

Table: `transcript_entity`
- `transcript_id` (FK to transcript)
- `person_id` (FK to person)
- `mention_count` (INT)
- `first_seen_char` (INT, offset of first mention)
- `max_confidence` (FLOAT 0.0-1.0)
- `evidence_sample` (TEXT, short snippet showing mention)
- `extraction_version` (TEXT, semantic version)

Table: `entity_mention` (Optional - Feature-Flagged for Phase 7)
- `transcript_id`, `person_id`, `start_char`, `end_char`, `surface_text`, `confidence`, `context_snippet`

**Gemini Metadata (Simplified for Search Filtering):**

Custom metadata field: `mentioned_entities`
- Type: List of strings
- Value: `["Ayn Rand", "Leonard Peikoff", "Onkar Ghate"]`
- Purpose: Enable queries like `metadata.mentioned_entities: "Onkar Ghate"`
- Rationale: Keeps filtering simple; counts stay in SQLite for analytics

**Open questions:**
- Do users need **highlighted mentions** in UI soon (requires offsets)?
- Should we store confidence per transcript-person or per mention?
- Is SQLite size/performance a concern if we store every mention span?
- What is the character limit for metadata values in Gemini? (If transcript mentions all 11, does it truncate?)

**Confidence:** ‚úÖ Both providers agreed on two-tier approach

---

### ‚úÖ 4. Workflow Integration & Persistence Strategy (Consensus)

**What needs to be decided:**
Where entity extraction runs in the pipeline relative to Phase 6 metadata extraction and Gemini File Search upload. Whether it's a mandatory gate, how to handle failures, and how to apply to already-processed files.

**Why it's ambiguous:**
"Add entity extraction before upload" doesn't specify whether it runs once per file, on reprocessing, or as a gate that can fail the upload. Since Phase 6 is complete and 281+ files are already processed/uploaded, unclear whether this backfills or only applies to new files.

**Provider synthesis:**
- **OpenAI:** Proposes mandatory pre-upload step with hard gate: parse ‚Üí extract entities ‚Üí validate ‚Üí write SQLite ‚Üí upload. If extraction fails, mark as `blocked_entity_extraction` and do NOT upload.
- **Gemini:** Recommends check for `if file_processed AND entities_extracted is NULL`. Extraction runs against local text file, updates SQLite DB, then file is queued for Gemini upload. Phase 6.1 output is primarily a DB update which Phase 6.2 (Enriched Upload) reads.

**Proposed implementation decision:**
Make entity extraction a **mandatory pre-upload step with graceful degradation**:

**Pipeline order:**
1. Parse transcript (existing Phase 1)
2. Run Phase 6 metadata extraction (existing)
3. **NEW:** Run entity extraction/normalization
4. Validate results (schema + referential integrity)
5. Write to SQLite (transaction: metadata + entities together)
6. Upload to Gemini with enriched metadata (Phase 6.2)
7. Mark file as uploaded

**Failure handling:**
- If entity extraction fails: Mark transcript state as `blocked_entity_extraction`, log error, do NOT upload
- If no entities found (valid case): Store empty set, mark `entities_done`, proceed to upload
- Pipeline continues with next file (fail-one-continue-batch)

**Backfill strategy for existing 281+ files:**
- Iterate all transcripts with `uploaded=true` and `entities_extracted IS NULL`
- Run entity extraction, write to SQLite
- Do NOT re-upload unless Phase 6.2 decides enriched metadata requires it
- Implement as separate CLI command: `objlib extract entities --backfill`

**Open questions:**
- Should uploads proceed if entity extraction fails (degraded mode), or is it a hard gate?
- Do we re-run entity extraction when canonical aliases are updated?
- Is there a "backfill entities for already-uploaded files" requirement?
- Do we need to store timestamps of mentions for future video seeking? (Proposal: No for v0.1.0)

**Confidence:** ‚úÖ Both providers agreed on pre-upload integration

---

### ‚úÖ 5. Backfill Strategy for Existing 1,614+ Processed Files (Consensus)

**What needs to be decided:**
Whether to retroactively extract entities for already-processed/uploaded transcripts and how to keep consistency. Phase 6 is complete for 281+ files; adding a pre-upload step doesn't automatically fix already-uploaded content.

**Why it's ambiguous:**
Current status says Phase 6 complete for many files. Adding Phase 6.1 creates a metadata gap: new uploads will have entity metadata, but existing uploads won't. This creates inconsistent user experience where only new uploads have entity filters.

**Provider synthesis:**
- **OpenAI:** Recommends backfill job iterating all transcripts with `uploaded=true` and `entities_done=false`. Run extraction, write SQLite. Do not re-upload unless search index depends on entity metadata inside Gemini.
- **Gemini:** Similar proposal with check for `entities_extracted IS NULL`. Notes Phase 6.1 output is primarily DB update which Phase 6.2 reads, so backfill can happen independently of re-upload decision.

**Proposed implementation decision:**
Implement **separate backfill job with controlled re-upload**:

**Backfill CLI command:**
```bash
objlib extract entities --backfill
```

**Backfill logic:**
1. Query: `SELECT * FROM transcript WHERE uploaded=true AND entities_extracted IS NULL`
2. For each transcript:
   - Read local text file (requires source disk connected)
   - Run entity extraction
   - Write results to `transcript_entity` table
   - Mark `entities_extracted=true`, `extraction_version=6.1.0`
3. Track: extracted_count, error_count, skipped_count (file not found)
4. Report: summary statistics, list of errors

**Re-upload decision (Phase 6.2):**
- Does Gemini File Search index need entity metadata embedded in uploaded document?
- If YES: Schedule controlled re-upload batch (expensive, breaks references)
- If NO: SQLite is primary search DB, Gemini only needs enriched metadata in Phase 6.2 for NEW uploads

**Open questions:**
- Does Gemini File Search index need entity metadata embedded in the uploaded document, or is SQLite the primary search DB?
- Is re-uploading 1,614 files acceptable (cost/time)?
- Should backfill be a one-time migration or a repeatable command?

**Confidence:** ‚úÖ Both providers agreed backfill is necessary

---

### ‚úÖ 6. Library Choices & Determinism (Consensus)

**What needs to be decided:**
Which libraries to use for fuzzy matching and how to keep results stable across environments. Python fuzzy matching can vary by library/version; LLM outputs can drift; reproducibility matters for trust and debugging.

**Why it's ambiguous:**
Multiple fuzzy matching libraries available (fuzzywuzzy, RapidFuzz, difflib, Levenshtein). Without pinned versions and deterministic settings, extraction results may vary across environments or updates, breaking trust and regression testing.

**Provider synthesis:**
- **OpenAI:** Recommends RapidFuzz pinned to specific version in requirements.txt, LLM fallback prompt versioned and stored in results, temperature 0 for LLM.
- **Gemini:** Strongly recommends RapidFuzz for speed (100x faster than LLM) given small canonical list. Notes unique surnames make deterministic matching highly accurate.

**Proposed implementation decision:**
Use **RapidFuzz with strict versioning and deterministic settings**:

**Dependency pinning:**
```python
# requirements.txt
rapidfuzz==3.6.1  # Exact version, not ~= or >=
```

**Deterministic settings:**
- RapidFuzz: Use `token_set_ratio` scorer (order-independent)
- LLM fallback: Temperature 0.1 (minimal randomness)
- Prompt versioning: Store `prompt_version` field in extraction results
- Seed: Set random seed for any sampling operations

**Regression testing:**
- Gold set: 20-30 manually verified transcripts with expected entities
- Test suite: Assert exact entity lists match expected (no fuzzy tolerance)
- Run on every PR to catch drift

**Open questions:**
- Is strict reproducibility a requirement, or is "generally correct" acceptable?
- Do we need automated regression tests with a gold set of transcripts?
- Should we forbid LLM usage entirely to maximize determinism?

**Confidence:** ‚úÖ Both providers agreed on RapidFuzz + determinism

---

### ‚ö†Ô∏è 7. Canonical Entity Registry: Source of Truth + Identifier Strategy (Recommended)

**What needs to be decided:**
Where the canonical list lives, how it's versioned, and what stable IDs are used for each person. Requirements list names but don't specify whether canonical data is code-embedded, DB-backed, or externally managed; without stable IDs, normalized mentions can't be reliably stored, searched, or migrated.

**Why it's ambiguous:**
The canonical list is provided as plain text (15 names), but implementation needs:
- Stable identifiers for database foreign keys
- Alias management (nicknames, misspellings, partial names)
- Versioning (what if we add names later?)
- Audit trail (who approved changes?)

**Proposed implementation decision:**
Create a **versioned canonical registry in SQLite** with stable IDs:

**Table: `person`**
- `person_id` (TEXT PRIMARY KEY, deterministic slug: "ayn-rand")
- `canonical_name` (TEXT, "Ayn Rand")
- `type` (TEXT, "philosopher" | "ari_instructor")
- `notes` (TEXT, optional)
- `created_at`, `updated_at` (TIMESTAMP)

**Table: `person_alias`**
- `alias_text` (TEXT, "Rand", "Dr. Peikoff", "LP")
- `person_id` (FK to person)
- `alias_type` (TEXT, "nickname|misspelling|partial|initials|title_variant")
- `confidence_hint` (FLOAT, optional)

**Initial seed migration:**
- Ship `migrations/003_canonical_persons.sql` with 15 names + known aliases
- Use deterministic slugs: "ayn-rand", "leonard-peikoff", "onkar-ghate", etc.

**Rationale:** SQLite is already in stack; DB-backed registry enables updates without code deploys and supports audit/history.

**Open questions:**
- Do stakeholders want stable IDs to be **UUIDs** or human-readable **slugs** (e.g., `ayn-rand`)?
- Will the canonical list expand beyond the 15 names soon (and who approves changes)?
- Do we need to store extra attributes (e.g., birth/death years, ARI role, external URLs)?

**Confidence:** ‚ö†Ô∏è OpenAI identified, Gemini assumed hard-coded list

---

### ‚ö†Ô∏è 8. "Mention" Definition: What Counts as a Person Reference? (Recommended)

**What needs to be decided:**
Rules for what constitutes a valid mention: spoken references, speaker labels, bibliographic citations, possessives ("Rand's"), pronouns, and ambiguous last names. Transcripts can contain speaker metadata, citations, or casual references; requirements don't define inclusion/exclusion criteria.

**Why it's ambiguous:**
Without clear rules, extraction may:
- Over-count: "she", "the author", "the philosopher" (pronouns)
- Under-count: Speaker labels ("Leonard Peikoff:"), possessives ("Rand's theory")
- Confuse: Citations ("See Rand, 1957") vs discussions ("Rand argued...")

**Proposed implementation decision:**
Define a mention as: **a contiguous span in transcript text that explicitly contains a canonical name or alias string** (including possessive forms), excluding pronouns.

**Include:**
- Full names: "Ayn Rand", "Leonard Peikoff"
- Surnames (when unambiguous): "Peikoff", "Ghate", "Binswanger"
- Possessives: "Rand's", "Peikoff's theory"
- Titles: "Dr. Peikoff", "Professor Salmieri"
- Speaker labels: "Leonard Peikoff:" (if exact match to canonical/alias)

**Exclude:**
- Pronouns: "she", "he", "they", "the author"
- Generic references: "the philosopher", "the instructor" (without name)
- Initials alone: "A.R." (unless in alias list with high confidence)

**Speaker label handling:**
- Treat as mention if label text exactly matches canonical_name or alias
- Example: "Leonard Peikoff:" at line start ‚Üí count as mention

**Rationale:** Keeps extraction explainable and auditable; avoids speculative coreference resolution which requires NLP models and increases complexity/errors.

**Open questions:**
- Should we treat "Rand" alone as sufficient, or require "Ayn Rand" unless context disambiguates?
- Do we want to extract mentions from **titles/headers** if present in transcript files?
- Should references inside quoted text be handled differently?

**Confidence:** ‚ö†Ô∏è OpenAI identified, Gemini assumed it was obvious

---

### ‚ö†Ô∏è 9. State Management: Reprocessing, Idempotency, and Versioning (Recommended)

**What needs to be decided:**
How to track which extraction logic/canonical list version was used per transcript and when to re-run. Without versioning, updates to alias lists or matching thresholds will produce inconsistent metadata across the corpus.

**Why it's ambiguous:**
System needs to know:
- Was this file processed with old or new extraction logic?
- Should it be reprocessed when we update aliases?
- How to handle partial failures and retries?

**Proposed implementation decision:**
Add extraction state fields to `transcript` table:

**Fields:**
- `processing_state` (ENUM: `pending|entities_done|uploaded|error|blocked_entity_extraction`)
- `entity_extraction_version` (TEXT, semantic version: "6.1.0")
- `canonical_registry_version` (INTEGER, migration version)

**Idempotency:**
- Entity extraction overwrites prior `transcript_entity` rows for that transcript within a transaction
- Use UPSERT: `INSERT INTO transcript_entity ... ON CONFLICT (transcript_id, person_id) DO UPDATE`

**Reprocessing triggers:**
- Manual command: `objlib extract entities --force` (re-runs all)
- Version mismatch: `objlib extract entities --upgrade` (re-runs files with old version)
- Alias update: Mark affected files as `pending` when aliases change

**Rationale:** Enables safe backfills, reproducibility, and incremental improvements without breaking existing data.

**Open questions:**
- What triggers reprocessing: code change, alias update, manual command?
- Do we need to keep historical entity results for audit, or only latest?
- Should reprocessing automatically re-upload to Gemini or only update local metadata?

**Confidence:** ‚ö†Ô∏è OpenAI identified as critical for long-term maintenance

---

### ‚ö†Ô∏è 10. Validation Rules: Preventing Bad or Unsearchable Metadata (Recommended)

**What needs to be decided:**
What validations are required before persisting entity metadata (e.g., allowed IDs, confidence thresholds, empty results). "Normalize" implies correctness but doesn't specify acceptance criteria; empty entity sets may be valid or indicate failure.

**Why it's ambiguous:**
Without validation gates:
- Invalid person_ids corrupt the database
- Low-confidence garbage gets indexed
- Empty results might be valid (transcript mentions no canonical people) or bugs (extraction crashed)

**Proposed implementation decision:**
Implement strict validation with Pydantic:

**Schema validation:**
```python
class TranscriptEntityOutput(BaseModel):
    person_id: str
    canonical_name: str
    mention_count: int = Field(ge=1)
    max_confidence: float = Field(ge=0.0, le=1.0)
```

**Validation rules:**
1. `person_id` must exist in `person` table (FK constraint)
2. `mention_count >= 1` (if zero, don't include in output)
3. `confidence >= 0.5` (below 50% are too uncertain)
4. If no entities found: Store empty set, mark `entities_done` (not an error)

**Error handling:**
- Schema validation failure ‚Üí log error, mark `blocked_entity_extraction`
- Foreign key violation ‚Üí crash early (data corruption)
- Empty results ‚Üí proceed (valid case: transcript discusses concepts, not people)

**Rationale:** Prevents corrupted references and supports transcripts with no relevant mentions.

**Open questions:**
- Should we require a minimum confidence (0.5? 0.7?) to store an entity at all?
- Do stakeholders want to see "no entities found" as a warning in dashboards?
- Should we store "unknown person" mentions for later review?

**Confidence:** ‚ö†Ô∏è OpenAI identified for data quality

---

### ‚ö†Ô∏è 11. Error Handling + Retries: Network, LLM, and Parsing Failures (Recommended)

**What needs to be decided:**
How to handle transient failures (API timeouts) vs permanent failures (invalid transcript format), and how many retries. Phase introduces new dependencies (LLM fallback, fuzzy libs) and failure modes; requirements don't specify resilience expectations.

**Why it's ambiguous:**
Extraction can fail at multiple stages:
- File read errors (disk unavailable)
- Parsing errors (malformed transcript)
- Fuzzy matching errors (library bugs)
- LLM API errors (429 rate limits, timeouts, malformed JSON)

**Proposed implementation decision:**
Standardize on:

**Deterministic stages (never retry):**
- File read, parsing, fuzzy matching are local operations
- If they fail, it's permanent (bad input data)
- Log error, mark `error` state, skip file

**LLM fallback (retry transient):**
- Retry **2 times** with exponential backoff on:
  - 429 (rate limit)
  - 5xx (server errors)
  - Timeouts
- Fail permanently on:
  - 400 (bad request)
  - 401/403 (auth errors)
  - JSON parse errors after 2 retries
- Log error with full context

**Error logging:**
Persist `error_log` table:
- `transcript_id`, `stage`, `error_type`, `message`, `stack`, `timestamp`

**Rationale:** Predictable ops behavior and debuggability. Retrying local failures wastes time; retrying transient API failures improves reliability.

**Open questions:**
- Is there an SLA for ingestion throughput or completion time?
- Should failures block the whole batch or continue with next transcript?
- Who needs to be alerted when errors exceed a threshold?

**Confidence:** ‚ö†Ô∏è OpenAI identified for operational stability

---

### ‚ö†Ô∏è 12. Observability + Visibility: How Stakeholders Review Extraction Quality (Recommended)

**What needs to be decided:**
What reporting exists for extraction results (per transcript, per person frequency, false positive review). Entity extraction quality directly affects UX; without visibility, stakeholders can't validate correctness or improve alias lists.

**Why it's ambiguous:**
Black-box extraction ‚Üí no way to assess quality, tune thresholds, or catch systematic errors (e.g., all "Smith" mentions mapped to wrong person).

**Proposed implementation decision:**
Add a minimal "review" capability:

**CLI report commands:**
```bash
objlib extract entities --report
objlib extract entities --review-person "Tara Smith"
objlib extract entities --low-confidence
```

**Report content:**
- Top extracted persons (mention frequency across corpus)
- Transcripts with highest entity counts (may indicate false positives)
- Transcripts with low-confidence matches (< 0.7)
- Evidence samples: short snippet per transcript-person for spot checks

**Storage:**
- `evidence_sample` field in `transcript_entity` table (e.g., first 100 chars around mention)

**Rationale:** Cheap to implement and supports iterative tuning of aliases and thresholds.

**Open questions:**
- Do stakeholders want a UI dashboard, or is CLI output sufficient for now?
- What's the acceptance process for adding new aliases based on review?
- Should we support exporting entity metadata to CSV for manual audit?

**Confidence:** ‚ö†Ô∏è OpenAI identified for iterative quality improvement

---

### ‚ö†Ô∏è 13. Security + Authentication: API Keys, Data Leakage, and Prompt Content (Recommended)

**What needs to be decided:**
How API keys are stored/rotated and what transcript content is allowed to be sent to LLMs for disambiguation. Stack includes Gemini/Mistral APIs; transcripts may include sensitive info (even if unlikely). Requirements don't set constraints on data sharing.

**Why it's ambiguous:**
Sending full transcripts to third-party LLMs may violate privacy policies or leak unreleased course content. API keys in logs/code create security risks.

**Proposed implementation decision:**

**API key management:**
- Store in environment variables or system keyring (already implemented for Gemini/Mistral)
- Never log API keys
- Rotate keys if exposed

**Data minimization for LLM fallback:**
- Send only **small context window** (¬±200 chars around mention)
- Send **fixed canonical list** (15 names)
- Do NOT send full transcript
- Redact emails/phone numbers with regex before sending

**Rationale:** Reduces data exposure and keeps prompts minimal, lowering cost and privacy risk.

**Open questions:**
- Are transcripts considered sensitive or restricted in any way?
- Is sending snippets to third-party LLMs approved legally/organizationally?
- Do we need an on-prem/offline mode that forbids external calls?

**Confidence:** ‚ö†Ô∏è OpenAI identified for compliance/security

---

### ‚ö†Ô∏è 14. Search UX Semantics: How Entities Are Queried and Displayed (Recommended)

**What needs to be decided:**
Whether search filters use canonical names, IDs, or both; and whether "mentions" affects ranking. "Searchable metadata" could mean faceted filtering, boosting, or just display tags; without decisions, indexing schema can't be finalized.

**Why it's ambiguous:**
User wants to search for files mentioning "Peikoff". Should they:
- Type exact canonical name "Leonard Peikoff"?
- Type any alias "Peikoff", "Dr. Peikoff", "LP"?
- Select from a dropdown?
- See mention counts in results?

**Proposed implementation decision:**
Use `person_id` as the primary filter key and store `canonical_name` for display.

**Search interface:**
```bash
objlib search "query" --person "Leonard Peikoff"
objlib search "query" --person "Peikoff"  # Auto-expands to canonical
objlib filter --person "Onkar Ghate"
```

**Ranking signal:**
- Boost transcripts by `mention_count` when filtering by a person
- Higher mention count ‚Üí more relevant

**Display:**
- Show canonical name in results: "Mentioned: Ayn Rand, Leonard Peikoff"
- Show mention count in detail view: "Ayn Rand (15 mentions)"

**Rationale:** IDs avoid name-change issues; mention_count improves relevance; canonical names improve clarity.

**Open questions:**
- Should users filter by "ARI instructor" vs "philosopher" categories?
- Should a transcript be returned if the person is only in a citation vs discussed substantively?
- Do we need synonym search so typing "Peikoff" auto-selects "Leonard Peikoff"?

**Confidence:** ‚ö†Ô∏è OpenAI identified for UX design

---

### ‚ö†Ô∏è 15. Handling Non-Canonical but High-Value Entities (Recommended - Gemini)

**What needs to be decided:**
Whether to strictly ignore entities not on the list of 11, or allow expansion to philosophers mentioned in discussions (Kant, Aristotle, Plato, Hume, Branden).

**Why it's ambiguous:**
Requirements say "fuzzy matches against canonical list," implying strict adherence. However, searching an Objectivist library often involves looking for mentions of **Kant**, **Aristotle**, **Plato**, or **Nathaniel Branden**. Excluding them adheres to Phase 6.1 specs but might miss low-hanging fruit for search value.

**Proposed implementation decision:**
**Strict adherence to scope for Phase 6.1** to avoid scope creep:

1. Implement *only* the provided list of 15 names
2. **Architecture for expansion:** Build `EntityExtractor` class to accept external YAML/JSON configuration file for "Canonical List"
3. Future expansion (Phase 6.3?): Add "Referenced Philosophers" category with Kant, Aristotle, Plato, etc.

**Rationale:** Ensures Phase 6.1 milestone is hit without delays, but code structure allows easy expansion later via configuration change (not code change).

**Open questions:**
- Should we plan Phase 6.3 now for "Referenced Philosophers" expansion?
- What other entity types might be valuable (organizations, books, concepts)?

**Confidence:** ‚ö†Ô∏è Gemini identified as future enhancement opportunity

---

## Summary: Decision Checklist

Before planning, confirm:

**Tier 1 (Blocking - Must Answer):**
- [ ] **Disambiguation Rules** - How to handle "Smith" and other ambiguous surnames?
- [ ] **Extraction Engine** - Deterministic (RapidFuzz) or LLM-based or hybrid?
- [ ] **Output Data Model** - SQLite schema + Gemini metadata format?
- [ ] **Workflow Integration** - Hard gate or graceful degradation on failure?
- [ ] **Backfill Strategy** - Re-extract existing files? Re-upload to Gemini?

**Tier 2 (Important - Should Answer):**
- [ ] **Canonical Registry** - SQLite table or hard-coded? Slugs or UUIDs?
- [ ] **Mention Definition** - What counts as a mention? Speaker labels? Possessives?
- [ ] **State Management** - How to version extraction logic and trigger reprocessing?
- [ ] **Validation Rules** - Minimum confidence threshold? Handle empty results?
- [ ] **Library Choices** - RapidFuzz pinned version? Regression test suite?

**Tier 3 (Polish - Can Defer):**
- [ ] **Error Handling** - Retry policy for LLM fallback?
- [ ] **Observability** - CLI report commands? Evidence samples?
- [ ] **Security** - Data minimization for LLM calls? API key rotation?
- [ ] **Search UX** - Filter by canonical name or alias? Mention count ranking?
- [ ] **Non-Canonical Entities** - Plan Phase 6.3 for Kant/Aristotle/Plato?

---

## Recommended Next Step (to unblock planning)

Approve decisions for:
1. **Extraction Engine** (Deterministic-first with RapidFuzz ‚â•92 threshold)
2. **Disambiguation Rules** (Conservative: block single-token "Smith", require full name or context)
3. **Output Data Model** (SQLite summary + Gemini List[str])
4. **Workflow Integration** (Mandatory pre-upload gate, fail-one-continue-batch)
5. **Canonical Registry** (SQLite table with slugs: "ayn-rand", "leonard-peikoff")

Those five determine the data model, workflow, and failure handling. Without them, planning will stall or require rework.

---

## Next Steps

**Non-YOLO Mode (current):**
1. Review this CONTEXT.md
2. Answer questions in CLARIFICATIONS-NEEDED.md
3. Create CLARIFICATIONS-ANSWERED.md with your decisions
4. Run `/gsd:plan-phase 6.1` to create execution plan

**Alternative (YOLO Mode):**
Run `/gsd:discuss-phase-ai 6.1 --yolo` to auto-generate answers

---

*Multi-provider synthesis by: OpenAI gpt-5.2, Gemini Pro*
*Generated: 2026-02-16*
*Perplexity provider unavailable (502 error)*
