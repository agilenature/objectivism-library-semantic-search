---
phase: 16.1-stability-instrument-correctness-audit
plan: 01
subsystem: testing
tags: [sqlite, gemini-file-search, stability-check, spike, evidence]

requires:
  - phase: 11-display-name-stability
    provides: "Phase 11 spike data confirming Document.display_name = file resource ID"
  - phase: 15-consistency-store-sync
    provides: "check_stability.py v2 with 7 assertions"
provides:
  - "SPIKE-EVIDENCE.md: 7-challenge evidence document with DB query results and code line citations"
  - "Identity contract confirmed: retrieved_context.title = 12-char store_doc_id prefix"
  - "SUBSTR fix validated for all 1,749 indexed files"
  - "All 12 A7 failures categorized as QUERY failures"
  - "Full corpus breakdown with per-category risk assessment"
  - "Exact fix locations for Plan 16.1-02 implementation"
affects: [16.1-02, 16.1-03, 16-02]

tech-stack:
  added: []
  patterns: ["HOSTILE evidence collection: live DB queries + source code line citations"]

key-files:
  created:
    - ".planning/phases/16.1-stability-instrument-correctness-audit/SPIKE-EVIDENCE.md"
  modified: []

key-decisions:
  - "Episode files (333) excluded from A7 sampling: zero discriminating metadata makes per-file queries undefined"
  - "File ID restoration rejected: 1 mismatch file would be corrupted; SUBSTR-based lookup is correct fix"
  - "A7 matching logic confirmed correct for all 1,749 files including NULL gemini_file_id; 12 failures are purely query-strategy failures"
  - "display_title and title metadata fields are NULL for all 1,749 files; topic is the only discriminating metadata available"

patterns-established:
  - "HOSTILE evidence pattern: every claim backed by tool output from the execution session, not from research or prior sessions"

duration: 6min
completed: 2026-02-24
---

# Phase 16.1 Plan 01: Stability Instrument Spike Evidence Summary

**HOSTILE-posture evidence document proving check_stability.py failure modes: identity contract confirmed via Phase 11 data, SUBSTR fix validated across all 1,749 files, 12 A7 failures categorized as QUERY failures with 5 exact fix locations identified**

## Performance

- **Duration:** 6 min
- **Started:** 2026-02-24T13:29:53Z
- **Completed:** 2026-02-24T13:35:51Z
- **Tasks:** 1
- **Files created:** 1

## Accomplishments

- Produced 622-line SPIKE-EVIDENCE.md answering all 7 challenges with affirmative evidence
- Confirmed identity contract: `retrieved_context.title` = 12-char prefix of `gemini_store_doc_id` (13/13 Phase 11 measurements)
- Validated SUBSTR fix: all 1,749 files have exactly 1 hyphen, 12-char unique prefix, 0 NULLs, 0 duplicates
- Proved 12 A7 failures are QUERY failures (not MATCHING failures) by demonstrating matching logic handles all edge cases and query strategy never reads available `topic` metadata
- Identified 5 fix locations with exact line numbers across 3 source files

## Task Commits

Each task was committed atomically:

1. **Task 1: Run all 7 validation queries and produce SPIKE-EVIDENCE.md** - `d744204` (docs)

## Files Created/Modified

- `.planning/phases/16.1-stability-instrument-correctness-audit/SPIKE-EVIDENCE.md` - 7-challenge evidence document with DB query results, Phase 11 spike data citations, and source code line references

## Decisions Made

- **Episode exclusion (333 files):** No discriminating metadata (topic=NULL, display_title=NULL, title=NULL, category='unknown'). Per-file targeted queries are undefined for these files. Exclusion is principled, not gaming the metric.
- **File ID restoration rejected:** 1 of 674 non-NULL gemini_file_id files has a mismatch between store_doc_id prefix and file_id suffix. Restoration would corrupt this file. SUBSTR-based lookup avoids this entirely.
- **display_title and title fields unused:** Both are NULL for all 1,749 files. The `topic` metadata field is the only discriminating field available for query construction.
- **Full corpus breakdown refined:** Plan expected Episode(333)/MOTM(468)/Course-stem(221)/Other(727). Actual data shows Episode(333)/MOTM(468)/Other-discriminating(508)/Other-stem(440). The "Course-stem" concept doesn't cleanly separate; the meaningful split is whether `topic` differs from filename stem.

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 1 - Bug] Corpus breakdown categories differ from plan expectation**
- **Found during:** Task 1 (Q6 query)
- **Issue:** Plan expected 4 categories (Episode/MOTM/Course-stem/Other with counts 333/468/221/727). Actual data shows 3 SQL categories (Episode/MOTM/Other) with Other subdividing into topic-differs(508) and topic-equals-stem(440).
- **Fix:** Used actual data categories instead of plan expectations. The "Course-stem" concept (files where topic = lesson title extracted from filename) overlaps with both Other subcategories. Documented the actual breakdown with correct counts summing to 1,749.
- **Files modified:** SPIKE-EVIDENCE.md
- **Verification:** 333 + 468 + 508 + 440 = 1,749

---

**Total deviations:** 1 auto-fixed (1 bug - incorrect plan expectation vs actual data)
**Impact on plan:** No scope creep. The actual data provides a cleaner breakdown than the plan anticipated.

## Issues Encountered

None -- all queries executed successfully, all source files readable, all evidence confirms the plan's hypotheses.

## User Setup Required

None -- no external service configuration required.

## Next Phase Readiness

- SPIKE-EVIDENCE.md gates Plan 16.1-02 (implementation of fixes): **UNBLOCKED**
- All 5 fix locations identified with exact line numbers
- Plan 16.1-02 can proceed without additional research or clarification
- The 440 "stem == topic" files remain a measured risk for A7 -- Plan 16.1-03 re-validation will determine the empirical miss rate

---
*Phase: 16.1-stability-instrument-correctness-audit*
*Completed: 2026-02-24*
