---
phase: 16.1-stability-instrument-correctness-audit
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/16.1-stability-instrument-correctness-audit/SPIKE-EVIDENCE.md
autonomous: true

must_haves:
  truths:
    - "SPIKE-EVIDENCE.md exists and answers all 7 challenges with affirmative evidence"
    - "Identity contract confirmed: retrieved_context.title == store_doc_id prefix (12-char), NOT gemini_file_id suffix -- with Phase 11 spike line references"
    - "SUBSTR fix validated: all 1,749 files have exactly 1 hyphen and 12-char unique prefix in gemini_store_doc_id"
    - "All 12 T+24h A7 failures categorized as QUERY failures (not MATCHING failures) with per-file evidence"
    - "Full corpus breakdown: Episode (333, exclude), MOTM (468, topic), Course-stem (221, risk noted), Other (727, topic)"
    - "Episode exclusion decision documented with exact count and rationale"
    - "Exact code line numbers identified for all 3 fix locations in check_stability.py, database.py, citations.py"
  artifacts:
    - path: ".planning/phases/16.1-stability-instrument-correctness-audit/SPIKE-EVIDENCE.md"
      provides: "7-challenge evidence document with DB query results and code line citations"
      min_lines: 150
  key_links:
    - from: "SPIKE-EVIDENCE.md"
      to: "spike/phase11_spike/raw_results.json"
      via: "Phase 11 spike data confirming Document.display_name = file resource ID"
      pattern: "display_name"
    - from: "SPIKE-EVIDENCE.md"
      to: "scripts/check_stability.py"
      via: "Line-number citations for A6 lookup (line 417-423), A7 sampling (line 479-487), A7 query (line 505-513), A7 matching (line 557-566), A7 tolerance (line 580)"
      pattern: "line [0-9]+"
---

<objective>
Produce SPIKE-EVIDENCE.md: a HOSTILE-posture evidence document that resolves all 7 challenges about check_stability.py's false-negative modes with affirmative evidence from live DB queries, Phase 11 spike data, and source code line references.

Purpose: This spike gates the implementation. No code changes happen until every claim is backed by tool output -- not inference, not "seems likely", not prior-session memory. The instrument is the adversarial target; we must prove we understand its failure modes before we touch it.

Output: SPIKE-EVIDENCE.md with 7 challenge answers, each containing: claim, evidence source, tool output, and confidence level.
</objective>

<execution_context>
@/Users/david/.claude/get-shit-done/workflows/execute-plan.md
@/Users/david/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16.1-stability-instrument-correctness-audit/16.1-RESEARCH.md
@scripts/check_stability.py
@src/objlib/search/citations.py
@src/objlib/database.py
@spike/phase11_spike/raw_results.json
@spike/phase11_spike/RESULTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run all 7 validation queries against live DB and source code</name>
  <files>
    .planning/phases/16.1-stability-instrument-correctness-audit/SPIKE-EVIDENCE.md
  </files>
  <action>
Run the following queries and code inspections. Record EVERY output verbatim. Do NOT summarize or paraphrase. Each query answers one or more challenges.

**Challenge 1: Identity Contract**
Read `spike/phase11_spike/raw_results.json` and extract the 13 measurement rows showing:
- `file_name` (Files API resource)
- `document_name` (store document resource)
- `doc_display_name` (what Document.display_name returns)
Verify: for all 13, `doc_display_name` == file_name suffix == store_doc_name prefix.

Then read `spike/phase11_spike/RESULTS.md` or `spike/phase11_spike/GATE-EVIDENCE.md` for the formal conclusion.

**Challenge 2: SUBSTR fix validity**
Run these exact queries against `data/library.db` via Python:
```python
import sqlite3
conn = sqlite3.connect('data/library.db')

# Q1: Total indexed, NULL store_doc_id, NULL file_id
print(conn.execute("""
  SELECT COUNT(*) as total,
         SUM(CASE WHEN gemini_store_doc_id IS NULL THEN 1 ELSE 0 END) as null_doc,
         SUM(CASE WHEN gemini_file_id IS NULL THEN 1 ELSE 0 END) as null_fid
  FROM files WHERE gemini_state='indexed'
""").fetchone())

# Q2: Hyphen count distribution
print(conn.execute("""
  SELECT LENGTH(gemini_store_doc_id) - LENGTH(REPLACE(gemini_store_doc_id, '-', '')) as hyphens,
         COUNT(*) as cnt
  FROM files WHERE gemini_state='indexed'
  GROUP BY hyphens
""").fetchall())

# Q3: Prefix length distribution
print(conn.execute("""
  SELECT LENGTH(SUBSTR(gemini_store_doc_id, 1, INSTR(gemini_store_doc_id, '-') - 1)) as prefix_len,
         COUNT(*) as cnt
  FROM files WHERE gemini_state='indexed'
  GROUP BY prefix_len
""").fetchall())

# Q4: Prefix uniqueness (any duplicates?)
print(conn.execute("""
  SELECT SUBSTR(gemini_store_doc_id, 1, INSTR(gemini_store_doc_id, '-') - 1) as prefix,
         COUNT(*) as cnt
  FROM files WHERE gemini_state='indexed'
  GROUP BY prefix HAVING cnt > 1
""").fetchall())

# Q5: The 1 mismatch file (store_doc_id prefix != file_id suffix)
print(conn.execute("""
  SELECT filename, gemini_file_id, gemini_store_doc_id,
         SUBSTR(gemini_store_doc_id, 1, INSTR(gemini_store_doc_id, '-') - 1) as doc_prefix,
         REPLACE(gemini_file_id, 'files/', '') as file_suffix
  FROM files
  WHERE gemini_state='indexed'
    AND gemini_file_id IS NOT NULL
    AND SUBSTR(gemini_store_doc_id, 1, INSTR(gemini_store_doc_id, '-') - 1) !=
        REPLACE(gemini_file_id, 'files/', '')
""").fetchall())
```

**Challenge 3: File ID restoration rejection**
The Q5 output above proves the mismatch. Document why restoration would corrupt this file.

**Challenge 4: Matching vs Query failure split**
Read `scripts/check_stability.py` lines 556-567 (A7 matching logic). Verify that `title_in_result in store_doc_id` is substring matching and works correctly for the identity contract (12-char prefix IN 25-char "prefix-suffix" string). Then demonstrate the 12 failures are query failures by running:
```python
# Q6: Full corpus breakdown
print(conn.execute("""
  SELECT
    CASE
      WHEN filename LIKE 'Episode %' THEN 'Episode'
      WHEN filename LIKE 'MOTM_%' THEN 'MOTM'
      ELSE 'Other'
    END as pattern,
    COUNT(*) as cnt,
    SUM(CASE WHEN json_extract(metadata_json, '$.topic') IS NOT NULL THEN 1 ELSE 0 END) as has_topic,
    SUM(CASE WHEN json_extract(metadata_json, '$.topic') IS NOT NULL
              AND json_extract(metadata_json, '$.topic') != REPLACE(REPLACE(filename, '.txt', ''), '_', ' ')
              THEN 1 ELSE 0 END) as topic_differs_from_stem
  FROM files WHERE gemini_state='indexed'
  GROUP BY pattern
""").fetchall())
```

**Challenge 5: Full corpus query audit**
Run:
```python
# Q7: Episode metadata fields
print(conn.execute("""
  SELECT filename, json_extract(metadata_json, '$.topic') as topic,
         json_extract(metadata_json, '$.display_title') as display_title,
         json_extract(metadata_json, '$.title') as title,
         json_extract(metadata_json, '$.category') as category,
         json_extract(metadata_json, '$.series') as series
  FROM files WHERE gemini_state='indexed' AND filename LIKE 'Episode %'
  LIMIT 5
""").fetchall())

# Q8: MOTM metadata sample
print(conn.execute("""
  SELECT filename, json_extract(metadata_json, '$.topic') as topic
  FROM files WHERE gemini_state='indexed' AND filename LIKE 'MOTM_%'
  LIMIT 5
""").fetchall())

# Q9: Course files where topic == stem
print(conn.execute("""
  SELECT COUNT(*) FROM files
  WHERE gemini_state='indexed'
    AND json_extract(metadata_json, '$.topic') IS NOT NULL
    AND json_extract(metadata_json, '$.topic') = REPLACE(REPLACE(filename, '.txt', ''), '_', ' ')
    AND filename NOT LIKE 'Episode %'
    AND filename NOT LIKE 'MOTM_%'
""").fetchone())
```

**Challenge 6: Zero tolerance + Episode exclusion**
The Q6 and Q7 outputs prove Episode files have no discriminating metadata. Document the exclusion decision.

**Challenge 7: Fix locations (exact line numbers)**
Read `scripts/check_stability.py` and record:
- A6 lookup: the exact line numbers of the SQL in `_check_citation_resolution`
- A7 sampling: the exact line numbers of the SQL in `_check_targeted_searchability`
- A7 query: the exact line numbers of `meta.get("display_title") or meta.get("title")`
- A7 matching: the exact line numbers of the `in` operator matching
- A7 tolerance: the exact line number of `max_misses = max(1, sample_size // 5)`

Read `src/objlib/database.py` and record:
- The line number range of `get_file_metadata_by_gemini_ids` (the method that needs a sibling)

Read `src/objlib/search/citations.py` and record:
- The line numbers of the `enrich_citations` function
- The line where `gemini_id_lookup` is called (the lookup chain to extend)

**Finally: Produce SPIKE-EVIDENCE.md**

Write `.planning/phases/16.1-stability-instrument-correctness-audit/SPIKE-EVIDENCE.md` with this structure:

```markdown
# Phase 16.1: SPIKE-EVIDENCE -- Stability Instrument Correctness Audit

**Date:** 2026-02-24
**Posture:** HOSTILE -- every claim backed by tool output
**Gate:** All 7 challenges answered with affirmative evidence

## Challenge 1: Identity Contract
[Claim, Phase 11 evidence, production confirmation, conclusion]

## Challenge 2: SUBSTR Fix Validity
[Q1-Q4 outputs verbatim, conclusion]

## Challenge 3: File ID Restoration Rejection
[Q5 output, rationale]

## Challenge 4: Matching vs Query Split
[Code line analysis, Q6 output, categorization of 12 failures]

## Challenge 5: Full Corpus Query Audit
[Q6-Q9 outputs, breakdown table]

## Challenge 6: Zero Tolerance + Episode Exclusion
[Evidence, decision, count]

## Challenge 7: Fix Locations
[Exact line numbers for all 3 files, change descriptions]

## Gate Verdict
[PASS/FAIL with summary]

## Appendix: Raw Query Outputs
[All Q1-Q9 outputs verbatim]
```

Every number in the document must come from a tool call in this session, not from the research file or prior sessions. The research file guides WHAT to check; this task produces the EVIDENCE.
  </action>
  <verify>
1. File exists: `ls .planning/phases/16.1-stability-instrument-correctness-audit/SPIKE-EVIDENCE.md`
2. All 7 challenges answered: `grep -c "^## Challenge" SPIKE-EVIDENCE.md` returns 7
3. Contains verbatim query outputs: `grep -c "^print\|fetchone\|fetchall" SPIKE-EVIDENCE.md` > 0 OR raw output blocks present
4. Contains line number citations for check_stability.py: `grep "line [0-9]" SPIKE-EVIDENCE.md`
5. Contains the 1 mismatch file identification: `grep "3oylo5ddxwvg\|rkkyrvbpc1iw" SPIKE-EVIDENCE.md`
6. Gate verdict present: `grep "Gate Verdict" SPIKE-EVIDENCE.md`
7. No "seems likely" or "probably" or "should be": `grep -i "seems likely\|probably\|should be" SPIKE-EVIDENCE.md` returns 0
  </verify>
  <done>
SPIKE-EVIDENCE.md exists with all 7 challenges answered. Every claim is backed by tool output from this session. Line numbers are exact. DB query results are verbatim. The gate verdict is PASS (or FAIL with specific blockers identified). Plan 16.1-02 is unblocked.
  </done>
</task>

</tasks>

<verification>
Plan 16.1-01 verification:
1. SPIKE-EVIDENCE.md answers all 7 challenges
2. Identity contract proven by Phase 11 empirical data (not inferred)
3. SUBSTR fix validated against all 1,749 files (not a sample)
4. 12 T+24h failures categorized with per-file evidence
5. Full corpus breakdown includes all 4 patterns with counts summing to 1,749
6. Episode exclusion has exact count (333) and principled rationale
7. Fix locations cite exact line numbers in all 3 source files
8. No hedging language ("seems", "probably", "should") in conclusions
9. Gate verdict explicitly gates Plan 02
</verification>

<success_criteria>
- SPIKE-EVIDENCE.md exists and is self-contained (readable without 16.1-RESEARCH.md)
- All evidence comes from tool calls in the execution session, not from research or prior sessions
- The document is sufficient for a different Claude instance to implement the fixes in Plan 02 without asking clarifying questions
- Gate: PASS means Plan 02 is unblocked; FAIL means specific blockers are identified for user triage
</success_criteria>

<output>
After completion, create `.planning/phases/16.1-stability-instrument-correctness-audit/16.1-01-SUMMARY.md`
</output>
