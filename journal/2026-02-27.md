# Journal — 2026-02-27

## Overview

A two-session work unit (the prior session ran out of context mid-execution) that
hardened the full CRAD corpus from 63 nominally-validated phrases to 63 phrases with
confirmed retrieval ranks. The session began with a 10-run A7 stress test, discovered
that one phrase (`ITOE AT - Class 07-02`) had a 0% hit rate despite carrying
`validation_status='validated'`, fixed it, then systematically probed all 44
conditionally-exposed files and re-ran genus-differentia on the 6 failures and 3
marginals. Ended with a STABLE 60/60 A7 run at the new permanent default
sample-count.

---

## Starting Point

Phase 16.6 was formally complete (16.6-03-SUMMARY marked PASS, three consecutive
STABLE runs at 20/20 A7). However, a concern remained: `validation_rank` was NULL
for 44 of 63 CRAD-validated files — meaning those phrases had passed the `any_pass`
gate (1 hit in 3 runs) but had never had their actual retrieval rank confirmed. The
session began by stress-testing the A7 instrument at 10 runs × 20 files to surface
any hidden fragility before closing the phase.

---

## Root Cause Analysis

### Problem 1: Class 07-02 had a 0% hit rate despite `validation_status='validated'`

**Observed:** The 10-run stress test (runs 1–6, 8–10: STABLE 20/20; run 7: UNSTABLE,
`ITOE Advanced Topics - Class 07-02.txt` not found) identified a file that always
missed when sampled. Manual targeted probing confirmed the phrase
`rejection analytic-synthetic dichotomy` had a 0% hit rate across 3 consecutive runs.

**Expected:** `validation_status='validated'` should mean the phrase reliably retrieves
the file. `validation_rank=NULL` indicated the validation only confirmed 1 hit in 3 runs
(`any_pass` gate), not a confirmed rank.

**Causal chain:** The `any_pass` gate in the CRAD validation pipeline marks a phrase
as `validated` if at least 1 of 3 Gemini runs returns the file at rank ≤ 5. A phrase
that scores [None, 1, None] passes the gate with `validation_rank=NULL` (NULL = the
best rank was never stored, not that the file was found). The phrase
`rejection analytic-synthetic dichotomy` is semantically generic in the Objectivism
corpus — the analytic-synthetic dichotomy is discussed across many transcripts, so
the phrase retrieves competing files at rank 1 before retrieving Class 07-02. The
single-pass hit was statistical noise. The NULL rank was never a red flag in the
instrument because A7 had only a 20-file sample over 60 files with CRAD phrases —
the probability of sampling this specific file in any given run was low (~1.1% per
file per run).

**Correct invariant:** A CRAD phrase is only meaningfully validated when its best rank
is confirmed as ≤ 3 across at least 2 of 3 runs. `validation_rank=NULL` means
"passed one run arbitrarily" — not "reliably retrievable." The NULL rank is itself
a signal of marginal or broken phrase quality.

**Fix:** New phrase `sensuous symbols unit economy word choice` — confirmed [1, 1, 1].
Updated DB, re-uploaded file with new `Discrimination:` header.

---

### Problem 2: 44 of 63 CRAD phrases had `validation_rank=NULL`

**Observed:** After the Class 07-02 discovery, a DB query found that 44 of 63
CRAD-validated files had `validation_rank IS NULL` or `validation_rank > 3`. These
files were "conditionally exposed" — A7 uses their CRAD phrase as the fallback but
has no confirmation the phrase actually works.

**Expected:** All 63 validated phrases should have confirmed retrieval ranks
demonstrating their effectiveness before being relied upon in A7.

**Causal chain:** The Phase 16.6-02 CRAD pipeline validated phrases with the `any_pass`
gate (1/3 hits at rank ≤ 5 = accepted). The `validation_rank` column was set only
when all 3 runs agreed; otherwise it was left NULL. This meant most phrases were
accepted without a confirmed rank. A7 then relied on these phrases as if they were
equivalent to phrases with confirmed rank=1 — which they were not.

**Fix:** Ran a targeted probe (3 Gemini runs × 44 files = 132 API calls). Results:
35 STRONG (best_rank ≤ 3), 3 MARGINAL (best_rank 4–5), 6 FAIL (never found). Updated
DB with confirmed ranks for all 35 STRONG and 3 MARGINAL. Re-ran genus-differentia
on the 6 FAIL + 3 MARGINAL files, found improved phrases for 7/9, and re-uploaded
those 7 files with updated `Discrimination:` headers.

**Correct invariant:** After any CRAD validation run, a secondary probe must confirm
the rank for every phrase with `validation_rank IS NULL`. The `any_pass` gate is
sufficient to accept a candidate phrase for embedding; it is not sufficient to
declare the phrase "validated" for A7 fallback purposes.

---

### Problem 3: Two files (ITOE AT 12-02 OH, ITOE AT 14-02 OH) have no reliable discriminating phrase

**Observed:** After multiple rounds of genus-differentia candidate testing, the best
achievable ranks for these two files are 5 (passes=1/3) and 4 (passes=3/3) respectively.
No combination of aspects achieves rank ≤ 3.

**Causal chain:** Both files cover epistemologically generic topics (concept
knowledge reference; historical concept formation) that are densely represented
across many ITOE class transcripts. No rarity signal is strong enough to
discriminate these files corpus-relatively. The genus of the ITOE Advanced Topics
series is so broad that these files' aspects are not unique to them.

**Current state:** Best available phrases kept at marginal ranks. A7 will occasionally
miss these files when they are sampled. At a 60-file sample over 1,749 files, each
of these two files has ~3.4% chance of being sampled per run. Practical A7 failure
risk from these two files is low.

---

## Decisions Made

### Decision 1: Raise A7 default sample-count from 5 to 60 (permanent)

**Decision:** Changed `--sample-count` default in `check_stability.py` from 5 → 60
in all three locations (argparse default, `__init__` signature, method signature).

**Rejected alternative:** Keep 20 as the "standard" sample and 60 as opt-in. Rejected
because: the 10-run stress test at 20/run only hit the failing Class 07-02 file once
in 10 runs — the small sample masks fragile files. 60/1,749 = 3.4% hit probability
per file per run, which is still low for rare problematic files but is the practical
limit given Gemini API call count and ~5.5 minutes per run.

**Rejected alternative:** Use 100. Rejected because A7 makes 1 Gemini API call per
file via S1, plus a CRAD fallback call (for the ~60 CRAD-covered files), plus an S4a
fallback call for any remaining misses. At 100 files, A7 would take 10–15 minutes per
run, making the 3-run gate require 30–45 minutes. 60 gives adequate coverage in ~5.5
minutes.

**Stable:** Yes. The new default applies to all future `check_stability.py` runs.
To run a lighter check (e.g., during development), use `--sample-count 20`.

---

### Decision 2: Accept two permanently marginal CRAD files

**Decision:** `ITOE AT - Class 12-02 - Office Hour.txt` (phrase: `meaning concept
knowledge reference`, best_rank=5) and `ITOE AT - Class 14-02 - Office Hour.txt`
(phrase: `Historical context of concept formation`, best_rank=4) are kept as-is.
No further re-CRAD attempts.

**Rejected alternative:** Continue attempting genus-differentia with additional aspect
combinations. Rejected because: multiple rounds (4+ per file) exhausted all aspect
combinations available in the DB. These files' topics are genuinely over-represented
in the corpus — no combination of their extracted aspects produces a corpus-relative
discriminating phrase.

**Rejected alternative:** Re-extract metadata for these files with different extraction
prompting to find better differentiating aspects. Rejected because: the metadata
extraction is high-quality; the problem is the corpus, not the extraction. These files
discuss foundational epistemology topics that appear across most Objectivism transcripts.

**Stable:** Yes, unless new discriminating content is added to these files' transcripts
or the corpus changes significantly. These two files are the known floor of CRAD
coverage. Future sessions should not attempt further phrase improvements for them
without first confirming the aspect vocabulary has changed.

---

## Current State

| Property | Value | Cause |
|----------|-------|-------|
| DB indexed | 1,749 | Full library uploaded via Phase 16.3-03 remediation |
| Store docs | 1,749 | store-sync confirmed clean; no orphans |
| CRAD phrases total | 63 | All 63 S1-failing class-number files have validated phrases |
| CRAD rank=1 | 42 files | Strong corpus-relative discrimination confirmed 3/3 runs |
| CRAD rank=2–3 | 12 files | STRONG; occasional miss but reliably found at ≤3 |
| CRAD rank=4–5 | 7 files | MARGINAL; found inconsistently; two have no better phrase |
| CRAD rank=None | 2 files | AT-12-02 and AT-14-02; marginal passes only; permanent floor |
| A7 default sample-count | 60 | Raised from 5 to expose fragile CRAD phrases in routine checks |
| A7 tolerance | 0 | Zero-tolerance; any miss = UNSTABLE |
| Phase 16.6 | COMPLETE | 3-plan execution plus hardening cycle confirmed STABLE 60/60 |
| Phase 16-02 (temporal stability) | IN PROGRESS | T=0 baseline 2026-02-26 01:48 UTC; T+24h gate pending |
| Phase 17 | UNBLOCKED | Next planned phase |

---

## Files Changed

| File | Change |
|------|--------|
| `scripts/check_stability.py` | A7 default sample-count raised from 5 → 60 in argparse, `__init__`, and method signature |
| `data/library.db` | `file_discrimination_phrases`: confirmed `validation_rank` for all 63 files; 7 new phrases replacing FAIL/MARGINAL ones |
| `data/library.db` | 7 files re-uploaded: `gemini_file_id`, `gemini_store_doc_id`, `gemini_state_updated_at` updated to new store docs |
| `scripts/crad_full_run.py` | New script: full CRAD run including genus-differentia, phrase testing, DB update, re-upload pipeline |
| `.planning/phases/16.6-crad/16.6-02-PLAN.md` | Created: plan for full CRAD run of all 63 files |
| `.planning/phases/16.6-crad/16.6-02-SUMMARY.md` | Created: full probe results, phrase table, gate PASS |
| `.planning/phases/16.6-crad/16.6-02-results.json` | Created: raw probe output (44 files × 3 runs) |
| `.planning/phases/16.6-crad/16.6-03-PLAN.md` | Created: plan for A7 update + stability gate |
| `.planning/phases/16.6-crad/16.6-03-SUMMARY.md` | Created: gate PASS, 3× STABLE 20/20 A7 |
| `.planning/STATE.md` | Phase 16.6 COMPLETE, Phase 17 UNBLOCKED |
| `.planning/ROADMAP.md` | Phase 16.6 status updated |

---

## Lessons Learned

- **`validation_rank=NULL` is a broken phrase signal, not a missing-data annotation.**
  The CRAD pipeline stores NULL when a phrase hit the file in only 1 of 3 runs. Future
  code or sessions reading the table should treat NULL rank the same as rank > 5: the
  phrase is unconfirmed and should not be relied upon in A7.

- **The `any_pass` gate is appropriate for accepting a candidate phrase, not for
  declaring production readiness.** A phrase that passes 1/3 runs is a promising
  candidate worth embedding via the `Discrimination:` header. It is not a phrase ready
  to be the sole A7 fallback for a file. After embedding (re-upload), a second
  validation round is required to confirm the embedding improved retrieval.

- **A7 with a small sample is a weak validator for rare file types.** At 20/1,749,
  any specific file has ~1.1% probability of being sampled per run. A file that fails
  50% of the time (like Class 07-02 before the fix) has only a ~0.55% probability of
  being caught per run. 10 runs at 20 samples = ~5.4% probability of detection. 60/run
  raises this to ~16% per run. This is still not high — it means fragile files may
  require many runs to surface. The correct fix is a dedicated per-file probe for any
  `validation_rank IS NULL` CRAD file, which this session now established as protocol.

- **Re-uploading with an improved `Discrimination:` header is the correct remediation
  for a failed CRAD phrase.** The embedding encodes the phrase at the document level.
  Updating the DB alone does nothing — the Gemini store document still encodes the old
  phrase. Both the DB update and the re-upload are required for the new phrase to take
  effect in retrieval.

- **Corpus-relative discrimination has a hard floor for files with generic topics.**
  Two ITOE Advanced Topics files discuss topics so prevalent in Objectivism discourse
  that no extracted aspect combination produces a corpus-discriminating phrase. This is
  a property of the corpus, not of the extraction or the algorithm. Future attempts to
  improve these files should start by verifying whether new aspects have been extracted
  since the last attempt, rather than re-running the same aspects in different
  combinations.
