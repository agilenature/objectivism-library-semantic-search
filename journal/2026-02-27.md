# Journal — 2026-02-27

## Overview

Phase 16.6 (CRAD — Corpus-Relative Aspect Differentiation): full arc from discovery
through hardening. Phase 16.5 had identified 63 files (3.6%) that S1 retrieval
deterministically fails on — class-number files whose stems carry no semantic content.
S4a (rarest-aspect fallback) covered them stochastically but unreliably because
aspect frequency ties at freq=1 are broken by list order, not philosophical
specificity. Phase 16.6 designed a 3-pass genus-differentia algorithm, used Claude
Haiku as the Pass 2 philosophical discriminator, piloted on 3 files, ran on all 63
files, and integrated the results permanently into `header_builder.py` and A7.

This session (a two-session continuation due to context exhaustion) hardened the
results: a 10-run A7 stress test exposed one phrase with a 0% hit rate
(`ITOE AT - Class 07-02`), a systematic probe of all 44 conditionally-exposed files
found 6 FAIL and 3 MARGINAL phrases, re-CRAD produced improved phrases for 7 of
those 9, and 7 files were re-uploaded with updated `Discrimination:` headers. Ended
with STABLE 60/60 A7 at the new permanent default sample-count.

---

## Starting Point

Phase 16.6 was formally complete (16.6-03-SUMMARY marked PASS, three consecutive
STABLE runs at 20/20 A7). However, a concern remained: `validation_rank` was NULL
for 44 of 63 CRAD-validated files — meaning those phrases had passed the `any_pass`
gate (1 hit in 3 runs) but had never had their actual retrieval rank confirmed. The
session began by stress-testing the A7 instrument at 10 runs × 20 files to surface
any hidden fragility before closing the phase.

---

## CRAD Arc: Why Phase 16.6 Was Initiated and What It Built

This section documents the full causal arc of Phase 16.6 — from the discovery that
triggered it through the three-plan execution — so that the hardening work documented
below has its structural context.

### What Caused the CRAD Discussion

Phase 16.5 ran a 1,749-file exhaustive retrievability audit (2026-02-25). The audit
confirmed that 63 files (3.6% of the corpus) fail S1 retrieval deterministically. S1
is the first-pass A7 query: `"What is '{stem}' about?"`. These are class-number files
whose stems look like `ITOE Advanced Topics - Class 14-01 - Office Hour` or
`ITOE - Class 03-01 - Office Hours`. The stem contains no semantic content — it is a
series name, a class number, and a session type. A semantic embedding of `"What is
'ITOE AT - Class 14-01 - OH' about?"` retrieves the general corpus of ITOE Advanced
Topics content, not this specific file.

Phase 16.5 introduced S4a as a fallback: query with the file's top-3 rarest aspects
by corpus frequency. S4a improved coverage — all 63 files became eventually
retrievable via S4a or the S4b cascade — but it was stochastic. The same file, on
different runs, might rank 1 or not appear at all, depending on which aspects were
selected and how Gemini weighted them. A7 with S4a was probabilistic rather than
deterministic for these 63 files. The concern was: S4a is stochastic because the
underlying selection mechanism (corpus frequency sort) collapses when many aspects
are tied at the same frequency. For ITOE Advanced Topics Office Hours (28 files),
~240 of ~280 unique aspects have series frequency = 1. Corpus frequency and series
frequency are both identical for large tied sets. The sort order within that tied set
is arbitrary — governed by list position in the JSON array, which is Mistral's
extraction order, which is essentially random from a discrimination standpoint.

The Phase 16.5 root cause analysis demonstrated this directly: "order of being vs.
order of knowing" (freq=1) MISSED as a query while "Zeno's arrow paradox" (also
freq=1) retrieved the target file at rank 1. Both aspects score identically under any
frequency-based sort. The difference is philosophical specificity — and frequency
cannot measure philosophical specificity.

### The Genus Method Insight

The solution required a conceptual reframe. To differentiate a file within a series,
you must know the genus (what all siblings share) before you can name the differentia
(what makes this specific file unique within the series). This is Binswanger's Genus
Method: "To be a genius, get the genus."

Standard aspect extraction is content-absolute: it asks "what topics does this file
cover?" A file covering Zeno's arrow paradox also covers epistemology, concept
formation, and the law of identity — because those are the standing topics of the
ITOE Advanced Topics course. Content-absolute extraction correctly returns all of
them. But for retrieval discrimination within the corpus, epistemology and concept
formation are genus (shared by all 58 siblings), while Zeno's arrow paradox is
differentia (specific to this file). The extraction process cannot make this
distinction because it sees only one file at a time.

CRAD — Corpus-Relative Aspect Differentiation — is the operationalization of the
Genus Method:
- **Pass 1 (Genus):** Query ALL files in the series from the DB. Compute an aspect
  frequency map across the series. Aspects appearing in ≥80% of series files = genus
  (shared, non-discriminating).
- **Pass 2 (Differentia):** For a single file, subtract genus aspects. Rank remaining
  aspects by series rarity (primary), corpus rarity (secondary). When frequency ties
  (which is common for ITOE AT OH), use Claude as the philosophical discriminator:
  provide the full series aspect inventory and ask which aspects are philosophically
  specific to this file versus generically present in the series.
- **Pass 3 (Essentialization):** Concatenate the top-N differentiating aspects within
  a ≤7-word budget, markdown-clean.

Pass 2's use of Claude as discriminator was a late design revision. The original plan
used pure frequency sorting with secondary tie-breaking by corpus frequency. Research
showed this was insufficient: the two aspects that differ in philosophical specificity
("Zeno's arrow paradox" vs "order of being vs. order of knowing") are tied at every
frequency level. The question "which of these is philosophically specific to this
session?" requires reasoning about content, not counting occurrences. Claude Haiku
was chosen as the Pass 2 agent: the task is well-defined (select the most
philosophically specific aspect from a provided candidate set), the input is short
(~400–700 tokens per call), and 63 sequential calls at ~$0.02 total made Haiku the
correct cost/quality tradeoff.

### Phase 16.6-01: Pilot (3 Files)

Three known-failing files were chosen as the pilot:

| File | Phrase | Validation |
|------|--------|------------|
| ITOE AT - Class 14-01 - Office Hour | `Zeno's arrow paradox` | [1,1,1] |
| ITOE AT - Class 13-02 - Office Hour | `DIMM hypothesis null hypothesis statistics` | [2,2,2] |
| OL - Class 14-02 - Open Office Hour | `humility as a package deal` | [1,1,1] |

All three confirmed at rank ≤ 3 with zero stochastic variance. The pilot also
confirmed that the 80% genus threshold produces zero genus aspects for both ITOE AT
(58 files) and OL (50 files) — no aspect appears in 80%+ of files in these series.
The algorithm degrades gracefully: when there is no computable genus, all aspects are
differentia candidates and discrimination is entirely based on rarity + judgment.
Gate: PASS.

### Phase 16.6-02: Full CRAD Run (63 Files)

63 discrimination phrases were computed inline (Claude Code session as the
discriminating agent, reading file content and series aspects directly). Each phrase
was validated 3× against Gemini using the `any_pass` gate (at least 1 of 3 runs finds
the file at rank ≤ 5). 63/63 phrases passed. All 63 files were re-uploaded with
`Discrimination: {phrase}` injected into the identity header before `--- END METADATA ---`.

A critical failure occurred mid-run: the DB update used the wrong column name
(`gemini_last_uploaded_at` vs `gemini_state_updated_at`). 62 new store documents were
created in Gemini (not tracked in DB); 62 old store documents were deleted from Gemini
(DB still referenced them). Recovery: fixed column name, added `--upload-only` flag to
skip stale-ID validation, re-ran upload (63/63 success), ran `store-sync --no-dry-run`
to purge the 63 orphaned documents from the broken run.

The `any_pass` gate was the correct choice for this phase. Gemini File Search has
inherent ~5–20% per-query miss rate on individual calls. A strict `all_pass` gate
would incorrectly reject good phrases. The concern — which this session later
confirmed — is that `any_pass` without a confirmed rank is too weak for production
reliance. The gate accepted the phrase for the `Discrimination:` embedding; it did
not guarantee reliable retrieval under A7 sampling. Gate: PASS.

### Phase 16.6-03: Permanent Integration

Two permanent changes were made:

1. **`header_builder.py`:** `Discrimination: {phrase}` field added to the identity
   header template. When a file has `validation_status='validated'` in
   `file_discrimination_phrases`, the phrase is automatically injected on every
   future upload (via `fsm-upload`, re-upload, or new file ingest). No manual
   injection needed going forward.

2. **`check_stability.py` A7:** CRAD fallback inserted between S1 and S4a. After S1
   misses, A7 queries the DB for a validated CRAD phrase. If one exists, A7 runs a
   second Gemini query with the phrase before falling back to S4a. This makes the
   63 class-number files deterministically covered (by their CRAD phrase) rather
   than stochastically covered (by S4a). S4a remains as the backstop only for files
   with no validated CRAD phrase.

Three consecutive STABLE runs at 20/20 A7 confirmed the integration. Gate: PASS.
Phase 16.6 declared complete.

---

## Root Cause Analysis

### Problem 1: Class 07-02 had a 0% hit rate despite `validation_status='validated'`

**Observed:** The 10-run stress test (runs 1–6, 8–10: STABLE 20/20; run 7: UNSTABLE,
`ITOE Advanced Topics - Class 07-02.txt` not found) identified a file that always
missed when sampled. Manual targeted probing confirmed the phrase
`rejection analytic-synthetic dichotomy` had a 0% hit rate across 3 consecutive runs.

**Expected:** `validation_status='validated'` should mean the phrase reliably retrieves
the file. `validation_rank=NULL` indicated the validation only confirmed 1 hit in 3 runs
(`any_pass` gate), not a confirmed rank.

**Causal chain:** The `any_pass` gate in the CRAD validation pipeline marks a phrase
as `validated` if at least 1 of 3 Gemini runs returns the file at rank ≤ 5. A phrase
that scores [None, 1, None] passes the gate with `validation_rank=NULL` (NULL = the
best rank was never stored, not that the file was found). The phrase
`rejection analytic-synthetic dichotomy` is semantically generic in the Objectivism
corpus — the analytic-synthetic dichotomy is discussed across many transcripts, so
the phrase retrieves competing files at rank 1 before retrieving Class 07-02. The
single-pass hit was statistical noise. The NULL rank was never a red flag in the
instrument because A7 had only a 20-file sample over 60 files with CRAD phrases —
the probability of sampling this specific file in any given run was low (~1.1% per
file per run).

**Correct invariant:** A CRAD phrase is only meaningfully validated when its best rank
is confirmed as ≤ 3 across at least 2 of 3 runs. `validation_rank=NULL` means
"passed one run arbitrarily" — not "reliably retrievable." The NULL rank is itself
a signal of marginal or broken phrase quality.

**Fix:** New phrase `sensuous symbols unit economy word choice` — confirmed [1, 1, 1].
Updated DB, re-uploaded file with new `Discrimination:` header.

---

### Problem 2: 44 of 63 CRAD phrases had `validation_rank=NULL`

**Observed:** After the Class 07-02 discovery, a DB query found that 44 of 63
CRAD-validated files had `validation_rank IS NULL` or `validation_rank > 3`. These
files were "conditionally exposed" — A7 uses their CRAD phrase as the fallback but
has no confirmation the phrase actually works.

**Expected:** All 63 validated phrases should have confirmed retrieval ranks
demonstrating their effectiveness before being relied upon in A7.

**Causal chain:** The Phase 16.6-02 CRAD pipeline validated phrases with the `any_pass`
gate (1/3 hits at rank ≤ 5 = accepted). The `validation_rank` column was set only
when all 3 runs agreed; otherwise it was left NULL. This meant most phrases were
accepted without a confirmed rank. A7 then relied on these phrases as if they were
equivalent to phrases with confirmed rank=1 — which they were not.

**Fix:** Ran a targeted probe (3 Gemini runs × 44 files = 132 API calls). Results:
35 STRONG (best_rank ≤ 3), 3 MARGINAL (best_rank 4–5), 6 FAIL (never found). Updated
DB with confirmed ranks for all 35 STRONG and 3 MARGINAL. Re-ran genus-differentia
on the 6 FAIL + 3 MARGINAL files, found improved phrases for 7/9, and re-uploaded
those 7 files with updated `Discrimination:` headers.

**Correct invariant:** After any CRAD validation run, a secondary probe must confirm
the rank for every phrase with `validation_rank IS NULL`. The `any_pass` gate is
sufficient to accept a candidate phrase for embedding; it is not sufficient to
declare the phrase "validated" for A7 fallback purposes.

---

### Problem 3: Two files (ITOE AT 12-02 OH, ITOE AT 14-02 OH) have no reliable discriminating phrase

**Observed:** After multiple rounds of genus-differentia candidate testing, the best
achievable ranks for these two files are 5 (passes=1/3) and 4 (passes=3/3) respectively.
No combination of aspects achieves rank ≤ 3.

**Causal chain:** Both files cover epistemologically generic topics (concept
knowledge reference; historical concept formation) that are densely represented
across many ITOE class transcripts. No rarity signal is strong enough to
discriminate these files corpus-relatively. The genus of the ITOE Advanced Topics
series is so broad that these files' aspects are not unique to them.

**Current state:** Best available phrases kept at marginal ranks. A7 will occasionally
miss these files when they are sampled. At a 60-file sample over 1,749 files, each
of these two files has ~3.4% chance of being sampled per run. Practical A7 failure
risk from these two files is low.

---

## Decisions Made

### Decision 1: Raise A7 default sample-count from 5 to 60 (permanent)

**Decision:** Changed `--sample-count` default in `check_stability.py` from 5 → 60
in all three locations (argparse default, `__init__` signature, method signature).

**Rejected alternative:** Keep 20 as the "standard" sample and 60 as opt-in. Rejected
because: the 10-run stress test at 20/run only hit the failing Class 07-02 file once
in 10 runs — the small sample masks fragile files. 60/1,749 = 3.4% hit probability
per file per run, which is still low for rare problematic files but is the practical
limit given Gemini API call count and ~5.5 minutes per run.

**Rejected alternative:** Use 100. Rejected because A7 makes 1 Gemini API call per
file via S1, plus a CRAD fallback call (for the ~60 CRAD-covered files), plus an S4a
fallback call for any remaining misses. At 100 files, A7 would take 10–15 minutes per
run, making the 3-run gate require 30–45 minutes. 60 gives adequate coverage in ~5.5
minutes.

**Stable:** Yes. The new default applies to all future `check_stability.py` runs.
To run a lighter check (e.g., during development), use `--sample-count 20`.

---

### Decision 2: Accept two permanently marginal CRAD files

**Decision:** `ITOE AT - Class 12-02 - Office Hour.txt` (phrase: `meaning concept
knowledge reference`, best_rank=5) and `ITOE AT - Class 14-02 - Office Hour.txt`
(phrase: `Historical context of concept formation`, best_rank=4) are kept as-is.
No further re-CRAD attempts.

**Rejected alternative:** Continue attempting genus-differentia with additional aspect
combinations. Rejected because: multiple rounds (4+ per file) exhausted all aspect
combinations available in the DB. These files' topics are genuinely over-represented
in the corpus — no combination of their extracted aspects produces a corpus-relative
discriminating phrase.

**Rejected alternative:** Re-extract metadata for these files with different extraction
prompting to find better differentiating aspects. Rejected because: the metadata
extraction is high-quality; the problem is the corpus, not the extraction. These files
discuss foundational epistemology topics that appear across most Objectivism transcripts.

**Stable:** Yes, unless new discriminating content is added to these files' transcripts
or the corpus changes significantly. These two files are the known floor of CRAD
coverage. Future sessions should not attempt further phrase improvements for them
without first confirming the aspect vocabulary has changed.

---

## Current State

| Property | Value | Cause |
|----------|-------|-------|
| DB indexed | 1,749 | Full library uploaded via Phase 16.3-03 remediation |
| Store docs | 1,749 | store-sync confirmed clean; no orphans |
| CRAD phrases total | 63 | All 63 S1-failing class-number files have validated phrases |
| CRAD rank=1 | 42 files | Strong corpus-relative discrimination confirmed 3/3 runs |
| CRAD rank=2–3 | 12 files | STRONG; occasional miss but reliably found at ≤3 |
| CRAD rank=4–5 | 7 files | MARGINAL; found inconsistently; two have no better phrase |
| CRAD rank=None | 2 files | AT-12-02 and AT-14-02; marginal passes only; permanent floor |
| A7 default sample-count | 60 | Raised from 5 to expose fragile CRAD phrases in routine checks |
| A7 tolerance | 0 | Zero-tolerance; any miss = UNSTABLE |
| Phase 16.6 | COMPLETE | 3-plan execution plus hardening cycle confirmed STABLE 60/60 |
| Phase 16-02 (temporal stability) | IN PROGRESS | T=0 baseline 2026-02-26 01:48 UTC; T+24h gate pending |
| Phase 17 | UNBLOCKED | Next planned phase |

---

## Files Changed

| File | Change |
|------|--------|
| `scripts/check_stability.py` | A7 default sample-count raised from 5 → 60 in argparse, `__init__`, and method signature |
| `data/library.db` | `file_discrimination_phrases`: confirmed `validation_rank` for all 63 files; 7 new phrases replacing FAIL/MARGINAL ones |
| `data/library.db` | 7 files re-uploaded: `gemini_file_id`, `gemini_store_doc_id`, `gemini_state_updated_at` updated to new store docs |
| `scripts/crad_full_run.py` | New script: full CRAD run including genus-differentia, phrase testing, DB update, re-upload pipeline |
| `.planning/phases/16.6-crad/16.6-02-PLAN.md` | Created: plan for full CRAD run of all 63 files |
| `.planning/phases/16.6-crad/16.6-02-SUMMARY.md` | Created: full probe results, phrase table, gate PASS |
| `.planning/phases/16.6-crad/16.6-02-results.json` | Created: raw probe output (44 files × 3 runs) |
| `.planning/phases/16.6-crad/16.6-03-PLAN.md` | Created: plan for A7 update + stability gate |
| `.planning/phases/16.6-crad/16.6-03-SUMMARY.md` | Created: gate PASS, 3× STABLE 20/20 A7 |
| `.planning/STATE.md` | Phase 16.6 COMPLETE, Phase 17 UNBLOCKED |
| `.planning/ROADMAP.md` | Phase 16.6 status updated |

---

## Lessons Learned

- **`validation_rank=NULL` is a broken phrase signal, not a missing-data annotation.**
  The CRAD pipeline stores NULL when a phrase hit the file in only 1 of 3 runs. Future
  code or sessions reading the table should treat NULL rank the same as rank > 5: the
  phrase is unconfirmed and should not be relied upon in A7.

- **The `any_pass` gate is appropriate for accepting a candidate phrase, not for
  declaring production readiness.** A phrase that passes 1/3 runs is a promising
  candidate worth embedding via the `Discrimination:` header. It is not a phrase ready
  to be the sole A7 fallback for a file. After embedding (re-upload), a second
  validation round is required to confirm the embedding improved retrieval.

- **A7 with a small sample is a weak validator for rare file types.** At 20/1,749,
  any specific file has ~1.1% probability of being sampled per run. A file that fails
  50% of the time (like Class 07-02 before the fix) has only a ~0.55% probability of
  being caught per run. 10 runs at 20 samples = ~5.4% probability of detection. 60/run
  raises this to ~16% per run. This is still not high — it means fragile files may
  require many runs to surface. The correct fix is a dedicated per-file probe for any
  `validation_rank IS NULL` CRAD file, which this session now established as protocol.

- **Re-uploading with an improved `Discrimination:` header is the correct remediation
  for a failed CRAD phrase.** The embedding encodes the phrase at the document level.
  Updating the DB alone does nothing — the Gemini store document still encodes the old
  phrase. Both the DB update and the re-upload are required for the new phrase to take
  effect in retrieval.

- **Corpus-relative discrimination has a hard floor for files with generic topics.**
  Two ITOE Advanced Topics files discuss topics so prevalent in Objectivism discourse
  that no extracted aspect combination produces a corpus-discriminating phrase. This is
  a property of the corpus, not of the extraction or the algorithm. Future attempts to
  improve these files should start by verifying whether new aspects have been extracted
  since the last attempt, rather than re-running the same aspects in different
  combinations.
